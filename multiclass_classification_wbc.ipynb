{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea890a67-2d61-4883-bc59-9bcacb06871b",
   "metadata": {},
   "source": [
    "### **MULTICLASS IMAGE CLASSIFICATION - WHITE BLOOD CELLS**\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1a5498e5-976d-4729-bd26-de973ddd1068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from collections import Counter,deque\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfbbcf-2a2b-41cf-b533-c30b5e27631a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **GLOBAL VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fad620d5-726d-4769-b787-b41e8a8a4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data'\n",
    "CATEGORIES = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\n",
    "SPLITS = ['train', 'validation', 'test']\n",
    "CHANNELS = ['red', 'green', 'blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3fb9e3-7b0a-417e-8a24-bc50fe845c9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **HELPER FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4c313356-b8bd-40bc-b9df-86fc7bdfad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_python_from_notebook(notebook,python=None):\n",
    "    if python is None: python=notebook\n",
    "    with open(notebook+'.ipynb','r') as f:\n",
    "        rawpy = json.load(f)\n",
    "    rawpy = [[] if c['source'] == [] else c['source'] for c in rawpy['cells'] if c['cell_type']=='code']\n",
    "    for r in rawpy:\n",
    "        r.extend(['\\n','\\n'])\n",
    "    raw = [l for r in rawpy for l in r]\n",
    "    with open(python+'.py', 'w') as f:\n",
    "        f.write(''.join(raw))\n",
    "get_raw_python_from_notebook('multiclass_classification_wbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e972cc57-b9c8-40d8-a21a-f9c3106c3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(m):\n",
    "    if type(m) is str:\n",
    "        m = tf.keras.models.load_model(m)\n",
    "    size = 0\n",
    "    for layer in m.layers:\n",
    "        weight_byte_heuristic = 13.69 # chosen from varied empirical evidence :)\n",
    "        for w in layer.weights:\n",
    "            s = w.numpy().shape\n",
    "            n = 1\n",
    "            for val in s:\n",
    "                n *= val\n",
    "            size += n\n",
    "    print('Total Neurons:', f'{size:,}')\n",
    "    print('Estimated Model Size:', np.round(1e-6*weight_byte_heuristic*size,2),'MB')\n",
    "    #return size,int(weight_byte_heuristic*size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012b4984-3990-4a1f-87d0-47b977f9bd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_file_counts(data_path=DATA_PATH, splits=SPLITS, categories=CATEGORIES):\n",
    "    dirs = {}\n",
    "    for j in splits:\n",
    "        dirs[j] = {}\n",
    "        for i in categories:\n",
    "            dirs[j][i] = os.path.join(data_path,j,i)\n",
    "            print('size of', j, 'directory for', i, ':', len(os.listdir(dirs[j][i])))\n",
    "        print('TOTAL length of', j, 'set :', sum([len(os.listdir(dirs[j][i])) for i in categories]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ea1e55-0684-47b9-825d-13101944133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(data_path=DATA_PATH, splits=SPLITS, categories=CATEGORIES, df=True):\n",
    "    paths = []\n",
    "    for j in splits:\n",
    "        for i in categories:\n",
    "            d = {'category': i , 'split': j}\n",
    "            p = os.path.join(data_path,j,i)\n",
    "            [paths.append(dict(d,**{'file_path':os.path.join(p,f),'file':f})) for f in os.listdir(p) if 'jpeg' in f]\n",
    "    if df is True: return pd.DataFrame.from_dict(paths)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3c6c26c-6b31-4551-8b2f-882f1bb2f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_channels(dataset_path, path, categories=CATEGORIES, splits=SPLITS, channels=CHANNELS):\n",
    "    filepaths = {}\n",
    "    for s in splits:\n",
    "        for c in categories:\n",
    "            for f in [f for f in os.listdir(os.path.join(dataset_path,s,c)) if 'jpeg' in f]:\n",
    "                p = os.path.join(dataset_path,s,c,f)\n",
    "                filepaths[p] = {}\n",
    "                for r in channels:\n",
    "                    filepaths[p][r] = os.path.join(dataset_path,'channels',s,c,(r+f))\n",
    "\n",
    "    img = image.load_img(path)\n",
    "    dims = np.array(img).shape\n",
    "    channels = np.reshape(img,(dims[0]*dims[1],3)).transpose()\n",
    "    (red,green,blue) = [np.reshape(channels[i],(dims[0],dims[1])).astype(float) for i in range(3)]\n",
    "    gb_mean = (0.5*(blue+green))\n",
    "    rb_mean = (0.5*(red+blue))\n",
    "    rg_mean = (0.5*(red+green))\n",
    "    red_dominance = red-gb_mean\n",
    "    green_dominance = green-rb_mean\n",
    "    blue_dominance = blue-rg_mean\n",
    "    red_dominance *= 255/np.max(red_dominance)\n",
    "    green_dominance *= 255/np.max(green_dominance)\n",
    "    blue_dominance *= 255/np.max(blue_dominance)\n",
    "\n",
    "    for r in ['red','green','blue']:\n",
    "        try:\n",
    "            os.mkdir(os.path.join(dataset_path,r))\n",
    "            #print('A')\n",
    "        except:\n",
    "            pass\n",
    "        for s in splits:\n",
    "            try:\n",
    "                os.mkdir(os.path.join(dataset_path,r,s))\n",
    "                #print('B')\n",
    "            except:\n",
    "                pass\n",
    "            for c in categories:\n",
    "                try:\n",
    "                    os.mkdir(os.path.join(dataset_path,r,s,c))\n",
    "                    #print('C')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    plt.imsave(filepaths[path]['red'], red_dominance, cmap='gray')\n",
    "    plt.imsave(filepaths[path]['green'], green_dominance, cmap='gray')\n",
    "    plt.imsave(filepaths[path]['blue'], blue_dominance, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d532f5-9038-4f94-99f0-b5165756fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_performance(model, data, indices=None, names=SPLITS, original_data=None, verbose=False):\n",
    "    if type(data) != list: data = [data]\n",
    "    if (original_data is not None) and (type(original_data) != list): original_data = [original_data]\n",
    "    if original_data is None: original_data = data\n",
    "    if indices is None: indices = [s for s in range(len(data[0].classes))]\n",
    "    blank_indices = [s for s in range(np.max(indices)) if s not in indices]\n",
    "\n",
    "    classes, true_classes, accuracy_tables, accuracy, cm = {}, {}, {}, {}, {}\n",
    "\n",
    "    for i in names:\n",
    "        classes[i] = [indices[np.argmax(c)] if len(c)>1 else indices[int(np.round(c))] for c in model.predict(data[names.index(i)], verbose=False)]+blank_indices\n",
    "        true_classes[i] = [indices[j] for j in data[names.index(i)].classes]+blank_indices\n",
    "        accuracy_tables[i] = pd.concat([pd.DataFrame(classes[i],columns=['classes']),pd.DataFrame(true_classes[i],columns=['true_classes'])],axis=1)\n",
    "        accuracy_tables[i]['accuracy'] = accuracy_tables[i].apply(lambda r: 1 if r['true_classes']==r['classes'] else 0,axis=1)\n",
    "        accuracy[i] = sum(accuracy_tables[i]['accuracy'])/len(accuracy_tables[i])\n",
    "        cm[i] = pd.DataFrame(confusion_matrix(true_classes[i], classes[i], normalize='true'))\n",
    "        for b in blank_indices:\n",
    "            cm[i].drop(b,axis=0,inplace=True)\n",
    "            cm[i].drop(b,axis=1,inplace=True)\n",
    "\n",
    "    return {\n",
    "        'classes': classes,\n",
    "        'true_classes': true_classes,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64cce0c-d2ee-438b-9537-eb2341520522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(d1, d2):\n",
    "    return pd.DataFrame(confusion_matrix(d1, d2, normalize='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b0badd1-929c-4271-8712-d54910aba5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MODEL CALLBACKS: reuseable\n",
    "\n",
    "# stop early if no improvement after 5 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    mode='max',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# save the model with the maximum validation accuracy \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/classification_wbc1.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    mode='max', \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "# reduce learning rate\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', #'val_loss',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lambda epoch: 1e-5 * 10**(1.5*epoch/EPOCHS))\n",
    "\n",
    "# traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/20) every epoch\n",
    "# def lr_scheduler(epochs=100, lrs=(1e-5,1e-2)):\n",
    "#     return LearningRateScheduler(\n",
    "#         lambda epoch: lrs[0] * 10**(np.log10(lrs[1]/lrs[0])*epoch/epochs)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320afb69-c400-43dd-b519-d1ceb74cfe0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "####\n",
    "#### **INITIAL ATTEMPTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54120475-e7a6-4132-bb22-dd43359cd7fe",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">First, let's read in data and get it into the format we expect. The method here will augment the training data, leaving the validation and test datasets untouched.</p>\n",
    "\n",
    "<p style=\"font-weight: 500; color: #556;\">We'll also rescale the data into the 0-1 range:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c09a4c16-9237-4ec3-aebf-ef2f6f4d6ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9957 images belonging to 4 classes.\n",
      "Found 1887 images belonging to 4 classes.\n",
      "Found 9957 images belonging to 4 classes.\n",
      "Found 1887 images belonging to 4 classes.\n",
      "Found 600 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "training_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0, # rotation_range=40,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_val_datagen = ImageDataGenerator(rescale = 1.0/255.0)\n",
    "\n",
    "def flow_data(generator, data_path, split,shuffle=True, classes=None, class_mode='categorical'):\n",
    "    return generator.flow_from_directory(\n",
    "        os.path.join(data_path,split),\n",
    "        target_size=(128,128),\n",
    "        classes=classes,\n",
    "        class_mode=class_mode,\n",
    "        batch_size=60,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "np.random.seed(67) # use a consistent seed so shuffling gives expected results\n",
    "train_data = flow_data(training_datagen, DATA_PATH, 'train')\n",
    "validation_data = flow_data(test_val_datagen, DATA_PATH, 'validation')\n",
    "train_data_unshuffled = flow_data(training_datagen, DATA_PATH, 'train', shuffle=False)\n",
    "validation_data_unshuffled = flow_data(test_val_datagen, DATA_PATH, 'validation', shuffle=False)\n",
    "test_data_unshuffled = flow_data(test_val_datagen, DATA_PATH, 'test', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "fb5fb3b8-d538-4a54-8dc2-605be397ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "6fc8eb9c-3c0c-4f2a-9a98-0eec5c1d6117",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_65 (Conv2D)          (None, 126, 126, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_65 (MaxPoolin  (None, 63, 63, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          (None, 61, 61, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_66 (MaxPoolin  (None, 30, 30, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_67 (MaxPoolin  (None, 14, 14, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_68 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_68 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 32)                147488    \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,780\n",
      "Trainable params: 407,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e73b932-ff45-4379-9c97-8f5bc197f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Neurons: 407,780\n",
      "Estimated Model Size: 5.58 MB\n"
     ]
    }
   ],
   "source": [
    "# here, we get a proxy for the model size based on the number of neurons.  \n",
    "get_model_size(model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "992f736d-bbd7-4bd1-a8ad-fc7773066b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(), #'rmsprop',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d23516d-5b0a-413c-a9d1-0508ccfb6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e82a2c3b-a4ea-4559-8668-63a050080e3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-03 01:24:20.922164: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - ETA: 0s - loss: 1.3868 - accuracy: 0.2523\n",
      "Epoch 1: val_accuracy improved from -inf to 0.24907, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 30s 175ms/step - loss: 1.3868 - accuracy: 0.2523 - val_loss: 1.3873 - val_accuracy: 0.2491 - lr: 5.0000e-05\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3860 - accuracy: 0.2598\n",
      "Epoch 2: val_accuracy improved from 0.24907 to 0.32220, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 1.3860 - accuracy: 0.2598 - val_loss: 1.3837 - val_accuracy: 0.3222 - lr: 5.1757e-05\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3803 - accuracy: 0.2826\n",
      "Epoch 3: val_accuracy did not improve from 0.32220\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 1.3803 - accuracy: 0.2826 - val_loss: 1.3768 - val_accuracy: 0.2570 - lr: 5.3576e-05\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.3219 - accuracy: 0.3674\n",
      "Epoch 4: val_accuracy improved from 0.32220 to 0.42713, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 1.3219 - accuracy: 0.3674 - val_loss: 1.1972 - val_accuracy: 0.4271 - lr: 5.5459e-05\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.1932 - accuracy: 0.4478\n",
      "Epoch 5: val_accuracy improved from 0.42713 to 0.49338, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 1.1932 - accuracy: 0.4478 - val_loss: 1.0556 - val_accuracy: 0.4934 - lr: 5.7408e-05\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 1.0819 - accuracy: 0.5231\n",
      "Epoch 6: val_accuracy improved from 0.49338 to 0.53842, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 1.0819 - accuracy: 0.5231 - val_loss: 0.9654 - val_accuracy: 0.5384 - lr: 5.9425e-05\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.9636 - accuracy: 0.5827\n",
      "Epoch 7: val_accuracy improved from 0.53842 to 0.57552, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.9636 - accuracy: 0.5827 - val_loss: 0.9390 - val_accuracy: 0.5755 - lr: 6.1513e-05\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.6167\n",
      "Epoch 8: val_accuracy improved from 0.57552 to 0.58347, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.8770 - accuracy: 0.6167 - val_loss: 0.8082 - val_accuracy: 0.5835 - lr: 6.3675e-05\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.8146 - accuracy: 0.6468\n",
      "Epoch 9: val_accuracy improved from 0.58347 to 0.61738, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.8146 - accuracy: 0.6468 - val_loss: 0.7799 - val_accuracy: 0.6174 - lr: 6.5913e-05\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.7558 - accuracy: 0.6743\n",
      "Epoch 10: val_accuracy improved from 0.61738 to 0.62639, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.7558 - accuracy: 0.6743 - val_loss: 0.7258 - val_accuracy: 0.6264 - lr: 6.8229e-05\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.7043\n",
      "Epoch 11: val_accuracy improved from 0.62639 to 0.64017, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.7060 - accuracy: 0.7043 - val_loss: 0.7292 - val_accuracy: 0.6402 - lr: 7.0627e-05\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6672 - accuracy: 0.7135\n",
      "Epoch 12: val_accuracy improved from 0.64017 to 0.66879, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.6672 - accuracy: 0.7135 - val_loss: 0.6971 - val_accuracy: 0.6688 - lr: 7.3109e-05\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.6249 - accuracy: 0.7420\n",
      "Epoch 13: val_accuracy did not improve from 0.66879\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.6249 - accuracy: 0.7420 - val_loss: 0.7244 - val_accuracy: 0.6645 - lr: 7.5678e-05\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.7557\n",
      "Epoch 14: val_accuracy improved from 0.66879 to 0.68945, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.5924 - accuracy: 0.7557 - val_loss: 0.6816 - val_accuracy: 0.6895 - lr: 7.8338e-05\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5447 - accuracy: 0.7806\n",
      "Epoch 15: val_accuracy did not improve from 0.68945\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.5447 - accuracy: 0.7806 - val_loss: 0.6774 - val_accuracy: 0.6688 - lr: 8.1091e-05\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5154 - accuracy: 0.7899\n",
      "Epoch 16: val_accuracy improved from 0.68945 to 0.70270, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5154 - accuracy: 0.7899 - val_loss: 0.6783 - val_accuracy: 0.7027 - lr: 8.3940e-05\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4907 - accuracy: 0.7947\n",
      "Epoch 17: val_accuracy did not improve from 0.70270\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.4907 - accuracy: 0.7947 - val_loss: 0.6760 - val_accuracy: 0.6926 - lr: 8.6890e-05\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4625 - accuracy: 0.8101\n",
      "Epoch 18: val_accuracy improved from 0.70270 to 0.71860, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.4625 - accuracy: 0.8101 - val_loss: 0.6608 - val_accuracy: 0.7186 - lr: 8.9944e-05\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4367 - accuracy: 0.8181\n",
      "Epoch 19: val_accuracy did not improve from 0.71860\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.4367 - accuracy: 0.8181 - val_loss: 0.5977 - val_accuracy: 0.6916 - lr: 9.3104e-05\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8289\n",
      "Epoch 20: val_accuracy improved from 0.71860 to 0.73768, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.4149 - accuracy: 0.8289 - val_loss: 0.5338 - val_accuracy: 0.7377 - lr: 9.6376e-05\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.8385\n",
      "Epoch 21: val_accuracy did not improve from 0.73768\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.3975 - accuracy: 0.8385 - val_loss: 0.5835 - val_accuracy: 0.7308 - lr: 9.9763e-05\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.8475\n",
      "Epoch 22: val_accuracy improved from 0.73768 to 0.76524, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.3727 - accuracy: 0.8475 - val_loss: 0.5238 - val_accuracy: 0.7652 - lr: 1.0327e-04\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3465 - accuracy: 0.8600\n",
      "Epoch 23: val_accuracy did not improve from 0.76524\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.3465 - accuracy: 0.8600 - val_loss: 0.4765 - val_accuracy: 0.7652 - lr: 1.0690e-04\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3425 - accuracy: 0.8573\n",
      "Epoch 24: val_accuracy did not improve from 0.76524\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.3425 - accuracy: 0.8573 - val_loss: 0.6099 - val_accuracy: 0.7472 - lr: 1.1065e-04\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3138 - accuracy: 0.8692\n",
      "Epoch 25: val_accuracy improved from 0.76524 to 0.77636, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.3138 - accuracy: 0.8692 - val_loss: 0.4616 - val_accuracy: 0.7764 - lr: 1.1454e-04\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.8712\n",
      "Epoch 26: val_accuracy improved from 0.77636 to 0.78961, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 177ms/step - loss: 0.3088 - accuracy: 0.8712 - val_loss: 0.4667 - val_accuracy: 0.7896 - lr: 1.1857e-04\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.8806\n",
      "Epoch 27: val_accuracy did not improve from 0.78961\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2894 - accuracy: 0.8806 - val_loss: 0.4997 - val_accuracy: 0.7848 - lr: 1.2274e-04\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2800 - accuracy: 0.8847\n",
      "Epoch 28: val_accuracy did not improve from 0.78961\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.2800 - accuracy: 0.8847 - val_loss: 0.6375 - val_accuracy: 0.7281 - lr: 1.2705e-04\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.8955\n",
      "Epoch 29: val_accuracy did not improve from 0.78961\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2564 - accuracy: 0.8955 - val_loss: 0.4551 - val_accuracy: 0.7636 - lr: 1.3151e-04\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.8869\n",
      "Epoch 30: val_accuracy did not improve from 0.78961\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.2711 - accuracy: 0.8869 - val_loss: 0.6536 - val_accuracy: 0.7753 - lr: 1.3614e-04\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2649 - accuracy: 0.8965\n",
      "Epoch 31: val_accuracy improved from 0.78961 to 0.79915, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2649 - accuracy: 0.8965 - val_loss: 0.4308 - val_accuracy: 0.7992 - lr: 1.4092e-04\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8946\n",
      "Epoch 32: val_accuracy improved from 0.79915 to 0.82353, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.2706 - accuracy: 0.8946 - val_loss: 0.3708 - val_accuracy: 0.8235 - lr: 1.4587e-04\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9091\n",
      "Epoch 33: val_accuracy did not improve from 0.82353\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2270 - accuracy: 0.9091 - val_loss: 0.4196 - val_accuracy: 0.7801 - lr: 1.5100e-04\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9211\n",
      "Epoch 34: val_accuracy did not improve from 0.82353\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2031 - accuracy: 0.9211 - val_loss: 0.5371 - val_accuracy: 0.7901 - lr: 1.5630e-04\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9191\n",
      "Epoch 35: val_accuracy did not improve from 0.82353\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.2009 - accuracy: 0.9191 - val_loss: 0.4072 - val_accuracy: 0.7901 - lr: 1.6180e-04\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9142\n",
      "Epoch 36: val_accuracy did not improve from 0.82353\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2227 - accuracy: 0.9142 - val_loss: 0.4058 - val_accuracy: 0.8023 - lr: 1.6748e-04\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9322\n",
      "Epoch 37: val_accuracy improved from 0.82353 to 0.82989, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1795 - accuracy: 0.9322 - val_loss: 0.3590 - val_accuracy: 0.8299 - lr: 1.7337e-04\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9290\n",
      "Epoch 38: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1823 - accuracy: 0.9290 - val_loss: 0.5339 - val_accuracy: 0.8098 - lr: 1.7946e-04\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9337\n",
      "Epoch 39: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1746 - accuracy: 0.9337 - val_loss: 0.3785 - val_accuracy: 0.8039 - lr: 1.8577e-04\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9291\n",
      "Epoch 40: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1782 - accuracy: 0.9291 - val_loss: 0.4160 - val_accuracy: 0.7970 - lr: 1.9230e-04\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9402\n",
      "Epoch 41: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1594 - accuracy: 0.9402 - val_loss: 0.4688 - val_accuracy: 0.7711 - lr: 1.9905e-04\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1709 - accuracy: 0.9339\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.0604876044671983e-05.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1709 - accuracy: 0.9339 - val_loss: 0.4618 - val_accuracy: 0.8050 - lr: 2.0605e-05\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1528 - accuracy: 0.9413\n",
      "Epoch 43: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1528 - accuracy: 0.9413 - val_loss: 0.5237 - val_accuracy: 0.7944 - lr: 2.1329e-04\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9385\n",
      "Epoch 44: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1647 - accuracy: 0.9385 - val_loss: 0.4945 - val_accuracy: 0.8161 - lr: 2.2079e-04\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9141\n",
      "Epoch 45: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2252 - accuracy: 0.9141 - val_loss: 0.5067 - val_accuracy: 0.8060 - lr: 2.2854e-04\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9514\n",
      "Epoch 46: val_accuracy did not improve from 0.82989\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1296 - accuracy: 0.9514 - val_loss: 0.4297 - val_accuracy: 0.8113 - lr: 2.3658e-04\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.9492\n",
      "Epoch 47: val_accuracy improved from 0.82989 to 0.83254, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1316 - accuracy: 0.9492 - val_loss: 0.3451 - val_accuracy: 0.8325 - lr: 2.4489e-04\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9498\n",
      "Epoch 48: val_accuracy did not improve from 0.83254\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1296 - accuracy: 0.9498 - val_loss: 0.3769 - val_accuracy: 0.8071 - lr: 2.5350e-04\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9457\n",
      "Epoch 49: val_accuracy did not improve from 0.83254\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1366 - accuracy: 0.9457 - val_loss: 0.7982 - val_accuracy: 0.7668 - lr: 2.6240e-04\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1867 - accuracy: 0.9283\n",
      "Epoch 50: val_accuracy did not improve from 0.83254\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1867 - accuracy: 0.9283 - val_loss: 0.3805 - val_accuracy: 0.8267 - lr: 2.7163e-04\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9414\n",
      "Epoch 51: val_accuracy did not improve from 0.83254\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1560 - accuracy: 0.9414 - val_loss: 0.4404 - val_accuracy: 0.8272 - lr: 2.8117e-04\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1123 - accuracy: 0.9584\n",
      "Epoch 52: val_accuracy improved from 0.83254 to 0.85215, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1123 - accuracy: 0.9584 - val_loss: 0.3174 - val_accuracy: 0.8521 - lr: 2.9105e-04\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9577\n",
      "Epoch 53: val_accuracy did not improve from 0.85215\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1098 - accuracy: 0.9577 - val_loss: 0.5840 - val_accuracy: 0.8145 - lr: 3.0128e-04\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9558\n",
      "Epoch 54: val_accuracy improved from 0.85215 to 0.86963, saving model to models/classification_wbc1_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1161 - accuracy: 0.9558 - val_loss: 0.3607 - val_accuracy: 0.8696 - lr: 3.1187e-04\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9574\n",
      "Epoch 55: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1165 - accuracy: 0.9574 - val_loss: 0.4869 - val_accuracy: 0.8235 - lr: 3.2283e-04\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9562\n",
      "Epoch 56: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1134 - accuracy: 0.9562 - val_loss: 0.5682 - val_accuracy: 0.8267 - lr: 3.3417e-04\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9587\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.459154977463186e-05.\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1107 - accuracy: 0.9587 - val_loss: 0.5735 - val_accuracy: 0.8283 - lr: 3.4592e-05\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9115\n",
      "Epoch 58: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.2449 - accuracy: 0.9115 - val_loss: 0.5008 - val_accuracy: 0.8368 - lr: 3.5807e-04\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9607\n",
      "Epoch 59: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1064 - accuracy: 0.9607 - val_loss: 0.4847 - val_accuracy: 0.8267 - lr: 3.7066e-04\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9527\n",
      "Epoch 60: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1199 - accuracy: 0.9527 - val_loss: 0.3467 - val_accuracy: 0.8437 - lr: 3.8368e-04\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9541\n",
      "Epoch 61: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1313 - accuracy: 0.9541 - val_loss: 0.5255 - val_accuracy: 0.8304 - lr: 3.9716e-04\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9552\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.111213202122599e-05.\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1164 - accuracy: 0.9552 - val_loss: 0.4061 - val_accuracy: 0.8553 - lr: 4.1112e-05\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9475\n",
      "Epoch 63: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1372 - accuracy: 0.9475 - val_loss: 0.3716 - val_accuracy: 0.8463 - lr: 4.2557e-04\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1050 - accuracy: 0.9604\n",
      "Epoch 64: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1050 - accuracy: 0.9604 - val_loss: 0.8516 - val_accuracy: 0.7944 - lr: 4.4052e-04\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9583\n",
      "Epoch 65: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1102 - accuracy: 0.9583 - val_loss: 0.5244 - val_accuracy: 0.8421 - lr: 4.5601e-04\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9711\n",
      "Epoch 66: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0751 - accuracy: 0.9711 - val_loss: 0.3423 - val_accuracy: 0.8521 - lr: 4.7203e-04\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1135 - accuracy: 0.9591\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.886186216026545e-05.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1135 - accuracy: 0.9591 - val_loss: 0.5672 - val_accuracy: 0.8431 - lr: 4.8862e-05\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9701\n",
      "Epoch 68: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0824 - accuracy: 0.9701 - val_loss: 0.3343 - val_accuracy: 0.8495 - lr: 5.0579e-04\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9669\n",
      "Epoch 69: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0896 - accuracy: 0.9669 - val_loss: 0.4218 - val_accuracy: 0.8468 - lr: 5.2356e-04\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9712\n",
      "Epoch 70: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0773 - accuracy: 0.9712 - val_loss: 0.8454 - val_accuracy: 0.8431 - lr: 5.4196e-04\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9529\n",
      "Epoch 71: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1247 - accuracy: 0.9529 - val_loss: 0.6111 - val_accuracy: 0.8421 - lr: 5.6101e-04\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9558\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.807243287563324e-05.\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1187 - accuracy: 0.9558 - val_loss: 0.6843 - val_accuracy: 0.8511 - lr: 5.8072e-05\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9640\n",
      "Epoch 73: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0956 - accuracy: 0.9640 - val_loss: 0.9120 - val_accuracy: 0.8447 - lr: 6.0113e-04\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9767\n",
      "Epoch 74: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0626 - accuracy: 0.9767 - val_loss: 0.7663 - val_accuracy: 0.8320 - lr: 6.2226e-04\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9473\n",
      "Epoch 75: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1415 - accuracy: 0.9473 - val_loss: 0.4602 - val_accuracy: 0.8219 - lr: 6.4412e-04\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9693\n",
      "Epoch 76: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0855 - accuracy: 0.9693 - val_loss: 0.6089 - val_accuracy: 0.8474 - lr: 6.6676e-04\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9768\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.901921587996185e-05.\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0631 - accuracy: 0.9768 - val_loss: 0.6153 - val_accuracy: 0.8580 - lr: 6.9019e-05\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9811\n",
      "Epoch 78: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0551 - accuracy: 0.9811 - val_loss: 0.8131 - val_accuracy: 0.8468 - lr: 7.1445e-04\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9587\n",
      "Epoch 79: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1128 - accuracy: 0.9587 - val_loss: 0.6854 - val_accuracy: 0.8400 - lr: 7.3955e-04\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9761\n",
      "Epoch 80: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0634 - accuracy: 0.9761 - val_loss: 0.7159 - val_accuracy: 0.8580 - lr: 7.6554e-04\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9668\n",
      "Epoch 81: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 30s 177ms/step - loss: 0.0892 - accuracy: 0.9668 - val_loss: 0.5945 - val_accuracy: 0.8267 - lr: 7.9245e-04\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9567\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 8.202948956750334e-05.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1202 - accuracy: 0.9567 - val_loss: 0.6668 - val_accuracy: 0.8384 - lr: 8.2029e-05\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9759\n",
      "Epoch 83: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 177ms/step - loss: 0.0679 - accuracy: 0.9759 - val_loss: 0.8659 - val_accuracy: 0.8082 - lr: 8.4912e-04\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9815\n",
      "Epoch 84: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0497 - accuracy: 0.9815 - val_loss: 0.8517 - val_accuracy: 0.8479 - lr: 8.7896e-04\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9649\n",
      "Epoch 85: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0943 - accuracy: 0.9649 - val_loss: 0.8071 - val_accuracy: 0.8442 - lr: 9.0985e-04\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0874 - accuracy: 0.9673\n",
      "Epoch 86: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0874 - accuracy: 0.9673 - val_loss: 1.0994 - val_accuracy: 0.6942 - lr: 9.4182e-04\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9678\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 9.749223245307803e-05.\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0832 - accuracy: 0.9678 - val_loss: 0.5672 - val_accuracy: 0.8299 - lr: 9.7492e-05\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1037 - accuracy: 0.9645\n",
      "Epoch 88: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1037 - accuracy: 0.9645 - val_loss: 0.6579 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9664\n",
      "Epoch 89: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0879 - accuracy: 0.9664 - val_loss: 0.7285 - val_accuracy: 0.8447 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9737\n",
      "Epoch 90: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0712 - accuracy: 0.9737 - val_loss: 0.7050 - val_accuracy: 0.8415 - lr: 0.0011\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9507\n",
      "Epoch 91: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.1365 - accuracy: 0.9507 - val_loss: 0.9240 - val_accuracy: 0.6948 - lr: 0.0011\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9681\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001158697297796607.\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0869 - accuracy: 0.9681 - val_loss: 0.9160 - val_accuracy: 0.8506 - lr: 1.1587e-04\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9804\n",
      "Epoch 93: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0557 - accuracy: 0.9804 - val_loss: 0.8574 - val_accuracy: 0.8521 - lr: 0.0012\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9731\n",
      "Epoch 94: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0755 - accuracy: 0.9731 - val_loss: 1.2015 - val_accuracy: 0.8151 - lr: 0.0012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9711\n",
      "Epoch 95: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.0829 - accuracy: 0.9711 - val_loss: 0.6416 - val_accuracy: 0.8537 - lr: 0.0013\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9401\n",
      "Epoch 96: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1656 - accuracy: 0.9401 - val_loss: 0.6181 - val_accuracy: 0.8124 - lr: 0.0013\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9776\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00013771143276244401.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.0597 - accuracy: 0.9776 - val_loss: 0.6898 - val_accuracy: 0.8574 - lr: 1.3771e-04\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9868\n",
      "Epoch 98: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.7064 - val_accuracy: 0.8537 - lr: 0.0014\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9653\n",
      "Epoch 99: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 177ms/step - loss: 0.0942 - accuracy: 0.9653 - val_loss: 0.5546 - val_accuracy: 0.8532 - lr: 0.0015\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9849\n",
      "Epoch 100: val_accuracy did not improve from 0.86963\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0438 - accuracy: 0.9849 - val_loss: 0.9170 - val_accuracy: 0.8500 - lr: 0.0015\n"
     ]
    }
   ],
   "source": [
    "history1 = model_1.fit(\n",
    "    train_data,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = validation_data,\n",
    "    verbose = 1,\n",
    "    callbacks = [reduce_lr,lr_scheduler,checkpoint]\n",
    ")\n",
    "# model_1.save('models/classification_wbc1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3a7b8aa-d335-4555-a3ed-86a9a8f6c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve best model saved from checkpoints\n",
    "model_1 = tf.keras.models.load_model('models/classification_wbc1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ce556-6f59-4854-a5b9-37c7ae1b6de4",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Using our initial model, we can now generate predictions and plot a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a8c2d05-0241-4660-9185-4bbd9854da12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 23s 140ms/step\n",
      "32/32 [==============================] - 2s 54ms/step\n",
      "10/10 [==============================] - 1s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "classes_1 = {}\n",
    "classes_1['train'] = [np.argmax(c) for c in model1.predict(train_data_unshuffled)]\n",
    "classes_1['validation'] = [np.argmax(c) for c in model1.predict(validation_data_unshuffled)]\n",
    "classes_1['test'] = [np.argmax(c) for c in model1.predict(test_data_unshuffled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ead30e29-e368-4259-8e7e-71f043e7b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we recover the classes from the main data - it's important to make sure that they are all unshuffled\n",
    "true_classes_1 = {}\n",
    "true_classes_1['train'] = train_data_unshuffled.classes\n",
    "true_classes_1['validation'] = validation_data_unshuffled.classes\n",
    "true_classes_1['test'] = test_data_unshuffled.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87eb0891-0d88-4a1e-9c70-2218ab07c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_tables_1 = {}\n",
    "accuracy_1 = {}\n",
    "for j in SPLITS:\n",
    "    accuracy_tables_1[j] = pd.concat([pd.DataFrame(classes_1[j],columns=['classes']),pd.DataFrame(true_classes_1[j],columns=['true_classes'])],axis=1)\n",
    "    accuracy_tables_1[j]['accuracy'] = accuracy_tables_1[j].apply(lambda r: 1 if r['true_classes']==r['classes'] else 0,axis=1)\n",
    "    accuracy_1[j] = sum(accuracy_tables_1[j]['accuracy'])/len(accuracy_tables_1[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d1a5902b-afce-4cfb-a5c1-d948b7f3992b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.9328110876770112, 'validation': 0.8696343402225755, 'test': 0.875}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391e0d4-8a13-4dc4-b387-70bfecaa8b58",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">View the confusion matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "40d2f774-b795-469f-922d-8336a17454b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          0         1         2         3\n",
      "0  0.778534  0.002803  0.008811  0.209852\n",
      "1  0.000000  0.979863  0.014901  0.005236\n",
      "2  0.000000  0.000807  0.987086  0.012107\n",
      "3  0.006002  0.002401  0.005202  0.986395\n",
      "\n",
      "          0        1         2         3\n",
      "0  0.636364  0.00000  0.000000  0.363636\n",
      "1  0.000000  0.97234  0.014894  0.012766\n",
      "2  0.000000  0.00000  0.912766  0.087234\n",
      "3  0.021097  0.00211  0.018987  0.957806\n",
      "\n",
      "          0         1         2         3\n",
      "0  0.680000  0.000000  0.000000  0.320000\n",
      "1  0.000000  0.966667  0.026667  0.006667\n",
      "2  0.000000  0.000000  0.873333  0.126667\n",
      "3  0.006667  0.000000  0.013333  0.980000\n"
     ]
    }
   ],
   "source": [
    "for j in SPLITS:\n",
    "    print()\n",
    "    print(pd.DataFrame(confusion_matrix(true_classes_1[j],classes_1[j],normalize='true')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29879855-f6e7-443c-b2a2-9ea057c710a4",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">These outputs have been rendered using a helper function and can be accessed in this way going forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e2d3e-5cef-4a0f-a2fb-49d5590e507e",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Our initial results are very promising, but we can see that the model has overfit here. What can we do to resolve this? Let's look at some regularization techniques, being careful not to increase the bias too much here</p>\n",
    "<p style=\"font-weight: 500; color: #556;\">We can see that group 1 has a near perfect ability to be identified, whereas confusion exists between the remaining groups.  The spit between 0 ('EOSINOPHIL') and 3 ('NEUTROPHIL') is our largest source of confusion</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e3cf7-9c41-46ca-9c44-d39820fc1777",
   "metadata": {
    "tags": []
   },
   "source": [
    "####\n",
    "#### **SUCCESSIVE REMOVAL OF CLASSES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f5dd78-7a22-4f8e-b7bc-a82a5796b15f",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">We have seen that class LYMPHOCYTE (label 1) gets picked out very well by initial models.  This suggests that once they have been predicted, a model with only 3 classes can then be made, to reduce noise when making further predictions. This process can be repeated until we ar left with a binary model for our final 2 classes.\n",
    "<p style=\"font-weight: 500; color: #556;\">We now look at the use of a waterfall system for identifying remaining items, resulting in models with successively lower numbers of classes to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a9378d1a-7104-4c3c-b19b-3f757de1e906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7474 images belonging to 3 classes.\n",
      "Found 1417 images belonging to 3 classes.\n",
      "Found 7474 images belonging to 3 classes.\n",
      "Found 1417 images belonging to 3 classes.\n",
      "Found 450 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# flow from directory using only the labels 0, 2 and 3\n",
    "\n",
    "categories_4a = ['EOSINOPHIL', 'MONOCYTE', 'NEUTROPHIL']\n",
    "\n",
    "train_data_4a = flow_data(training_datagen, DATA_PATH, 'train', classes=categories_4a)\n",
    "validation_data_4a = flow_data(test_val_datagen, DATA_PATH, 'validation', classes=categories_4a)\n",
    "train_data_unshuffled_4a = flow_data(training_datagen, DATA_PATH, 'train', shuffle=False, classes=categories_4a)\n",
    "validation_data_unshuffled_4a = flow_data(test_val_datagen, DATA_PATH, 'validation', shuffle=False, classes=categories_4a)\n",
    "test_data_unshuffled_4a = flow_data(test_val_datagen,DATA_PATH, 'test', shuffle=False, classes=categories_4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6546b8af-113b-49bf-b81e-61272571efb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_grid = pd.DataFrame(np.array([test_data_unshuffled.classes,\n",
    "#     test_data_unshuffled.labels,\n",
    "#     test_data_unshuffled.filepaths]).transpose(),\n",
    "#     columns=['preds','actual','filepath'])\n",
    "# test_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "02458de0-e951-4221-8354-1b8d28ae2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4a = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(), # tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "acc55a4a-09ae-4cda-9184-ba65a6b30726",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4a.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(), #'rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "77a25236-3448-475d-9a35-6d8861f2d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with the maximum validation accuracy \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/classification_wbc4a_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7525d8c2-9d3d-4b4b-9fc6-f7114d7438f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "92850343-b91b-4cbb-9b94-7e5bee5d380e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 1.0992 - accuracy: 0.3353 - val_loss: 1.0978 - val_accuracy: 0.3726 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 1.0984 - accuracy: 0.3409 - val_loss: 1.0971 - val_accuracy: 0.3846 - lr: 1.0351e-05\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 1.0981 - accuracy: 0.3344 - val_loss: 1.0964 - val_accuracy: 0.4495 - lr: 1.0715e-05\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 1.0974 - accuracy: 0.3578 - val_loss: 1.0955 - val_accuracy: 0.3352 - lr: 1.1092e-05\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 1.0966 - accuracy: 0.3602 - val_loss: 1.0934 - val_accuracy: 0.3359 - lr: 1.1482e-05\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 1.0949 - accuracy: 0.3770 - val_loss: 1.0889 - val_accuracy: 0.4820 - lr: 1.1885e-05\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 1.0903 - accuracy: 0.4025 - val_loss: 1.0794 - val_accuracy: 0.4397 - lr: 1.2303e-05\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 20s 155ms/step - loss: 1.0831 - accuracy: 0.4138 - val_loss: 1.0668 - val_accuracy: 0.4806 - lr: 1.2735e-05\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 1.0716 - accuracy: 0.4263 - val_loss: 1.0524 - val_accuracy: 0.4425 - lr: 1.3183e-05\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 1.0556 - accuracy: 0.4334 - val_loss: 1.0191 - val_accuracy: 0.4947 - lr: 1.3646e-05\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 1.0371 - accuracy: 0.4517 - val_loss: 0.9850 - val_accuracy: 0.4700 - lr: 1.4125e-05\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 1.0158 - accuracy: 0.4688 - val_loss: 0.9561 - val_accuracy: 0.5392 - lr: 1.4622e-05\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.9909 - accuracy: 0.4888 - val_loss: 0.9257 - val_accuracy: 0.5780 - lr: 1.5136e-05\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.9687 - accuracy: 0.5087 - val_loss: 0.8839 - val_accuracy: 0.5314 - lr: 1.5668e-05\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.9382 - accuracy: 0.5427 - val_loss: 0.8597 - val_accuracy: 0.5455 - lr: 1.6218e-05\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.9126 - accuracy: 0.5593 - val_loss: 0.8190 - val_accuracy: 0.5992 - lr: 1.6788e-05\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.8865 - accuracy: 0.5717 - val_loss: 0.7843 - val_accuracy: 0.5660 - lr: 1.7378e-05\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.8543 - accuracy: 0.6101 - val_loss: 0.7634 - val_accuracy: 0.5723 - lr: 1.7989e-05\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.8239 - accuracy: 0.6246 - val_loss: 0.7277 - val_accuracy: 0.6288 - lr: 1.8621e-05\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.7967 - accuracy: 0.6373 - val_loss: 0.7097 - val_accuracy: 0.5801 - lr: 1.9275e-05\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.7701 - accuracy: 0.6481 - val_loss: 0.6987 - val_accuracy: 0.5900 - lr: 1.9953e-05\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.7496 - accuracy: 0.6548 - val_loss: 0.6943 - val_accuracy: 0.5999 - lr: 2.0654e-05\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.7301 - accuracy: 0.6682 - val_loss: 0.6745 - val_accuracy: 0.5992 - lr: 2.1380e-05\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.7152 - accuracy: 0.6853 - val_loss: 0.7148 - val_accuracy: 0.5963 - lr: 2.2131e-05\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.6998 - accuracy: 0.6808 - val_loss: 0.6758 - val_accuracy: 0.5970 - lr: 2.2909e-05\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.6779 - accuracy: 0.7063 - val_loss: 0.6765 - val_accuracy: 0.6147 - lr: 2.3714e-05\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 20s 161ms/step - loss: 0.6706 - accuracy: 0.6984 - val_loss: 0.7784 - val_accuracy: 0.5900 - lr: 2.4547e-05\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.6559 - accuracy: 0.7011 - val_loss: 0.6513 - val_accuracy: 0.6422 - lr: 2.5410e-05\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.6384 - accuracy: 0.7180 - val_loss: 0.7210 - val_accuracy: 0.6090 - lr: 2.6303e-05\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.6294 - accuracy: 0.7177 - val_loss: 0.6686 - val_accuracy: 0.5857 - lr: 2.7227e-05\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.6168 - accuracy: 0.7230 - val_loss: 0.6477 - val_accuracy: 0.6422 - lr: 2.8184e-05\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.6046 - accuracy: 0.7292 - val_loss: 0.6548 - val_accuracy: 0.5970 - lr: 2.9174e-05\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.5987 - accuracy: 0.7289 - val_loss: 0.6803 - val_accuracy: 0.6069 - lr: 3.0200e-05\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.5824 - accuracy: 0.7410 - val_loss: 0.6770 - val_accuracy: 0.5921 - lr: 3.1261e-05\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.5741 - accuracy: 0.7395 - val_loss: 0.6517 - val_accuracy: 0.6429 - lr: 3.2359e-05\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5641 - accuracy: 0.7502 - val_loss: 0.6909 - val_accuracy: 0.5907 - lr: 3.3497e-05\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.5640 - accuracy: 0.7475 - val_loss: 0.7462 - val_accuracy: 0.6034 - lr: 3.4674e-05\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.5639 - accuracy: 0.7408 - val_loss: 0.7065 - val_accuracy: 0.5836 - lr: 3.5892e-05\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.5421 - accuracy: 0.7586 - val_loss: 0.7038 - val_accuracy: 0.6119 - lr: 3.7154e-05\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.5427 - accuracy: 0.7424 - val_loss: 0.6645 - val_accuracy: 0.6295 - lr: 3.8459e-05\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5218 - accuracy: 0.7626 - val_loss: 0.7480 - val_accuracy: 0.6062 - lr: 3.9811e-05\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.5233 - accuracy: 0.7602 - val_loss: 0.7436 - val_accuracy: 0.6168 - lr: 4.1210e-05\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.5113 - accuracy: 0.7720 - val_loss: 0.6007 - val_accuracy: 0.6464 - lr: 4.2658e-05\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 21s 165ms/step - loss: 0.5068 - accuracy: 0.7653 - val_loss: 0.6095 - val_accuracy: 0.6641 - lr: 4.4157e-05\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 20s 157ms/step - loss: 0.4939 - accuracy: 0.7783 - val_loss: 0.6300 - val_accuracy: 0.6471 - lr: 4.5709e-05\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.4836 - accuracy: 0.7851 - val_loss: 0.7156 - val_accuracy: 0.6309 - lr: 4.7315e-05\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.4782 - accuracy: 0.7858 - val_loss: 0.6292 - val_accuracy: 0.6217 - lr: 4.8978e-05\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.4730 - accuracy: 0.7828 - val_loss: 0.6656 - val_accuracy: 0.6648 - lr: 5.0699e-05\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.4532 - accuracy: 0.7976 - val_loss: 0.5993 - val_accuracy: 0.6923 - lr: 5.2481e-05\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.4525 - accuracy: 0.7998 - val_loss: 0.7727 - val_accuracy: 0.6443 - lr: 5.4325e-05\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.4361 - accuracy: 0.8075 - val_loss: 0.7318 - val_accuracy: 0.6697 - lr: 5.6234e-05\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.4515 - accuracy: 0.7931 - val_loss: 0.5979 - val_accuracy: 0.6916 - lr: 5.8210e-05\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.4354 - accuracy: 0.8012 - val_loss: 0.6262 - val_accuracy: 0.6965 - lr: 6.0256e-05\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 20s 155ms/step - loss: 0.4179 - accuracy: 0.8175 - val_loss: 0.5732 - val_accuracy: 0.6994 - lr: 6.2373e-05\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.4014 - accuracy: 0.8215 - val_loss: 0.5498 - val_accuracy: 0.7255 - lr: 6.4565e-05\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.3986 - accuracy: 0.8225 - val_loss: 0.4817 - val_accuracy: 0.7622 - lr: 6.6834e-05\n",
      "Epoch 57/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.3874 - accuracy: 0.8251 - val_loss: 0.5805 - val_accuracy: 0.7092 - lr: 6.9183e-05\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 19s 154ms/step - loss: 0.4022 - accuracy: 0.8200 - val_loss: 0.4429 - val_accuracy: 0.7939 - lr: 7.1614e-05\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.3656 - accuracy: 0.8354 - val_loss: 0.4804 - val_accuracy: 0.7431 - lr: 7.4131e-05\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.3607 - accuracy: 0.8436 - val_loss: 0.5212 - val_accuracy: 0.7459 - lr: 7.6736e-05\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 20s 159ms/step - loss: 0.3707 - accuracy: 0.8385 - val_loss: 0.4891 - val_accuracy: 0.7615 - lr: 7.9433e-05\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.3543 - accuracy: 0.8416 - val_loss: 0.3914 - val_accuracy: 0.8349 - lr: 8.2224e-05\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 20s 158ms/step - loss: 0.3585 - accuracy: 0.8415 - val_loss: 0.6931 - val_accuracy: 0.7057 - lr: 8.5114e-05\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 20s 155ms/step - loss: 0.3384 - accuracy: 0.8532 - val_loss: 0.6200 - val_accuracy: 0.7092 - lr: 8.8105e-05\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 19s 155ms/step - loss: 0.3532 - accuracy: 0.8433 - val_loss: 0.4990 - val_accuracy: 0.7657 - lr: 9.1201e-05\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.3121 - accuracy: 0.8659 - val_loss: 0.6026 - val_accuracy: 0.7424 - lr: 9.4406e-05\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 19s 153ms/step - loss: 0.3138 - accuracy: 0.8651 - val_loss: 0.3809 - val_accuracy: 0.8490 - lr: 9.7724e-05\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.3005 - accuracy: 0.8702 - val_loss: 0.5395 - val_accuracy: 0.7537 - lr: 1.0116e-04\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 20s 156ms/step - loss: 0.3023 - accuracy: 0.8671 - val_loss: 0.3674 - val_accuracy: 0.8363 - lr: 1.0471e-04\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 19s 151ms/step - loss: 0.2934 - accuracy: 0.8721 - val_loss: 0.3929 - val_accuracy: 0.8003 - lr: 1.0839e-04\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.2804 - accuracy: 0.8765 - val_loss: 0.4322 - val_accuracy: 0.7982 - lr: 1.1220e-04\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.2743 - accuracy: 0.8851 - val_loss: 0.4016 - val_accuracy: 0.7911 - lr: 1.1614e-04\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.2474 - accuracy: 0.8974 - val_loss: 0.4842 - val_accuracy: 0.7664 - lr: 1.2023e-04\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 19s 150ms/step - loss: 0.2714 - accuracy: 0.8803 - val_loss: 0.3457 - val_accuracy: 0.8497 - lr: 1.2445e-04\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 19s 155ms/step - loss: 0.2598 - accuracy: 0.8958 - val_loss: 0.3515 - val_accuracy: 0.8518 - lr: 1.2882e-04\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 19s 152ms/step - loss: 0.2366 - accuracy: 0.9026 - val_loss: 0.4927 - val_accuracy: 0.7706 - lr: 1.3335e-04\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.2810 - accuracy: 0.8754 - val_loss: 0.5042 - val_accuracy: 0.7770 - lr: 1.3804e-04\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 22s 176ms/step - loss: 0.3024 - accuracy: 0.8717 - val_loss: 0.3648 - val_accuracy: 0.8243 - lr: 1.4289e-04\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2263 - accuracy: 0.9078 - val_loss: 0.3520 - val_accuracy: 0.8306 - lr: 1.4791e-04\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2374 - accuracy: 0.8960 - val_loss: 0.3096 - val_accuracy: 0.8574 - lr: 1.5311e-04\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 21s 170ms/step - loss: 0.2268 - accuracy: 0.9074 - val_loss: 0.4600 - val_accuracy: 0.7798 - lr: 1.5849e-04\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2352 - accuracy: 0.9027 - val_loss: 0.4594 - val_accuracy: 0.7805 - lr: 1.6406e-04\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.2283 - accuracy: 0.9066 - val_loss: 0.4317 - val_accuracy: 0.7946 - lr: 1.6982e-04\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2192 - accuracy: 0.9065 - val_loss: 0.5390 - val_accuracy: 0.7452 - lr: 1.7579e-04\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2050 - accuracy: 0.9156 - val_loss: 0.3772 - val_accuracy: 0.8179 - lr: 1.8197e-04\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.1966 - accuracy: 0.9208 - val_loss: 0.4380 - val_accuracy: 0.7939 - lr: 1.8836e-04\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2152 - accuracy: 0.9101 - val_loss: 0.3804 - val_accuracy: 0.8144 - lr: 1.9498e-04\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.2441 - accuracy: 0.8983 - val_loss: 0.5564 - val_accuracy: 0.7396 - lr: 2.0184e-04\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.2260 - accuracy: 0.9086 - val_loss: 0.4029 - val_accuracy: 0.8080 - lr: 2.0893e-04\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9207\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 2.162718592444435e-05.\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.1941 - accuracy: 0.9207 - val_loss: 0.5514 - val_accuracy: 0.7509 - lr: 2.1627e-04\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.1711 - accuracy: 0.9310 - val_loss: 0.4005 - val_accuracy: 0.8024 - lr: 2.2387e-04\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.1894 - accuracy: 0.9213 - val_loss: 0.5223 - val_accuracy: 0.7770 - lr: 2.3174e-04\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 22s 175ms/step - loss: 0.1649 - accuracy: 0.9339 - val_loss: 0.4447 - val_accuracy: 0.8017 - lr: 2.3988e-04\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.1557 - accuracy: 0.9367 - val_loss: 0.4946 - val_accuracy: 0.7960 - lr: 2.4831e-04\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.2831 - accuracy: 0.8805 - val_loss: 0.2638 - val_accuracy: 0.8645 - lr: 2.5704e-04\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 21s 168ms/step - loss: 0.1550 - accuracy: 0.9390 - val_loss: 0.3566 - val_accuracy: 0.8377 - lr: 2.6607e-04\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 21s 166ms/step - loss: 0.1758 - accuracy: 0.9264 - val_loss: 0.2970 - val_accuracy: 0.8504 - lr: 2.7542e-04\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 21s 169ms/step - loss: 0.3571 - accuracy: 0.8527 - val_loss: 0.4807 - val_accuracy: 0.7939 - lr: 2.8510e-04\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.2072 - accuracy: 0.9137 - val_loss: 0.4221 - val_accuracy: 0.7862 - lr: 2.9512e-04\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 21s 167ms/step - loss: 0.1814 - accuracy: 0.9256 - val_loss: 0.6102 - val_accuracy: 0.7706 - lr: 3.0549e-04\n"
     ]
    }
   ],
   "source": [
    "history_4a = model_4a.fit(\n",
    "    train_data_4a,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_data_4a,\n",
    "    verbose=1,\n",
    "    callbacks=[lr_scheduler,reduce_lr,checkpoint] # early_stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "773c6ba7-9a3f-47b1-ae9b-4d27dac60076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4a = tf.keras.models.load_model('models/classification_wbc4a_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ab6d4d12-ea1f-49a8-b28e-626ebe8e0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = [train_data_unshuffled, validation_data_unshuffled, test_data_unshuffled]\n",
    "data_4a = [train_data_unshuffled_4a, validation_data_unshuffled_4a, test_data_unshuffled_4a]\n",
    "\n",
    "model_results_4a = calc_performance(model_4a, data_4a, [0, 2, 3], original_data=data_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "bbf986d2-c86d-43dd-ae64-c5603a8d275d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.9277591973244147,\n",
       " 'validation': 0.8645980253878702,\n",
       " 'test': 0.88470066518847}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_4a['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9294d5f8-363d-4af6-a5f5-0734d1202a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         2         3\n",
      "0  0.867040  0.004005  0.128955\n",
      "2  0.001211  0.985876  0.012914\n",
      "3  0.059624  0.009604  0.930772\n",
      "\n",
      "          0         2         3\n",
      "0  0.826638  0.000000  0.173362\n",
      "2  0.000000  0.938298  0.061702\n",
      "3  0.143460  0.027426  0.829114\n",
      "\n",
      "          0         2         3\n",
      "0  0.853333  0.000000  0.146667\n",
      "2  0.006667  0.933333  0.060000\n",
      "3  0.126667  0.006667  0.866667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in model_results_4a['confusion_matrix']:\n",
    "    print(model_results_4a['confusion_matrix'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979ec1f-048f-401b-a25d-7177b2f47606",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">Here, we have no doubt that the ability to find class 2 is more easily separable than the others.  This leaves us with just classes 0 and 3, from which we can build a binary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "47921395-4e09-4dfb-81a0-ebb0c1009e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4996 images belonging to 2 classes.\n",
      "Found 947 images belonging to 2 classes.\n",
      "Found 4996 images belonging to 2 classes.\n",
      "Found 947 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# flow from directory using only the labels 0,3\n",
    "categories_4b = ['EOSINOPHIL', 'NEUTROPHIL']\n",
    "\n",
    "train_data_4b = flow_data(training_datagen, DATA_PATH, 'train', classes=categories_4b, class_mode='binary')\n",
    "validation_data_4b = flow_data(test_val_datagen, DATA_PATH, 'validation', classes=categories_4b, class_mode='binary')\n",
    "train_data_unshuffled_4b = flow_data(training_datagen, DATA_PATH, 'train', shuffle=False, classes=categories_4b, class_mode='binary')\n",
    "validation_data_unshuffled_4b = flow_data(test_val_datagen, DATA_PATH, 'validation', shuffle=False, classes=categories_4b, class_mode='binary')\n",
    "test_data_unshuffled_4b = flow_data(test_val_datagen, DATA_PATH, 'test', shuffle=False, classes=categories_4b, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "83108a85-d3aa-421f-8855-72c261d6464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4b = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(), # tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "50aab1f4-f690-4dfc-958c-410fe036e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4b.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(), # 'rmsprop',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "109004bf-3e97-4e9d-b6ba-b87ec3c384d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with the maximum validation accuracy \n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/classification_wbc4b_best.h5',\n",
    "    monitor='val_accuracy',\n",
    "    verbose=0,\n",
    "    mode='max', \n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "fbc7496f-8f67-488f-a040-bca878fcec99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.6933 - accuracy: 0.4976 - val_loss: 0.6933 - val_accuracy: 0.5005 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.6931 - accuracy: 0.5004 - val_loss: 0.6932 - val_accuracy: 0.4952 - lr: 1.0351e-05\n",
      "Epoch 3/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6930 - accuracy: 0.5070 - val_loss: 0.6935 - val_accuracy: 0.5005 - lr: 1.0715e-05\n",
      "Epoch 4/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6931 - accuracy: 0.4974 - val_loss: 0.6932 - val_accuracy: 0.5005 - lr: 1.1092e-05\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.6931 - accuracy: 0.5016 - val_loss: 0.6933 - val_accuracy: 0.5005 - lr: 1.1482e-05\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6925 - accuracy: 0.5102 - val_loss: 0.6931 - val_accuracy: 0.4995 - lr: 1.1885e-05\n",
      "Epoch 7/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.6925 - accuracy: 0.5158 - val_loss: 0.6930 - val_accuracy: 0.5005 - lr: 1.2303e-05\n",
      "Epoch 8/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.6925 - accuracy: 0.5020 - val_loss: 0.6928 - val_accuracy: 0.5259 - lr: 1.2735e-05\n",
      "Epoch 9/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.6923 - accuracy: 0.5370 - val_loss: 0.6928 - val_accuracy: 0.4963 - lr: 1.3183e-05\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.6919 - accuracy: 0.5316 - val_loss: 0.6930 - val_accuracy: 0.5026 - lr: 1.3646e-05\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.6914 - accuracy: 0.5222 - val_loss: 0.6925 - val_accuracy: 0.5026 - lr: 1.4125e-05\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.6918 - accuracy: 0.5102 - val_loss: 0.6928 - val_accuracy: 0.5048 - lr: 1.4622e-05\n",
      "Epoch 13/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6911 - accuracy: 0.5456 - val_loss: 0.6922 - val_accuracy: 0.5301 - lr: 1.5136e-05\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.6907 - accuracy: 0.5356 - val_loss: 0.6931 - val_accuracy: 0.4995 - lr: 1.5668e-05\n",
      "Epoch 15/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.6911 - accuracy: 0.5376 - val_loss: 0.6916 - val_accuracy: 0.5312 - lr: 1.6218e-05\n",
      "Epoch 16/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.6895 - accuracy: 0.5390 - val_loss: 0.6911 - val_accuracy: 0.5385 - lr: 1.6788e-05\n",
      "Epoch 17/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6898 - accuracy: 0.5356 - val_loss: 0.6915 - val_accuracy: 0.5100 - lr: 1.7378e-05\n",
      "Epoch 18/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6892 - accuracy: 0.5360 - val_loss: 0.6904 - val_accuracy: 0.5238 - lr: 1.7989e-05\n",
      "Epoch 19/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.6875 - accuracy: 0.5835 - val_loss: 0.6895 - val_accuracy: 0.5301 - lr: 1.8621e-05\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.6867 - accuracy: 0.5456 - val_loss: 0.6882 - val_accuracy: 0.5681 - lr: 1.9275e-05\n",
      "Epoch 21/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.6846 - accuracy: 0.5827 - val_loss: 0.6867 - val_accuracy: 0.5607 - lr: 1.9953e-05\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.6831 - accuracy: 0.5679 - val_loss: 0.6885 - val_accuracy: 0.5185 - lr: 2.0654e-05\n",
      "Epoch 23/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.6800 - accuracy: 0.5853 - val_loss: 0.6822 - val_accuracy: 0.5755 - lr: 2.1380e-05\n",
      "Epoch 24/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6777 - accuracy: 0.5945 - val_loss: 0.6798 - val_accuracy: 0.5818 - lr: 2.2131e-05\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6715 - accuracy: 0.6163 - val_loss: 0.6823 - val_accuracy: 0.5491 - lr: 2.2909e-05\n",
      "Epoch 26/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6669 - accuracy: 0.6079 - val_loss: 0.6754 - val_accuracy: 0.5892 - lr: 2.3714e-05\n",
      "Epoch 27/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6659 - accuracy: 0.5989 - val_loss: 0.6711 - val_accuracy: 0.5977 - lr: 2.4547e-05\n",
      "Epoch 28/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.6523 - accuracy: 0.6487 - val_loss: 0.6635 - val_accuracy: 0.5987 - lr: 2.5410e-05\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - 13s 150ms/step - loss: 0.6460 - accuracy: 0.6343 - val_loss: 0.6642 - val_accuracy: 0.5956 - lr: 2.6303e-05\n",
      "Epoch 30/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.6395 - accuracy: 0.6551 - val_loss: 0.6561 - val_accuracy: 0.6019 - lr: 2.7227e-05\n",
      "Epoch 31/100\n",
      "84/84 [==============================] - 13s 157ms/step - loss: 0.6255 - accuracy: 0.6661 - val_loss: 0.6639 - val_accuracy: 0.6103 - lr: 2.8184e-05\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.6108 - accuracy: 0.6869 - val_loss: 0.6480 - val_accuracy: 0.6177 - lr: 2.9174e-05\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.6101 - accuracy: 0.6787 - val_loss: 0.6324 - val_accuracy: 0.6431 - lr: 3.0200e-05\n",
      "Epoch 34/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.5854 - accuracy: 0.7106 - val_loss: 0.6246 - val_accuracy: 0.6420 - lr: 3.1261e-05\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.5718 - accuracy: 0.7192 - val_loss: 0.6234 - val_accuracy: 0.6653 - lr: 3.2359e-05\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.5683 - accuracy: 0.7180 - val_loss: 0.6096 - val_accuracy: 0.6790 - lr: 3.3497e-05\n",
      "Epoch 37/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.5542 - accuracy: 0.7306 - val_loss: 0.5998 - val_accuracy: 0.6769 - lr: 3.4674e-05\n",
      "Epoch 38/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.5303 - accuracy: 0.7454 - val_loss: 0.6003 - val_accuracy: 0.6864 - lr: 3.5892e-05\n",
      "Epoch 39/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.5252 - accuracy: 0.7424 - val_loss: 0.5802 - val_accuracy: 0.7149 - lr: 3.7154e-05\n",
      "Epoch 40/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.5171 - accuracy: 0.7480 - val_loss: 0.5661 - val_accuracy: 0.7107 - lr: 3.8459e-05\n",
      "Epoch 41/100\n",
      "84/84 [==============================] - 13s 155ms/step - loss: 0.5013 - accuracy: 0.7594 - val_loss: 0.6150 - val_accuracy: 0.6748 - lr: 3.9811e-05\n",
      "Epoch 42/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.4764 - accuracy: 0.7726 - val_loss: 0.5450 - val_accuracy: 0.7487 - lr: 4.1210e-05\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.4711 - accuracy: 0.7774 - val_loss: 0.5194 - val_accuracy: 0.7592 - lr: 4.2658e-05\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.4529 - accuracy: 0.7890 - val_loss: 0.5515 - val_accuracy: 0.6906 - lr: 4.4157e-05\n",
      "Epoch 45/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.4305 - accuracy: 0.8102 - val_loss: 0.5061 - val_accuracy: 0.7698 - lr: 4.5709e-05\n",
      "Epoch 46/100\n",
      "84/84 [==============================] - 15s 174ms/step - loss: 0.4260 - accuracy: 0.8084 - val_loss: 0.5110 - val_accuracy: 0.7476 - lr: 4.7315e-05\n",
      "Epoch 47/100\n",
      "84/84 [==============================] - 14s 161ms/step - loss: 0.4049 - accuracy: 0.8149 - val_loss: 0.5392 - val_accuracy: 0.6917 - lr: 4.8978e-05\n",
      "Epoch 48/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.3956 - accuracy: 0.8241 - val_loss: 0.4767 - val_accuracy: 0.7709 - lr: 5.0699e-05\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - 13s 159ms/step - loss: 0.3884 - accuracy: 0.8263 - val_loss: 0.4831 - val_accuracy: 0.7550 - lr: 5.2481e-05\n",
      "Epoch 50/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.3881 - accuracy: 0.8271 - val_loss: 0.4580 - val_accuracy: 0.7814 - lr: 5.4325e-05\n",
      "Epoch 51/100\n",
      "84/84 [==============================] - 13s 155ms/step - loss: 0.3628 - accuracy: 0.8383 - val_loss: 0.4716 - val_accuracy: 0.7698 - lr: 5.6234e-05\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.3633 - accuracy: 0.8347 - val_loss: 0.5874 - val_accuracy: 0.7265 - lr: 5.8210e-05\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.3700 - accuracy: 0.8325 - val_loss: 0.4947 - val_accuracy: 0.7466 - lr: 6.0256e-05\n",
      "Epoch 54/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.3628 - accuracy: 0.8313 - val_loss: 0.4539 - val_accuracy: 0.8004 - lr: 6.2373e-05\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - 13s 159ms/step - loss: 0.3419 - accuracy: 0.8473 - val_loss: 0.4852 - val_accuracy: 0.7624 - lr: 6.4565e-05\n",
      "Epoch 56/100\n",
      "84/84 [==============================] - 13s 158ms/step - loss: 0.3339 - accuracy: 0.8469 - val_loss: 0.4192 - val_accuracy: 0.8057 - lr: 6.6834e-05\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - 13s 157ms/step - loss: 0.3249 - accuracy: 0.8537 - val_loss: 0.4342 - val_accuracy: 0.7825 - lr: 6.9183e-05\n",
      "Epoch 58/100\n",
      "84/84 [==============================] - 13s 158ms/step - loss: 0.3437 - accuracy: 0.8407 - val_loss: 0.4478 - val_accuracy: 0.7761 - lr: 7.1614e-05\n",
      "Epoch 59/100\n",
      "84/84 [==============================] - 13s 158ms/step - loss: 0.3315 - accuracy: 0.8541 - val_loss: 0.4956 - val_accuracy: 0.7540 - lr: 7.4131e-05\n",
      "Epoch 60/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.3087 - accuracy: 0.8645 - val_loss: 0.4627 - val_accuracy: 0.7793 - lr: 7.6736e-05\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.3148 - accuracy: 0.8585 - val_loss: 0.4546 - val_accuracy: 0.7687 - lr: 7.9433e-05\n",
      "Epoch 62/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.2898 - accuracy: 0.8769 - val_loss: 0.4187 - val_accuracy: 0.7962 - lr: 8.2224e-05\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.2936 - accuracy: 0.8739 - val_loss: 0.4768 - val_accuracy: 0.7635 - lr: 8.5114e-05\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - 14s 164ms/step - loss: 0.2872 - accuracy: 0.8749 - val_loss: 0.4289 - val_accuracy: 0.7878 - lr: 8.8105e-05\n",
      "Epoch 65/100\n",
      "84/84 [==============================] - 14s 161ms/step - loss: 0.2750 - accuracy: 0.8811 - val_loss: 0.4342 - val_accuracy: 0.7804 - lr: 9.1201e-05\n",
      "Epoch 66/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8719\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 9.440608846489341e-06.\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.2969 - accuracy: 0.8719 - val_loss: 0.4056 - val_accuracy: 0.7951 - lr: 9.4406e-06\n",
      "Epoch 67/100\n",
      "84/84 [==============================] - 14s 163ms/step - loss: 0.2987 - accuracy: 0.8629 - val_loss: 0.4255 - val_accuracy: 0.7846 - lr: 9.7724e-05\n",
      "Epoch 68/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.2752 - accuracy: 0.8823 - val_loss: 0.4268 - val_accuracy: 0.7825 - lr: 1.0116e-04\n",
      "Epoch 69/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.2610 - accuracy: 0.8909 - val_loss: 0.4239 - val_accuracy: 0.7909 - lr: 1.0471e-04\n",
      "Epoch 70/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.2591 - accuracy: 0.8883 - val_loss: 0.4621 - val_accuracy: 0.7804 - lr: 1.0839e-04\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.2808 - accuracy: 0.8745 - val_loss: 0.5403 - val_accuracy: 0.7149 - lr: 1.1220e-04\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.2499 - accuracy: 0.8933 - val_loss: 0.4635 - val_accuracy: 0.7825 - lr: 1.1614e-04\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - 13s 151ms/step - loss: 0.2419 - accuracy: 0.8933 - val_loss: 0.4153 - val_accuracy: 0.7835 - lr: 1.2023e-04\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - 18s 215ms/step - loss: 0.2633 - accuracy: 0.8843 - val_loss: 0.4074 - val_accuracy: 0.7951 - lr: 1.2445e-04\n",
      "Epoch 75/100\n",
      "84/84 [==============================] - 15s 175ms/step - loss: 0.2351 - accuracy: 0.9011 - val_loss: 0.4373 - val_accuracy: 0.7825 - lr: 1.2882e-04\n",
      "Epoch 76/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.8741\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 1.3335215044207871e-05.\n",
      "84/84 [==============================] - 15s 176ms/step - loss: 0.2839 - accuracy: 0.8741 - val_loss: 0.3870 - val_accuracy: 0.7983 - lr: 1.3335e-05\n",
      "Epoch 77/100\n",
      "84/84 [==============================] - 14s 165ms/step - loss: 0.2278 - accuracy: 0.9081 - val_loss: 0.4378 - val_accuracy: 0.7867 - lr: 1.3804e-04\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.2439 - accuracy: 0.8903 - val_loss: 0.4128 - val_accuracy: 0.7909 - lr: 1.4289e-04\n",
      "Epoch 79/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.2172 - accuracy: 0.9087 - val_loss: 0.4467 - val_accuracy: 0.7846 - lr: 1.4791e-04\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.2319 - accuracy: 0.9015 - val_loss: 0.4610 - val_accuracy: 0.7656 - lr: 1.5311e-04\n",
      "Epoch 81/100\n",
      "84/84 [==============================] - 13s 155ms/step - loss: 0.2134 - accuracy: 0.9121 - val_loss: 0.4541 - val_accuracy: 0.7793 - lr: 1.5849e-04\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - 14s 167ms/step - loss: 0.2380 - accuracy: 0.8971 - val_loss: 0.4690 - val_accuracy: 0.8036 - lr: 1.6406e-04\n",
      "Epoch 83/100\n",
      "84/84 [==============================] - 13s 154ms/step - loss: 0.2611 - accuracy: 0.8837 - val_loss: 0.4606 - val_accuracy: 0.7962 - lr: 1.6982e-04\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.2127 - accuracy: 0.9083 - val_loss: 0.5549 - val_accuracy: 0.7687 - lr: 1.7579e-04\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - 14s 171ms/step - loss: 0.2556 - accuracy: 0.8887 - val_loss: 0.4254 - val_accuracy: 0.7994 - lr: 1.8197e-04\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.8889\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 1.8836490926332772e-05.\n",
      "84/84 [==============================] - 14s 164ms/step - loss: 0.2448 - accuracy: 0.8889 - val_loss: 0.3957 - val_accuracy: 0.7973 - lr: 1.8836e-05\n",
      "Epoch 87/100\n",
      "84/84 [==============================] - 14s 169ms/step - loss: 0.2100 - accuracy: 0.9091 - val_loss: 0.4842 - val_accuracy: 0.7899 - lr: 1.9498e-04\n",
      "Epoch 88/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.1762 - accuracy: 0.9289 - val_loss: 0.4909 - val_accuracy: 0.7941 - lr: 2.0184e-04\n",
      "Epoch 89/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.1833 - accuracy: 0.9227 - val_loss: 0.4499 - val_accuracy: 0.7973 - lr: 2.0893e-04\n",
      "Epoch 90/100\n",
      "84/84 [==============================] - 14s 162ms/step - loss: 0.2289 - accuracy: 0.8955 - val_loss: 0.4499 - val_accuracy: 0.7846 - lr: 2.1627e-04\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - 14s 160ms/step - loss: 0.2222 - accuracy: 0.9035 - val_loss: 0.4469 - val_accuracy: 0.7951 - lr: 2.2387e-04\n",
      "Epoch 92/100\n",
      "84/84 [==============================] - 14s 166ms/step - loss: 0.1960 - accuracy: 0.9195 - val_loss: 0.5037 - val_accuracy: 0.7740 - lr: 2.3174e-04\n",
      "Epoch 93/100\n",
      "84/84 [==============================] - 14s 161ms/step - loss: 0.2064 - accuracy: 0.9159 - val_loss: 0.4429 - val_accuracy: 0.8057 - lr: 2.3988e-04\n",
      "Epoch 94/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.1925 - accuracy: 0.9191 - val_loss: 0.5705 - val_accuracy: 0.7508 - lr: 2.4831e-04\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - 13s 156ms/step - loss: 0.1812 - accuracy: 0.9231 - val_loss: 0.4553 - val_accuracy: 0.8141 - lr: 2.5704e-04\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.1879 - accuracy: 0.9245 - val_loss: 0.5273 - val_accuracy: 0.8089 - lr: 2.6607e-04\n",
      "Epoch 97/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.1535 - accuracy: 0.9384 - val_loss: 0.5943 - val_accuracy: 0.8046 - lr: 2.7542e-04\n",
      "Epoch 98/100\n",
      "84/84 [==============================] - 13s 152ms/step - loss: 0.1666 - accuracy: 0.9303 - val_loss: 0.5485 - val_accuracy: 0.7941 - lr: 2.8510e-04\n",
      "Epoch 99/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.1640 - accuracy: 0.9313 - val_loss: 0.5862 - val_accuracy: 0.7656 - lr: 2.9512e-04\n",
      "Epoch 100/100\n",
      "84/84 [==============================] - 13s 153ms/step - loss: 0.2541 - accuracy: 0.8915 - val_loss: 0.4873 - val_accuracy: 0.8078 - lr: 3.0549e-04\n"
     ]
    }
   ],
   "source": [
    "history_4b = model_4b.fit(\n",
    "    train_data_4b,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_data_4b,\n",
    "    verbose=1,\n",
    "    callbacks=[reduce_lr,lr_scheduler,checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "55af867b-fbef-4423-a1a2-5621fe1483d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4b = tf.keras.models.load_model('models/classification_wbc4b_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b24894e7-d480-43b8-8eb1-8d0dadf148b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0.9221688675470188,\n",
       " 'validation': 0.8145416227608009,\n",
       " 'test': 0.8145695364238411}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_4b = calc_performance(model_4b, [train_data_unshuffled_4b, validation_data_unshuffled_4b, test_data_unshuffled_4b], [0, 3])\n",
    "model_results_4b['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "30211d21-3d41-452d-8891-8938911bbd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         3\n",
      "0  0.875050  0.124950\n",
      "3  0.030812  0.969188\n",
      "\n",
      "          0         3\n",
      "0  0.693446  0.306554\n",
      "3  0.065401  0.934599\n",
      "\n",
      "          0         3\n",
      "0  0.700000  0.300000\n",
      "3  0.073333  0.926667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in model_results_4b['confusion_matrix']:\n",
    "    print(model_results_4b['confusion_matrix'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617edef6-abe3-4c3d-b007-c202a55d8f15",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">We can now calculate a combined probability, allowing a model's predictions to stand for the least 'confusing' class in its matrix, and moving on to the next model otherwise, unti, the final two classes use the binary model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c40a2c2a-7aab-41ec-b22c-3b2112fbc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_1 = [np.argmax(p) for p in model_1.predict(test_data_unshuffled, verbose=0)] # classes 0,1,2,3\n",
    "preds_4a = [np.argmax(p) for p in model_4a.predict(test_data_unshuffled, verbose=0)] # classes 0,2,3 (2 is at index 1)\n",
    "preds_4b = [int(np.round(p)) for p in model_4b.predict(test_data_unshuffled, verbose=0)] # classes 0,3 (3 is at index 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f003cd57-bbac-4194-b59c-ad1c2f43aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_test_combined_4 = [1 if preds_1[i] == 1 else 2 if preds_4a[i] == 1 else 3*preds_4b[i] for i in range(len(preds_1))]\n",
    "combined_accuracy_4 = np.mean([1 if classes_test_combined_4[i]==test_data_unshuffled.classes[i] else 0 for i in range(len(preds_1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "5c435852-95fd-4d56-bdb8-282a60b03586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_accuracy_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "8e20879d-e40f-41be-81b8-1af78e9b772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.700000  0.000000  0.000000  0.300000\n",
      "1  0.000000  0.966667  0.026667  0.006667\n",
      "2  0.000000  0.000000  0.933333  0.066667\n",
      "3  0.073333  0.000000  0.006667  0.920000\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_data_unshuffled.classes, classes_test_combined_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f961d2f3-67b7-4e39-8b44-6851aca72876",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Using the 128-pixel resolution, the method of binary predictions is still not decisive enough in separating classes 0 and 3 but has shown gains elsewhere.\n",
    "<p style=\"font-weight: 500; color: #556;\">As such, we can combine these results further with our higher-resolution model (model_2), to improve our precision for class 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "1aebdba9-b436-4908-b9c7-4e2263fac6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_test_combined_2_4 = [0 if classes_test_combined_2[i] == 0 else classes_test_combined_4[i] for i in range(len(preds_1))]\n",
    "combined_accuracy_2_4 = np.mean([1 if classes_test_combined_2_4[i]==test_data_unshuffled.classes[i] else 0 for i in range(len(preds_1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "b5d7c245-9a47-45c7-b1d0-02cfea55cead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.905"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_accuracy_2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "25e3b8f0-16e7-4cd1-aad6-8a4dd2d90913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.886667  0.000000  0.000000  0.113333\n",
      "1  0.000000  0.966667  0.026667  0.006667\n",
      "2  0.006667  0.000000  0.933333  0.060000\n",
      "3  0.160000  0.000000  0.006667  0.833333\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_data_unshuffled.classes, classes_test_combined_2_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47847e90-57c4-462a-90e3-915222013444",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Superb! We've now surpassed 90% accuracy for our test set with the right mix of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11292e3-346c-4f67-b824-fb4b9e5834d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "####\n",
    "#### **FULL SET OF BINARY MODELS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb1c85-3734-4749-adc2-5780d4c8607d",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">Here, we will attempt to treat each of our 4 classes in a 'one vs rest' fashion, building a binary model for all of them, and allowing the model with the highest score to be the winner for each test case:get_file_paths()\n",
    "<p style=\"font-weight: 500; color: #556;\">(Note: this was written using a 2-class one-hot label setup with softmax activation and a categorical crossentropy loss.  How this affects performance from actually using binary crossentropy, is yet to be tested.  This is a potential evolution for the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3f1b8536-f6ca-46be-9dbe-02737afa5059",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5693 - accuracy: 0.7446\n",
      "Epoch 1: val_accuracy improved from -inf to 0.74934, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 30s 177ms/step - loss: 0.5693 - accuracy: 0.7446 - val_loss: 0.5641 - val_accuracy: 0.7493 - lr: 5.0000e-05\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5622 - accuracy: 0.7492\n",
      "Epoch 2: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.5622 - accuracy: 0.7492 - val_loss: 0.5651 - val_accuracy: 0.7493 - lr: 5.1757e-05\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5617 - accuracy: 0.7492\n",
      "Epoch 3: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5617 - accuracy: 0.7492 - val_loss: 0.5631 - val_accuracy: 0.7493 - lr: 5.3576e-05\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5606 - accuracy: 0.7492\n",
      "Epoch 4: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5606 - accuracy: 0.7492 - val_loss: 0.5628 - val_accuracy: 0.7493 - lr: 5.5459e-05\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.7492\n",
      "Epoch 5: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.5592 - accuracy: 0.7492 - val_loss: 0.5578 - val_accuracy: 0.7493 - lr: 5.7408e-05\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5573 - accuracy: 0.7492\n",
      "Epoch 6: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 0.5573 - accuracy: 0.7492 - val_loss: 0.5482 - val_accuracy: 0.7493 - lr: 5.9425e-05\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5531 - accuracy: 0.7492\n",
      "Epoch 7: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.5531 - accuracy: 0.7492 - val_loss: 0.5385 - val_accuracy: 0.7493 - lr: 6.1513e-05\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5386 - accuracy: 0.7492\n",
      "Epoch 8: val_accuracy did not improve from 0.74934\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5386 - accuracy: 0.7492 - val_loss: 0.4923 - val_accuracy: 0.7493 - lr: 6.3675e-05\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.7492\n",
      "Epoch 9: val_accuracy improved from 0.74934 to 0.74987, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5143 - accuracy: 0.7492 - val_loss: 0.4585 - val_accuracy: 0.7499 - lr: 6.5913e-05\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.7604\n",
      "Epoch 10: val_accuracy improved from 0.74987 to 0.75146, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 30s 178ms/step - loss: 0.4915 - accuracy: 0.7604 - val_loss: 0.4415 - val_accuracy: 0.7515 - lr: 6.8229e-05\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4755 - accuracy: 0.7730\n",
      "Epoch 11: val_accuracy improved from 0.75146 to 0.80604, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 30s 178ms/step - loss: 0.4755 - accuracy: 0.7730 - val_loss: 0.4254 - val_accuracy: 0.8060 - lr: 7.0627e-05\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4494 - accuracy: 0.7882\n",
      "Epoch 12: val_accuracy did not improve from 0.80604\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.4494 - accuracy: 0.7882 - val_loss: 0.3955 - val_accuracy: 0.7806 - lr: 7.3109e-05\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.7930\n",
      "Epoch 13: val_accuracy improved from 0.80604 to 0.82830, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.4296 - accuracy: 0.7930 - val_loss: 0.4262 - val_accuracy: 0.8283 - lr: 7.5678e-05\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4054 - accuracy: 0.8097\n",
      "Epoch 14: val_accuracy improved from 0.82830 to 0.83201, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.4054 - accuracy: 0.8097 - val_loss: 0.4146 - val_accuracy: 0.8320 - lr: 7.8338e-05\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3936 - accuracy: 0.8179\n",
      "Epoch 15: val_accuracy improved from 0.83201 to 0.86539, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.3936 - accuracy: 0.8179 - val_loss: 0.3516 - val_accuracy: 0.8654 - lr: 8.1091e-05\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8271\n",
      "Epoch 16: val_accuracy improved from 0.86539 to 0.88341, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.3752 - accuracy: 0.8271 - val_loss: 0.2990 - val_accuracy: 0.8834 - lr: 8.3940e-05\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8426\n",
      "Epoch 17: val_accuracy did not improve from 0.88341\n",
      "166/166 [==============================] - 31s 183ms/step - loss: 0.3444 - accuracy: 0.8426 - val_loss: 0.3086 - val_accuracy: 0.8824 - lr: 8.6890e-05\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8515\n",
      "Epoch 18: val_accuracy did not improve from 0.88341\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.3102 - accuracy: 0.8515 - val_loss: 0.3117 - val_accuracy: 0.8580 - lr: 8.9944e-05\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2919 - accuracy: 0.8682\n",
      "Epoch 19: val_accuracy did not improve from 0.88341\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.2919 - accuracy: 0.8682 - val_loss: 0.2920 - val_accuracy: 0.8824 - lr: 9.3104e-05\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.8811\n",
      "Epoch 20: val_accuracy improved from 0.88341 to 0.89083, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.2615 - accuracy: 0.8811 - val_loss: 0.2259 - val_accuracy: 0.8908 - lr: 9.6376e-05\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2372 - accuracy: 0.8922\n",
      "Epoch 21: val_accuracy did not improve from 0.89083\n",
      "166/166 [==============================] - 29s 176ms/step - loss: 0.2372 - accuracy: 0.8922 - val_loss: 0.2683 - val_accuracy: 0.8617 - lr: 9.9763e-05\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.8997\n",
      "Epoch 22: val_accuracy improved from 0.89083 to 0.90991, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.2299 - accuracy: 0.8997 - val_loss: 0.2060 - val_accuracy: 0.9099 - lr: 1.0327e-04\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2007 - accuracy: 0.9108\n",
      "Epoch 23: val_accuracy did not improve from 0.90991\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2007 - accuracy: 0.9108 - val_loss: 0.1953 - val_accuracy: 0.9089 - lr: 1.0690e-04\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9067\n",
      "Epoch 24: val_accuracy did not improve from 0.90991\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.2110 - accuracy: 0.9067 - val_loss: 0.2580 - val_accuracy: 0.9073 - lr: 1.1065e-04\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1818 - accuracy: 0.9212\n",
      "Epoch 25: val_accuracy improved from 0.90991 to 0.91574, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.1818 - accuracy: 0.9212 - val_loss: 0.1745 - val_accuracy: 0.9157 - lr: 1.1454e-04\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1789 - accuracy: 0.9229\n",
      "Epoch 26: val_accuracy improved from 0.91574 to 0.92051, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1789 - accuracy: 0.9229 - val_loss: 0.1629 - val_accuracy: 0.9205 - lr: 1.1857e-04\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9245\n",
      "Epoch 27: val_accuracy did not improve from 0.92051\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.1713 - accuracy: 0.9245 - val_loss: 0.1891 - val_accuracy: 0.9057 - lr: 1.2274e-04\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9362\n",
      "Epoch 28: val_accuracy did not improve from 0.92051\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1565 - accuracy: 0.9362 - val_loss: 0.1888 - val_accuracy: 0.9126 - lr: 1.2705e-04\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1600 - accuracy: 0.9331\n",
      "Epoch 29: val_accuracy did not improve from 0.92051\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1600 - accuracy: 0.9331 - val_loss: 0.1803 - val_accuracy: 0.9157 - lr: 1.3151e-04\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9349\n",
      "Epoch 30: val_accuracy improved from 0.92051 to 0.92422, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1546 - accuracy: 0.9349 - val_loss: 0.1657 - val_accuracy: 0.9242 - lr: 1.3614e-04\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9485\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 1.4091914636082947e-05.\n",
      "\n",
      "Epoch 31: val_accuracy improved from 0.92422 to 0.92581, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1258 - accuracy: 0.9485 - val_loss: 0.1860 - val_accuracy: 0.9258 - lr: 1.4092e-05\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9426\n",
      "Epoch 32: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1426 - accuracy: 0.9426 - val_loss: 0.1736 - val_accuracy: 0.9189 - lr: 1.4587e-04\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9387\n",
      "Epoch 33: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.1460 - accuracy: 0.9387 - val_loss: 0.1724 - val_accuracy: 0.9083 - lr: 1.5100e-04\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9583\n",
      "Epoch 34: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.1073 - accuracy: 0.9583 - val_loss: 0.1735 - val_accuracy: 0.9163 - lr: 1.5630e-04\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1243 - accuracy: 0.9505\n",
      "Epoch 35: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1243 - accuracy: 0.9505 - val_loss: 0.2062 - val_accuracy: 0.9078 - lr: 1.6180e-04\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1061 - accuracy: 0.9582\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.674827217357233e-05.\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.1061 - accuracy: 0.9582 - val_loss: 0.2222 - val_accuracy: 0.9104 - lr: 1.6748e-05\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9532\n",
      "Epoch 37: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1200 - accuracy: 0.9532 - val_loss: 0.1906 - val_accuracy: 0.9242 - lr: 1.7337e-04\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1056 - accuracy: 0.9591\n",
      "Epoch 38: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.1056 - accuracy: 0.9591 - val_loss: 0.1807 - val_accuracy: 0.9258 - lr: 1.7946e-04\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9560\n",
      "Epoch 39: val_accuracy did not improve from 0.92581\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1068 - accuracy: 0.9560 - val_loss: 0.3059 - val_accuracy: 0.8797 - lr: 1.8577e-04\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9608\n",
      "Epoch 40: val_accuracy improved from 0.92581 to 0.93217, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0981 - accuracy: 0.9608 - val_loss: 0.2019 - val_accuracy: 0.9322 - lr: 1.9230e-04\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9679\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.9905358203686774e-05.\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0868 - accuracy: 0.9679 - val_loss: 0.2279 - val_accuracy: 0.9216 - lr: 1.9905e-05\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9682\n",
      "Epoch 42: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0850 - accuracy: 0.9682 - val_loss: 0.2464 - val_accuracy: 0.9274 - lr: 2.0605e-04\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9666\n",
      "Epoch 43: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0846 - accuracy: 0.9666 - val_loss: 0.2238 - val_accuracy: 0.9306 - lr: 2.1329e-04\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9702\n",
      "Epoch 44: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0750 - accuracy: 0.9702 - val_loss: 0.1991 - val_accuracy: 0.9290 - lr: 2.2079e-04\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9633\n",
      "Epoch 45: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0948 - accuracy: 0.9633 - val_loss: 0.2548 - val_accuracy: 0.8967 - lr: 2.2854e-04\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9657\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 2.3657562269363553e-05.\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0870 - accuracy: 0.9657 - val_loss: 0.1681 - val_accuracy: 0.9285 - lr: 2.3658e-05\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9656\n",
      "Epoch 47: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0883 - accuracy: 0.9656 - val_loss: 0.2046 - val_accuracy: 0.9237 - lr: 2.4489e-04\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9741\n",
      "Epoch 48: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0663 - accuracy: 0.9741 - val_loss: 0.2133 - val_accuracy: 0.9306 - lr: 2.5350e-04\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9714\n",
      "Epoch 49: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0736 - accuracy: 0.9714 - val_loss: 0.2113 - val_accuracy: 0.9216 - lr: 2.6240e-04\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9726\n",
      "Epoch 50: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0719 - accuracy: 0.9726 - val_loss: 0.2017 - val_accuracy: 0.9205 - lr: 2.7163e-04\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9692\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.8117065085098147e-05.\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0775 - accuracy: 0.9692 - val_loss: 0.2201 - val_accuracy: 0.9279 - lr: 2.8117e-05\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9664\n",
      "Epoch 52: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0882 - accuracy: 0.9664 - val_loss: 0.2310 - val_accuracy: 0.9253 - lr: 2.9105e-04\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9714\n",
      "Epoch 53: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0729 - accuracy: 0.9714 - val_loss: 0.2120 - val_accuracy: 0.9258 - lr: 3.0128e-04\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9693\n",
      "Epoch 54: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0786 - accuracy: 0.9693 - val_loss: 0.2287 - val_accuracy: 0.9173 - lr: 3.1187e-04\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9725\n",
      "Epoch 55: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0739 - accuracy: 0.9725 - val_loss: 0.2485 - val_accuracy: 0.9004 - lr: 3.2283e-04\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9760\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.341719566378743e-05.\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0643 - accuracy: 0.9760 - val_loss: 0.1966 - val_accuracy: 0.9258 - lr: 3.3417e-05\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9766\n",
      "Epoch 57: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0639 - accuracy: 0.9766 - val_loss: 0.2695 - val_accuracy: 0.9253 - lr: 3.4592e-04\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9694\n",
      "Epoch 58: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0771 - accuracy: 0.9694 - val_loss: 0.2659 - val_accuracy: 0.9306 - lr: 3.5807e-04\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9782\n",
      "Epoch 59: val_accuracy did not improve from 0.93217\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0606 - accuracy: 0.9782 - val_loss: 0.2144 - val_accuracy: 0.9322 - lr: 3.7066e-04\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9777\n",
      "Epoch 60: val_accuracy improved from 0.93217 to 0.93482, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0581 - accuracy: 0.9777 - val_loss: 0.1845 - val_accuracy: 0.9348 - lr: 3.8368e-04\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9794\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 3.971641126554459e-05.\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0520 - accuracy: 0.9794 - val_loss: 0.2455 - val_accuracy: 0.9269 - lr: 3.9716e-05\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9697\n",
      "Epoch 62: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0818 - accuracy: 0.9697 - val_loss: 0.2168 - val_accuracy: 0.9322 - lr: 4.1112e-04\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9760\n",
      "Epoch 63: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0638 - accuracy: 0.9760 - val_loss: 0.2278 - val_accuracy: 0.9322 - lr: 4.2557e-04\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9722\n",
      "Epoch 64: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0722 - accuracy: 0.9722 - val_loss: 0.2090 - val_accuracy: 0.9269 - lr: 4.4052e-04\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9670\n",
      "Epoch 65: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0883 - accuracy: 0.9670 - val_loss: 0.2118 - val_accuracy: 0.9338 - lr: 4.5601e-04\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9786\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 4.720304277725518e-05.\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.93482\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0596 - accuracy: 0.9786 - val_loss: 0.3494 - val_accuracy: 0.9295 - lr: 4.7203e-05\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9775\n",
      "Epoch 67: val_accuracy improved from 0.93482 to 0.93588, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0593 - accuracy: 0.9775 - val_loss: 0.2583 - val_accuracy: 0.9359 - lr: 4.8862e-04\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9717\n",
      "Epoch 68: val_accuracy improved from 0.93588 to 0.94065, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0773 - accuracy: 0.9717 - val_loss: 0.1638 - val_accuracy: 0.9406 - lr: 5.0579e-04\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9823\n",
      "Epoch 69: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0513 - accuracy: 0.9823 - val_loss: 0.2444 - val_accuracy: 0.9295 - lr: 5.2356e-04\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9861\n",
      "Epoch 70: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0413 - accuracy: 0.9861 - val_loss: 0.3238 - val_accuracy: 0.9232 - lr: 5.4196e-04\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9787\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 5.6100921938195825e-05.\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0584 - accuracy: 0.9787 - val_loss: 0.3899 - val_accuracy: 0.8574 - lr: 5.6101e-05\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9799\n",
      "Epoch 72: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0535 - accuracy: 0.9799 - val_loss: 0.2308 - val_accuracy: 0.9279 - lr: 5.8072e-04\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9797\n",
      "Epoch 73: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0527 - accuracy: 0.9797 - val_loss: 0.2942 - val_accuracy: 0.9290 - lr: 6.0113e-04\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9869\n",
      "Epoch 74: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0371 - accuracy: 0.9869 - val_loss: 0.4061 - val_accuracy: 0.9338 - lr: 6.2226e-04\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9737\n",
      "Epoch 75: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0729 - accuracy: 0.9737 - val_loss: 0.2477 - val_accuracy: 0.9306 - lr: 6.4412e-04\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9685\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 6.66760723106563e-05.\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0866 - accuracy: 0.9685 - val_loss: 0.2131 - val_accuracy: 0.9269 - lr: 6.6676e-05\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9844\n",
      "Epoch 77: val_accuracy did not improve from 0.94065\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0452 - accuracy: 0.9844 - val_loss: 0.1948 - val_accuracy: 0.9385 - lr: 6.9019e-04\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9711\n",
      "Epoch 78: val_accuracy improved from 0.94065 to 0.94807, saving model to models/classification_wbc_binary_EOSINOPHIL_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.0802 - accuracy: 0.9711 - val_loss: 0.1804 - val_accuracy: 0.9481 - lr: 7.1445e-04\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0484 - accuracy: 0.9838\n",
      "Epoch 79: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0484 - accuracy: 0.9838 - val_loss: 0.2124 - val_accuracy: 0.9348 - lr: 7.3955e-04\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9890\n",
      "Epoch 80: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.2737 - val_accuracy: 0.9332 - lr: 7.6554e-04\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8810\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 7.924466044642032e-05.\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.3126 - accuracy: 0.8810 - val_loss: 0.5625 - val_accuracy: 0.7493 - lr: 7.9245e-05\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5377 - accuracy: 0.7492\n",
      "Epoch 82: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5377 - accuracy: 0.7492 - val_loss: 0.4285 - val_accuracy: 0.7493 - lr: 8.2029e-04\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4756 - accuracy: 0.7500\n",
      "Epoch 83: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.4756 - accuracy: 0.7500 - val_loss: 0.3819 - val_accuracy: 0.7541 - lr: 8.4912e-04\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4480 - accuracy: 0.7585\n",
      "Epoch 84: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.4480 - accuracy: 0.7585 - val_loss: 0.4201 - val_accuracy: 0.7944 - lr: 8.7896e-04\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3780 - accuracy: 0.8051\n",
      "Epoch 85: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.3780 - accuracy: 0.8051 - val_loss: 0.2934 - val_accuracy: 0.8638 - lr: 9.0985e-04\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.8515\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 9.418245172128082e-05.\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.3150 - accuracy: 0.8515 - val_loss: 0.3052 - val_accuracy: 0.8765 - lr: 9.4182e-05\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.8875\n",
      "Epoch 87: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.2477 - accuracy: 0.8875 - val_loss: 0.2595 - val_accuracy: 0.9295 - lr: 9.7492e-04\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9025\n",
      "Epoch 88: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.2219 - accuracy: 0.9025 - val_loss: 0.2069 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9365\n",
      "Epoch 89: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1560 - accuracy: 0.9365 - val_loss: 0.2511 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1425 - accuracy: 0.9435\n",
      "Epoch 90: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1425 - accuracy: 0.9435 - val_loss: 0.2984 - val_accuracy: 0.9157 - lr: 0.0011\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9455\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 0.00011193605605512858.\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1348 - accuracy: 0.9455 - val_loss: 0.3262 - val_accuracy: 0.9163 - lr: 1.1194e-04\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9620\n",
      "Epoch 92: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0945 - accuracy: 0.9620 - val_loss: 0.2454 - val_accuracy: 0.9179 - lr: 0.0012\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9578\n",
      "Epoch 93: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1071 - accuracy: 0.9578 - val_loss: 0.1988 - val_accuracy: 0.9205 - lr: 0.0012\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1085 - accuracy: 0.9546\n",
      "Epoch 94: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1085 - accuracy: 0.9546 - val_loss: 0.1915 - val_accuracy: 0.9269 - lr: 0.0012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9688\n",
      "Epoch 95: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0757 - accuracy: 0.9688 - val_loss: 0.2323 - val_accuracy: 0.9173 - lr: 0.0013\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9692\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 0.00013303625164553523.\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0797 - accuracy: 0.9692 - val_loss: 0.2179 - val_accuracy: 0.9232 - lr: 1.3304e-04\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9630\n",
      "Epoch 97: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0913 - accuracy: 0.9630 - val_loss: 0.2044 - val_accuracy: 0.9306 - lr: 0.0014\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9525\n",
      "Epoch 98: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1133 - accuracy: 0.9525 - val_loss: 0.2021 - val_accuracy: 0.9269 - lr: 0.0014\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9658\n",
      "Epoch 99: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0851 - accuracy: 0.9658 - val_loss: 0.2718 - val_accuracy: 0.9258 - lr: 0.0015\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0781 - accuracy: 0.9679\n",
      "Epoch 100: val_accuracy did not improve from 0.94807\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0781 - accuracy: 0.9679 - val_loss: 0.2647 - val_accuracy: 0.9232 - lr: 0.0015\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.7457\n",
      "Epoch 1: val_accuracy improved from -inf to 0.75093, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.5744 - accuracy: 0.7457 - val_loss: 0.5617 - val_accuracy: 0.7509 - lr: 5.0000e-05\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.7506\n",
      "Epoch 2: val_accuracy did not improve from 0.75093\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.5577 - accuracy: 0.7506 - val_loss: 0.5514 - val_accuracy: 0.7509 - lr: 5.1757e-05\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7506\n",
      "Epoch 3: val_accuracy did not improve from 0.75093\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.5298 - accuracy: 0.7506 - val_loss: 0.4707 - val_accuracy: 0.7509 - lr: 5.3576e-05\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4682 - accuracy: 0.7591\n",
      "Epoch 4: val_accuracy improved from 0.75093 to 0.82141, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.4682 - accuracy: 0.7591 - val_loss: 0.3664 - val_accuracy: 0.8214 - lr: 5.5459e-05\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.8302\n",
      "Epoch 5: val_accuracy improved from 0.82141 to 0.89083, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.3818 - accuracy: 0.8302 - val_loss: 0.2692 - val_accuracy: 0.8908 - lr: 5.7408e-05\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.8903\n",
      "Epoch 6: val_accuracy improved from 0.89083 to 0.91786, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2876 - accuracy: 0.8903 - val_loss: 0.1949 - val_accuracy: 0.9179 - lr: 5.9425e-05\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9239\n",
      "Epoch 7: val_accuracy improved from 0.91786 to 0.99205, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2173 - accuracy: 0.9239 - val_loss: 0.1271 - val_accuracy: 0.9921 - lr: 6.1513e-05\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9364\n",
      "Epoch 8: val_accuracy improved from 0.99205 to 0.99523, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1782 - accuracy: 0.9364 - val_loss: 0.0957 - val_accuracy: 0.9952 - lr: 6.3675e-05\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9434\n",
      "Epoch 9: val_accuracy did not improve from 0.99523\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1521 - accuracy: 0.9434 - val_loss: 0.0761 - val_accuracy: 0.9952 - lr: 6.5913e-05\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9327\n",
      "Epoch 10: val_accuracy improved from 0.99523 to 0.99894, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 189ms/step - loss: 0.1641 - accuracy: 0.9327 - val_loss: 0.0736 - val_accuracy: 0.9989 - lr: 6.8229e-05\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9505\n",
      "Epoch 11: val_accuracy improved from 0.99894 to 0.99947, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.1275 - accuracy: 0.9505 - val_loss: 0.0674 - val_accuracy: 0.9995 - lr: 7.0627e-05\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9504\n",
      "Epoch 12: val_accuracy did not improve from 0.99947\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.1248 - accuracy: 0.9504 - val_loss: 0.0649 - val_accuracy: 0.9894 - lr: 7.3109e-05\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9599\n",
      "Epoch 13: val_accuracy did not improve from 0.99947\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.1005 - accuracy: 0.9599 - val_loss: 0.0941 - val_accuracy: 0.9746 - lr: 7.5678e-05\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1052 - accuracy: 0.9602\n",
      "Epoch 14: val_accuracy did not improve from 0.99947\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.1052 - accuracy: 0.9602 - val_loss: 0.0817 - val_accuracy: 0.9709 - lr: 7.8338e-05\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9625\n",
      "Epoch 15: val_accuracy improved from 0.99947 to 1.00000, saving model to models/classification_wbc_binary_LYMPHOCYTE_best.h5\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0920 - accuracy: 0.9625 - val_loss: 0.0444 - val_accuracy: 1.0000 - lr: 8.1091e-05\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9632\n",
      "Epoch 16: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 195ms/step - loss: 0.0826 - accuracy: 0.9632 - val_loss: 0.0372 - val_accuracy: 1.0000 - lr: 8.3940e-05\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1024 - accuracy: 0.9599\n",
      "Epoch 17: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.1024 - accuracy: 0.9599 - val_loss: 0.0369 - val_accuracy: 1.0000 - lr: 8.6890e-05\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9630\n",
      "Epoch 18: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 33s 196ms/step - loss: 0.0941 - accuracy: 0.9630 - val_loss: 0.0363 - val_accuracy: 1.0000 - lr: 8.9944e-05\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9689\n",
      "Epoch 19: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 195ms/step - loss: 0.0787 - accuracy: 0.9689 - val_loss: 0.0602 - val_accuracy: 0.9889 - lr: 9.3104e-05\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9753\n",
      "Epoch 20: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0594 - accuracy: 0.9753 - val_loss: 0.0229 - val_accuracy: 1.0000 - lr: 9.6376e-05\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9713\n",
      "Epoch 21: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0698 - accuracy: 0.9713 - val_loss: 0.0300 - val_accuracy: 0.9952 - lr: 9.9763e-05\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9781\n",
      "Epoch 22: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.0580 - accuracy: 0.9781 - val_loss: 0.0283 - val_accuracy: 1.0000 - lr: 1.0327e-04\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9779\n",
      "Epoch 23: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0566 - accuracy: 0.9779 - val_loss: 0.0372 - val_accuracy: 0.9836 - lr: 1.0690e-04\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9762\n",
      "Epoch 24: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0590 - accuracy: 0.9762 - val_loss: 0.0317 - val_accuracy: 1.0000 - lr: 1.1065e-04\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9749\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.1454338527983055e-05.\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0688 - accuracy: 0.9749 - val_loss: 0.0416 - val_accuracy: 0.9942 - lr: 1.1454e-05\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9806\n",
      "Epoch 26: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0461 - accuracy: 0.9806 - val_loss: 0.0184 - val_accuracy: 1.0000 - lr: 1.1857e-04\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9780\n",
      "Epoch 27: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0523 - accuracy: 0.9780 - val_loss: 0.0550 - val_accuracy: 0.9841 - lr: 1.2274e-04\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9744\n",
      "Epoch 28: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0617 - accuracy: 0.9744 - val_loss: 0.0229 - val_accuracy: 0.9995 - lr: 1.2705e-04\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9827\n",
      "Epoch 29: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0480 - accuracy: 0.9827 - val_loss: 0.0137 - val_accuracy: 1.0000 - lr: 1.3151e-04\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9791\n",
      "Epoch 30: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 33s 195ms/step - loss: 0.0644 - accuracy: 0.9791 - val_loss: 0.0224 - val_accuracy: 0.9984 - lr: 1.3614e-04\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9815\n",
      "Epoch 31: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0458 - accuracy: 0.9815 - val_loss: 0.0205 - val_accuracy: 0.9958 - lr: 1.4092e-04\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0372 - accuracy: 0.9859\n",
      "Epoch 32: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0372 - accuracy: 0.9859 - val_loss: 0.0126 - val_accuracy: 0.9989 - lr: 1.4587e-04\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9815\n",
      "Epoch 33: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0441 - accuracy: 0.9815 - val_loss: 0.0131 - val_accuracy: 1.0000 - lr: 1.5100e-04\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9802\n",
      "Epoch 34: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0562 - accuracy: 0.9802 - val_loss: 0.0123 - val_accuracy: 1.0000 - lr: 1.5630e-04\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9841\n",
      "Epoch 35: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0405 - accuracy: 0.9841 - val_loss: 0.0548 - val_accuracy: 0.9751 - lr: 1.6180e-04\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9840\n",
      "Epoch 36: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0381 - accuracy: 0.9840 - val_loss: 0.0244 - val_accuracy: 0.9989 - lr: 1.6748e-04\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9846\n",
      "Epoch 37: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0422 - accuracy: 0.9846 - val_loss: 0.0705 - val_accuracy: 0.9762 - lr: 1.7337e-04\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9883\n",
      "Epoch 38: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0361 - accuracy: 0.9883 - val_loss: 0.0133 - val_accuracy: 0.9984 - lr: 1.7946e-04\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9903\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.8576761067379267e-05.\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0247 - accuracy: 0.9903 - val_loss: 0.0123 - val_accuracy: 0.9984 - lr: 1.8577e-05\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
      "Epoch 40: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.0139 - val_accuracy: 0.9995 - lr: 1.9230e-04\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9875\n",
      "Epoch 41: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.0235 - val_accuracy: 0.9968 - lr: 1.9905e-04\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9920\n",
      "Epoch 42: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 195ms/step - loss: 0.0231 - accuracy: 0.9920 - val_loss: 0.0121 - val_accuracy: 0.9979 - lr: 2.0605e-04\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9790\n",
      "Epoch 43: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 33s 195ms/step - loss: 0.0606 - accuracy: 0.9790 - val_loss: 0.0194 - val_accuracy: 0.9947 - lr: 2.1329e-04\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9915\n",
      "Epoch 44: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.0345 - val_accuracy: 0.9931 - lr: 2.2079e-04\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9933\n",
      "Epoch 45: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.0124 - val_accuracy: 0.9968 - lr: 2.2854e-04\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9929\n",
      "Epoch 46: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.0194 - accuracy: 0.9929 - val_loss: 0.0093 - val_accuracy: 0.9979 - lr: 2.3658e-04\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3327 - accuracy: 0.8579\n",
      "Epoch 47: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.3327 - accuracy: 0.8579 - val_loss: 0.5921 - val_accuracy: 0.7509 - lr: 2.4489e-04\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5694 - accuracy: 0.7506\n",
      "Epoch 48: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.5694 - accuracy: 0.7506 - val_loss: 0.5658 - val_accuracy: 0.7509 - lr: 2.5350e-04\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5648 - accuracy: 0.7506\n",
      "Epoch 49: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.5648 - accuracy: 0.7506 - val_loss: 0.5594 - val_accuracy: 0.7509 - lr: 2.6240e-04\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5059 - accuracy: 0.7611\n",
      "Epoch 50: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.5059 - accuracy: 0.7611 - val_loss: 0.3831 - val_accuracy: 0.8521 - lr: 2.7163e-04\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2970 - accuracy: 0.8799\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 2.8117065085098147e-05.\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.2970 - accuracy: 0.8799 - val_loss: 0.1942 - val_accuracy: 0.9624 - lr: 2.8117e-05\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9309\n",
      "Epoch 52: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.1842 - accuracy: 0.9309 - val_loss: 0.0879 - val_accuracy: 0.9921 - lr: 2.9105e-04\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9490\n",
      "Epoch 53: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.1296 - accuracy: 0.9490 - val_loss: 0.0571 - val_accuracy: 0.9968 - lr: 3.0128e-04\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0974 - accuracy: 0.9623\n",
      "Epoch 54: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.0974 - accuracy: 0.9623 - val_loss: 0.0382 - val_accuracy: 0.9974 - lr: 3.1187e-04\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9665\n",
      "Epoch 55: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0881 - accuracy: 0.9665 - val_loss: 0.0303 - val_accuracy: 0.9984 - lr: 3.2283e-04\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9712\n",
      "Epoch 56: ReduceLROnPlateau reducing learning rate to 3.341719566378743e-05.\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0703 - accuracy: 0.9712 - val_loss: 0.0198 - val_accuracy: 0.9989 - lr: 3.3417e-05\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9803\n",
      "Epoch 57: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 192ms/step - loss: 0.0510 - accuracy: 0.9803 - val_loss: 0.0173 - val_accuracy: 1.0000 - lr: 3.4592e-04\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9813\n",
      "Epoch 58: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 193ms/step - loss: 0.0515 - accuracy: 0.9813 - val_loss: 0.0241 - val_accuracy: 0.9963 - lr: 3.5807e-04\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9880\n",
      "Epoch 59: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 32s 194ms/step - loss: 0.0329 - accuracy: 0.9880 - val_loss: 0.0205 - val_accuracy: 0.9979 - lr: 3.7066e-04\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9861\n",
      "Epoch 60: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 33s 197ms/step - loss: 0.0358 - accuracy: 0.9861 - val_loss: 0.0122 - val_accuracy: 0.9989 - lr: 3.8368e-04\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9886\n",
      "Epoch 61: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 320ms/step - loss: 0.0310 - accuracy: 0.9886 - val_loss: 0.0089 - val_accuracy: 0.9995 - lr: 3.9716e-04\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9804\n",
      "Epoch 62: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 315ms/step - loss: 0.0534 - accuracy: 0.9804 - val_loss: 0.0080 - val_accuracy: 1.0000 - lr: 4.1112e-04\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9909\n",
      "Epoch 63: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 317ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.0108 - val_accuracy: 0.9989 - lr: 4.2557e-04\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9872\n",
      "Epoch 64: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 318ms/step - loss: 0.0355 - accuracy: 0.9872 - val_loss: 0.0249 - val_accuracy: 0.9899 - lr: 4.4052e-04\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9881\n",
      "Epoch 65: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 317ms/step - loss: 0.0348 - accuracy: 0.9881 - val_loss: 0.0234 - val_accuracy: 0.9947 - lr: 4.5601e-04\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9913\n",
      "Epoch 66: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 315ms/step - loss: 0.0249 - accuracy: 0.9913 - val_loss: 0.0403 - val_accuracy: 0.9830 - lr: 4.7203e-04\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9933\n",
      "Epoch 67: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 52s 314ms/step - loss: 0.0191 - accuracy: 0.9933 - val_loss: 0.0042 - val_accuracy: 0.9995 - lr: 4.8862e-04\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9871\n",
      "Epoch 68: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 52s 315ms/step - loss: 0.0399 - accuracy: 0.9871 - val_loss: 0.0046 - val_accuracy: 1.0000 - lr: 5.0579e-04\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9845\n",
      "Epoch 69: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 316ms/step - loss: 0.0441 - accuracy: 0.9845 - val_loss: 0.0558 - val_accuracy: 0.9857 - lr: 5.2356e-04\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9796\n",
      "Epoch 70: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 317ms/step - loss: 0.0562 - accuracy: 0.9796 - val_loss: 0.0157 - val_accuracy: 0.9968 - lr: 5.4196e-04\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9931\n",
      "Epoch 71: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 53s 318ms/step - loss: 0.0205 - accuracy: 0.9931 - val_loss: 0.0115 - val_accuracy: 0.9968 - lr: 5.6101e-04\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9948 \n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.807243287563324e-05.\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 6799s 41s/step - loss: 0.0144 - accuracy: 0.9948 - val_loss: 0.0312 - val_accuracy: 0.9878 - lr: 5.8072e-05\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9876\n",
      "Epoch 73: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 45s 273ms/step - loss: 0.0429 - accuracy: 0.9876 - val_loss: 0.0390 - val_accuracy: 0.9899 - lr: 6.0113e-04\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9911\n",
      "Epoch 74: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 46s 276ms/step - loss: 0.0257 - accuracy: 0.9911 - val_loss: 0.0180 - val_accuracy: 0.9947 - lr: 6.2226e-04\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9930\n",
      "Epoch 75: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 46s 275ms/step - loss: 0.0195 - accuracy: 0.9930 - val_loss: 0.0638 - val_accuracy: 0.9735 - lr: 6.4412e-04\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9507\n",
      "Epoch 76: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 46s 274ms/step - loss: 0.1463 - accuracy: 0.9507 - val_loss: 0.3762 - val_accuracy: 0.8405 - lr: 6.6676e-04\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1138 - accuracy: 0.9589\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.901921587996185e-05.\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 46s 274ms/step - loss: 0.1138 - accuracy: 0.9589 - val_loss: 0.0232 - val_accuracy: 0.9921 - lr: 6.9019e-05\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9836\n",
      "Epoch 78: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 46s 274ms/step - loss: 0.0463 - accuracy: 0.9836 - val_loss: 0.0416 - val_accuracy: 0.9868 - lr: 7.1445e-04\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9866\n",
      "Epoch 79: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 33s 199ms/step - loss: 0.0385 - accuracy: 0.9866 - val_loss: 0.0628 - val_accuracy: 0.9793 - lr: 7.3955e-04\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9813\n",
      "Epoch 80: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0526 - accuracy: 0.9813 - val_loss: 0.0213 - val_accuracy: 0.9974 - lr: 7.6554e-04\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9910\n",
      "Epoch 81: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0260 - accuracy: 0.9910 - val_loss: 0.0239 - val_accuracy: 0.9910 - lr: 7.9245e-04\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9924\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 8.202948956750334e-05.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.0140 - val_accuracy: 0.9963 - lr: 8.2029e-05\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9924\n",
      "Epoch 83: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0205 - accuracy: 0.9924 - val_loss: 0.0129 - val_accuracy: 0.9968 - lr: 8.4912e-04\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9872\n",
      "Epoch 84: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0373 - accuracy: 0.9872 - val_loss: 0.0104 - val_accuracy: 0.9989 - lr: 8.7896e-04\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0161 - accuracy: 0.9939\n",
      "Epoch 85: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.0161 - val_accuracy: 0.9947 - lr: 9.0985e-04\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9809\n",
      "Epoch 86: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0590 - accuracy: 0.9809 - val_loss: 0.1319 - val_accuracy: 0.9698 - lr: 9.4182e-04\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9893\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 9.749223245307803e-05.\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0298 - accuracy: 0.9893 - val_loss: 0.0108 - val_accuracy: 0.9963 - lr: 9.7492e-05\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9947\n",
      "Epoch 88: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0193 - val_accuracy: 0.9942 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.9940\n",
      "Epoch 89: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0163 - accuracy: 0.9940 - val_loss: 0.0212 - val_accuracy: 0.9936 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9908\n",
      "Epoch 90: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0270 - accuracy: 0.9908 - val_loss: 0.0174 - val_accuracy: 0.9931 - lr: 0.0011\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9750\n",
      "Epoch 91: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0707 - accuracy: 0.9750 - val_loss: 0.0549 - val_accuracy: 0.9857 - lr: 0.0011\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9886\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001158697297796607.\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0320 - accuracy: 0.9886 - val_loss: 0.0631 - val_accuracy: 0.9825 - lr: 1.1587e-04\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9913\n",
      "Epoch 93: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0270 - accuracy: 0.9913 - val_loss: 0.0100 - val_accuracy: 0.9963 - lr: 0.0012\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9944\n",
      "Epoch 94: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0129 - accuracy: 0.9944 - val_loss: 0.0858 - val_accuracy: 0.9830 - lr: 0.0012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9900\n",
      "Epoch 95: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0272 - accuracy: 0.9900 - val_loss: 0.0220 - val_accuracy: 0.9931 - lr: 0.0013\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9909\n",
      "Epoch 96: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0239 - accuracy: 0.9909 - val_loss: 0.0075 - val_accuracy: 0.9995 - lr: 0.0013\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9898\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00013771143276244401.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0264 - accuracy: 0.9898 - val_loss: 0.0101 - val_accuracy: 0.9974 - lr: 1.3771e-04\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9961\n",
      "Epoch 98: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.0016 - val_accuracy: 1.0000 - lr: 0.0014\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0098 - accuracy: 0.9967\n",
      "Epoch 99: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0098 - accuracy: 0.9967 - val_loss: 0.0061 - val_accuracy: 0.9984 - lr: 0.0015\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9894\n",
      "Epoch 100: val_accuracy did not improve from 1.00000\n",
      "166/166 [==============================] - 29s 172ms/step - loss: 0.0289 - accuracy: 0.9894 - val_loss: 0.0261 - val_accuracy: 0.9926 - lr: 0.0015\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5652 - accuracy: 0.7511\n",
      "Epoch 1: val_accuracy improved from -inf to 0.75093, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5652 - accuracy: 0.7511 - val_loss: 0.5587 - val_accuracy: 0.7509 - lr: 5.0000e-05\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5561 - accuracy: 0.7511\n",
      "Epoch 2: val_accuracy did not improve from 0.75093\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.5561 - accuracy: 0.7511 - val_loss: 0.5321 - val_accuracy: 0.7509 - lr: 5.1757e-05\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.7648\n",
      "Epoch 3: val_accuracy improved from 0.75093 to 0.83519, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 30s 177ms/step - loss: 0.5160 - accuracy: 0.7648 - val_loss: 0.4134 - val_accuracy: 0.8352 - lr: 5.3576e-05\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4071 - accuracy: 0.8379\n",
      "Epoch 4: val_accuracy improved from 0.83519 to 0.91309, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 30s 183ms/step - loss: 0.4071 - accuracy: 0.8379 - val_loss: 0.2680 - val_accuracy: 0.9131 - lr: 5.5459e-05\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.8829\n",
      "Epoch 5: val_accuracy improved from 0.91309 to 0.93482, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.3144 - accuracy: 0.8829 - val_loss: 0.2239 - val_accuracy: 0.9348 - lr: 5.7408e-05\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2550 - accuracy: 0.9126\n",
      "Epoch 6: val_accuracy improved from 0.93482 to 0.93641, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.2550 - accuracy: 0.9126 - val_loss: 0.1776 - val_accuracy: 0.9364 - lr: 5.9425e-05\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9251\n",
      "Epoch 7: val_accuracy did not improve from 0.93641\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.2177 - accuracy: 0.9251 - val_loss: 0.1684 - val_accuracy: 0.9338 - lr: 6.1513e-05\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9296\n",
      "Epoch 8: val_accuracy improved from 0.93641 to 0.93800, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.2065 - accuracy: 0.9296 - val_loss: 0.1849 - val_accuracy: 0.9380 - lr: 6.3675e-05\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1886 - accuracy: 0.9347\n",
      "Epoch 9: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1886 - accuracy: 0.9347 - val_loss: 0.2606 - val_accuracy: 0.9338 - lr: 6.5913e-05\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1892 - accuracy: 0.9314\n",
      "Epoch 10: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 174ms/step - loss: 0.1892 - accuracy: 0.9314 - val_loss: 0.1967 - val_accuracy: 0.9332 - lr: 6.8229e-05\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9417\n",
      "Epoch 11: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1654 - accuracy: 0.9417 - val_loss: 0.2145 - val_accuracy: 0.9263 - lr: 7.0627e-05\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 0.9405\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 7.3108858487103134e-06.\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 175ms/step - loss: 0.1613 - accuracy: 0.9405 - val_loss: 0.2931 - val_accuracy: 0.9300 - lr: 7.3109e-06\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9436\n",
      "Epoch 13: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 30s 177ms/step - loss: 0.1526 - accuracy: 0.9436 - val_loss: 0.1898 - val_accuracy: 0.9089 - lr: 7.5678e-05\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9411\n",
      "Epoch 14: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1551 - accuracy: 0.9411 - val_loss: 0.2558 - val_accuracy: 0.9380 - lr: 7.8338e-05\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9456\n",
      "Epoch 15: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 29s 173ms/step - loss: 0.1419 - accuracy: 0.9456 - val_loss: 0.2071 - val_accuracy: 0.9221 - lr: 8.1091e-05\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1368 - accuracy: 0.9448\n",
      "Epoch 16: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 30s 183ms/step - loss: 0.1368 - accuracy: 0.9448 - val_loss: 0.2483 - val_accuracy: 0.9147 - lr: 8.3940e-05\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9513\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 8.689003880135716e-06.\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.1301 - accuracy: 0.9513 - val_loss: 0.2149 - val_accuracy: 0.9083 - lr: 8.6890e-06\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9534\n",
      "Epoch 18: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1216 - accuracy: 0.9534 - val_loss: 0.2507 - val_accuracy: 0.9258 - lr: 8.9944e-05\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9567\n",
      "Epoch 19: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1173 - accuracy: 0.9567 - val_loss: 0.2699 - val_accuracy: 0.9210 - lr: 9.3104e-05\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1028 - accuracy: 0.9633\n",
      "Epoch 20: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 188ms/step - loss: 0.1028 - accuracy: 0.9633 - val_loss: 0.2070 - val_accuracy: 0.9009 - lr: 9.6376e-05\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9564\n",
      "Epoch 21: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1099 - accuracy: 0.9564 - val_loss: 0.2590 - val_accuracy: 0.9279 - lr: 9.9763e-05\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9570\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 1.0326900519430638e-05.\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1097 - accuracy: 0.9570 - val_loss: 0.3499 - val_accuracy: 0.9226 - lr: 1.0327e-05\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9642\n",
      "Epoch 23: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0937 - accuracy: 0.9642 - val_loss: 0.1769 - val_accuracy: 0.9205 - lr: 1.0690e-04\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9564\n",
      "Epoch 24: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 188ms/step - loss: 0.1117 - accuracy: 0.9564 - val_loss: 0.2495 - val_accuracy: 0.9359 - lr: 1.1065e-04\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9666\n",
      "Epoch 25: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0895 - accuracy: 0.9666 - val_loss: 0.2310 - val_accuracy: 0.9226 - lr: 1.1454e-04\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9707\n",
      "Epoch 26: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0795 - accuracy: 0.9707 - val_loss: 0.2970 - val_accuracy: 0.9285 - lr: 1.1857e-04\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.9670\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 1.227354514412582e-05.\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0840 - accuracy: 0.9670 - val_loss: 0.2863 - val_accuracy: 0.9343 - lr: 1.2274e-05\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9647\n",
      "Epoch 28: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0910 - accuracy: 0.9647 - val_loss: 0.3229 - val_accuracy: 0.9216 - lr: 1.2705e-04\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9726\n",
      "Epoch 29: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0763 - accuracy: 0.9726 - val_loss: 0.1846 - val_accuracy: 0.9067 - lr: 1.3151e-04\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9777\n",
      "Epoch 30: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0670 - accuracy: 0.9777 - val_loss: 0.3175 - val_accuracy: 0.8617 - lr: 1.3614e-04\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9768\n",
      "Epoch 31: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0652 - accuracy: 0.9768 - val_loss: 0.3186 - val_accuracy: 0.8903 - lr: 1.4092e-04\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9812\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.4587135228794069e-05.\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0568 - accuracy: 0.9812 - val_loss: 0.2631 - val_accuracy: 0.9247 - lr: 1.4587e-05\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9814\n",
      "Epoch 33: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0524 - accuracy: 0.9814 - val_loss: 0.2977 - val_accuracy: 0.9258 - lr: 1.5100e-04\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9708\n",
      "Epoch 34: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0717 - accuracy: 0.9708 - val_loss: 0.3441 - val_accuracy: 0.8940 - lr: 1.5630e-04\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9791\n",
      "Epoch 35: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0597 - accuracy: 0.9791 - val_loss: 0.2719 - val_accuracy: 0.9094 - lr: 1.6180e-04\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9787\n",
      "Epoch 36: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.0570 - accuracy: 0.9787 - val_loss: 0.2925 - val_accuracy: 0.9359 - lr: 1.6748e-04\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9805\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.7336841847281902e-05.\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.0485 - accuracy: 0.9805 - val_loss: 0.2295 - val_accuracy: 0.9020 - lr: 1.7337e-05\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9848\n",
      "Epoch 38: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0442 - accuracy: 0.9848 - val_loss: 0.3528 - val_accuracy: 0.8617 - lr: 1.7946e-04\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9809\n",
      "Epoch 39: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0561 - accuracy: 0.9809 - val_loss: 0.2184 - val_accuracy: 0.9332 - lr: 1.8577e-04\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9711\n",
      "Epoch 40: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0764 - accuracy: 0.9711 - val_loss: 0.2616 - val_accuracy: 0.9083 - lr: 1.9230e-04\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9847\n",
      "Epoch 41: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0421 - accuracy: 0.9847 - val_loss: 0.2676 - val_accuracy: 0.8797 - lr: 1.9905e-04\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9894\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 2.0604876044671983e-05.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.4665 - val_accuracy: 0.8882 - lr: 2.0605e-05\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9883\n",
      "Epoch 43: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0348 - accuracy: 0.9883 - val_loss: 0.4988 - val_accuracy: 0.9083 - lr: 2.1329e-04\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9855\n",
      "Epoch 44: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0408 - accuracy: 0.9855 - val_loss: 0.4754 - val_accuracy: 0.9083 - lr: 2.2079e-04\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9875\n",
      "Epoch 45: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0360 - accuracy: 0.9875 - val_loss: 0.3665 - val_accuracy: 0.9269 - lr: 2.2854e-04\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9906\n",
      "Epoch 46: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0277 - accuracy: 0.9906 - val_loss: 0.2156 - val_accuracy: 0.9364 - lr: 2.3658e-04\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9920\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.4488940834999087e-05.\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.2747 - val_accuracy: 0.9290 - lr: 2.4489e-05\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9914\n",
      "Epoch 48: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.3017 - val_accuracy: 0.9375 - lr: 2.5350e-04\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9785\n",
      "Epoch 49: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0590 - accuracy: 0.9785 - val_loss: 0.3214 - val_accuracy: 0.9131 - lr: 2.6240e-04\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9926\n",
      "Epoch 50: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0212 - accuracy: 0.9926 - val_loss: 0.3669 - val_accuracy: 0.9343 - lr: 2.7163e-04\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9914\n",
      "Epoch 51: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0254 - accuracy: 0.9914 - val_loss: 0.2657 - val_accuracy: 0.9348 - lr: 2.8117e-04\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9888\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.910516050178558e-05.\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0334 - accuracy: 0.9888 - val_loss: 0.3673 - val_accuracy: 0.9316 - lr: 2.9105e-05\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9844\n",
      "Epoch 53: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0425 - accuracy: 0.9844 - val_loss: 0.3029 - val_accuracy: 0.9300 - lr: 3.0128e-04\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9748\n",
      "Epoch 54: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0682 - accuracy: 0.9748 - val_loss: 0.2620 - val_accuracy: 0.9306 - lr: 3.1187e-04\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9901\n",
      "Epoch 55: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0271 - accuracy: 0.9901 - val_loss: 0.4451 - val_accuracy: 0.9364 - lr: 3.2283e-04\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9938\n",
      "Epoch 56: val_accuracy did not improve from 0.93800\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0174 - accuracy: 0.9938 - val_loss: 0.3935 - val_accuracy: 0.9380 - lr: 3.3417e-04\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9933\n",
      "Epoch 57: val_accuracy improved from 0.93800 to 0.95390, saving model to models/classification_wbc_binary_MONOCYTE_best.h5\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.0186 - accuracy: 0.9933 - val_loss: 0.1247 - val_accuracy: 0.9539 - lr: 3.4592e-04\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9878\n",
      "Epoch 58: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.0325 - accuracy: 0.9878 - val_loss: 0.4709 - val_accuracy: 0.9375 - lr: 3.5807e-04\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0183 - accuracy: 0.9931\n",
      "Epoch 59: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0183 - accuracy: 0.9931 - val_loss: 0.3770 - val_accuracy: 0.9311 - lr: 3.7066e-04\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9863\n",
      "Epoch 60: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0408 - accuracy: 0.9863 - val_loss: 0.4540 - val_accuracy: 0.9332 - lr: 3.8368e-04\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9929\n",
      "Epoch 61: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0199 - accuracy: 0.9929 - val_loss: 0.6278 - val_accuracy: 0.9380 - lr: 3.9716e-04\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9946\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.111213202122599e-05.\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0152 - accuracy: 0.9946 - val_loss: 0.3303 - val_accuracy: 0.9300 - lr: 4.1112e-05\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9589\n",
      "Epoch 63: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1070 - accuracy: 0.9589 - val_loss: 0.5216 - val_accuracy: 0.9020 - lr: 4.2557e-04\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9920\n",
      "Epoch 64: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0237 - accuracy: 0.9920 - val_loss: 0.4936 - val_accuracy: 0.9369 - lr: 4.4052e-04\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9942\n",
      "Epoch 65: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 0.4838 - val_accuracy: 0.9338 - lr: 4.5601e-04\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9671\n",
      "Epoch 66: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0898 - accuracy: 0.9671 - val_loss: 0.2270 - val_accuracy: 0.9179 - lr: 4.7203e-04\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9926\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.886186216026545e-05.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0238 - accuracy: 0.9926 - val_loss: 0.4170 - val_accuracy: 0.9369 - lr: 4.8862e-05\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9927\n",
      "Epoch 68: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.4622 - val_accuracy: 0.9353 - lr: 5.0579e-04\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9939\n",
      "Epoch 69: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.4187 - val_accuracy: 0.9316 - lr: 5.2356e-04\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9953\n",
      "Epoch 70: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0140 - accuracy: 0.9953 - val_loss: 0.3601 - val_accuracy: 0.8988 - lr: 5.4196e-04\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9888\n",
      "Epoch 71: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0327 - accuracy: 0.9888 - val_loss: 0.3009 - val_accuracy: 0.9073 - lr: 5.6101e-04\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9907\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.807243287563324e-05.\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0252 - accuracy: 0.9907 - val_loss: 0.4869 - val_accuracy: 0.9364 - lr: 5.8072e-05\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9949\n",
      "Epoch 73: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0139 - accuracy: 0.9949 - val_loss: 0.5677 - val_accuracy: 0.9353 - lr: 6.0113e-04\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9958\n",
      "Epoch 74: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.6509 - val_accuracy: 0.9290 - lr: 6.2226e-04\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9883\n",
      "Epoch 75: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0349 - accuracy: 0.9883 - val_loss: 0.3715 - val_accuracy: 0.9306 - lr: 6.4412e-04\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9613\n",
      "Epoch 76: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1092 - accuracy: 0.9613 - val_loss: 0.4528 - val_accuracy: 0.9353 - lr: 6.6676e-04\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9919\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.901921587996185e-05.\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0221 - accuracy: 0.9919 - val_loss: 0.5704 - val_accuracy: 0.9375 - lr: 6.9019e-05\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9877\n",
      "Epoch 78: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0320 - accuracy: 0.9877 - val_loss: 0.4654 - val_accuracy: 0.9327 - lr: 7.1445e-04\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9937\n",
      "Epoch 79: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 30s 183ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.5593 - val_accuracy: 0.9258 - lr: 7.3955e-04\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9903\n",
      "Epoch 80: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.3995 - val_accuracy: 0.9364 - lr: 7.6554e-04\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9947\n",
      "Epoch 81: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.0166 - accuracy: 0.9947 - val_loss: 0.6678 - val_accuracy: 0.9369 - lr: 7.9245e-04\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9971\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 8.202948956750334e-05.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 0.5845 - val_accuracy: 0.9147 - lr: 8.2029e-05\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9953\n",
      "Epoch 83: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0119 - accuracy: 0.9953 - val_loss: 0.5121 - val_accuracy: 0.9205 - lr: 8.4912e-04\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9798\n",
      "Epoch 84: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0525 - accuracy: 0.9798 - val_loss: 0.8296 - val_accuracy: 0.9327 - lr: 8.7896e-04\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9959\n",
      "Epoch 85: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.6849 - val_accuracy: 0.9359 - lr: 9.0985e-04\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9982\n",
      "Epoch 86: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.6679 - val_accuracy: 0.9306 - lr: 9.4182e-04\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0949 - accuracy: 0.9637\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 9.749223245307803e-05.\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0949 - accuracy: 0.9637 - val_loss: 0.2404 - val_accuracy: 0.9247 - lr: 9.7492e-05\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9799\n",
      "Epoch 88: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0527 - accuracy: 0.9799 - val_loss: 0.6454 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9837\n",
      "Epoch 89: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0442 - accuracy: 0.9837 - val_loss: 0.4229 - val_accuracy: 0.9343 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9823\n",
      "Epoch 90: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0520 - accuracy: 0.9823 - val_loss: 0.3885 - val_accuracy: 0.9353 - lr: 0.0011\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9922\n",
      "Epoch 91: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0220 - accuracy: 0.9922 - val_loss: 0.4796 - val_accuracy: 0.9237 - lr: 0.0011\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9892\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001158697297796607.\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0303 - accuracy: 0.9892 - val_loss: 0.3961 - val_accuracy: 0.9348 - lr: 1.1587e-04\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0116 - accuracy: 0.9957\n",
      "Epoch 93: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0116 - accuracy: 0.9957 - val_loss: 0.5408 - val_accuracy: 0.9332 - lr: 0.0012\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9931\n",
      "Epoch 94: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0184 - accuracy: 0.9931 - val_loss: 0.7805 - val_accuracy: 0.9332 - lr: 0.0012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9910\n",
      "Epoch 95: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0231 - accuracy: 0.9910 - val_loss: 0.8453 - val_accuracy: 0.9353 - lr: 0.0013\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9975\n",
      "Epoch 96: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0081 - accuracy: 0.9975 - val_loss: 0.9398 - val_accuracy: 0.9380 - lr: 0.0013\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9021\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00013771143276244401.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.2396 - accuracy: 0.9021 - val_loss: 0.5613 - val_accuracy: 0.7509 - lr: 1.3771e-04\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5613 - accuracy: 0.7511\n",
      "Epoch 98: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5613 - accuracy: 0.7511 - val_loss: 0.5613 - val_accuracy: 0.7509 - lr: 0.0014\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.7511\n",
      "Epoch 99: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5614 - accuracy: 0.7511 - val_loss: 0.5613 - val_accuracy: 0.7509 - lr: 0.0015\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.7511\n",
      "Epoch 100: val_accuracy did not improve from 0.95390\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.5612 - accuracy: 0.7511 - val_loss: 0.5621 - val_accuracy: 0.7509 - lr: 0.0015\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5728 - accuracy: 0.7383\n",
      "Epoch 1: val_accuracy improved from -inf to 0.74881, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5728 - accuracy: 0.7383 - val_loss: 0.5628 - val_accuracy: 0.7488 - lr: 5.0000e-05\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.7490\n",
      "Epoch 2: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.5638 - accuracy: 0.7490 - val_loss: 0.5624 - val_accuracy: 0.7488 - lr: 5.1757e-05\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.7490\n",
      "Epoch 3: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.5643 - accuracy: 0.7490 - val_loss: 0.5622 - val_accuracy: 0.7488 - lr: 5.3576e-05\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5635 - accuracy: 0.7490\n",
      "Epoch 4: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.5635 - accuracy: 0.7490 - val_loss: 0.5671 - val_accuracy: 0.7488 - lr: 5.5459e-05\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5626 - accuracy: 0.7490\n",
      "Epoch 5: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5626 - accuracy: 0.7490 - val_loss: 0.5648 - val_accuracy: 0.7488 - lr: 5.7408e-05\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.7490\n",
      "Epoch 6: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.5603 - accuracy: 0.7490 - val_loss: 0.5672 - val_accuracy: 0.7488 - lr: 5.9425e-05\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.7490\n",
      "Epoch 7: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5456 - accuracy: 0.7490 - val_loss: 0.5324 - val_accuracy: 0.7488 - lr: 6.1513e-05\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.7490\n",
      "Epoch 8: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.5069 - accuracy: 0.7490 - val_loss: 0.5336 - val_accuracy: 0.7488 - lr: 6.3675e-05\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.7505\n",
      "Epoch 9: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.4853 - accuracy: 0.7505 - val_loss: 0.5295 - val_accuracy: 0.7488 - lr: 6.5913e-05\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4584 - accuracy: 0.7723\n",
      "Epoch 10: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.4584 - accuracy: 0.7723 - val_loss: 0.5138 - val_accuracy: 0.7234 - lr: 6.8229e-05\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4318 - accuracy: 0.7945\n",
      "Epoch 11: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.4318 - accuracy: 0.7945 - val_loss: 0.4787 - val_accuracy: 0.7260 - lr: 7.0627e-05\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.8091\n",
      "Epoch 12: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.4083 - accuracy: 0.8091 - val_loss: 0.4565 - val_accuracy: 0.7414 - lr: 7.3109e-05\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3753 - accuracy: 0.8236\n",
      "Epoch 13: val_accuracy did not improve from 0.74881\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.3753 - accuracy: 0.8236 - val_loss: 0.5172 - val_accuracy: 0.7091 - lr: 7.5678e-05\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3438 - accuracy: 0.8432\n",
      "Epoch 14: val_accuracy improved from 0.74881 to 0.81187, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.3438 - accuracy: 0.8432 - val_loss: 0.3715 - val_accuracy: 0.8119 - lr: 7.8338e-05\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.3079 - accuracy: 0.8601\n",
      "Epoch 15: val_accuracy did not improve from 0.81187\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.3079 - accuracy: 0.8601 - val_loss: 0.3614 - val_accuracy: 0.8113 - lr: 8.1091e-05\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.8683\n",
      "Epoch 16: val_accuracy did not improve from 0.81187\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2908 - accuracy: 0.8683 - val_loss: 0.3998 - val_accuracy: 0.7901 - lr: 8.3940e-05\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8743\n",
      "Epoch 17: val_accuracy improved from 0.81187 to 0.83837, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.2737 - accuracy: 0.8743 - val_loss: 0.3187 - val_accuracy: 0.8384 - lr: 8.6890e-05\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8809\n",
      "Epoch 18: val_accuracy did not improve from 0.83837\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.2621 - accuracy: 0.8809 - val_loss: 0.3369 - val_accuracy: 0.8299 - lr: 8.9944e-05\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2432 - accuracy: 0.8915\n",
      "Epoch 19: val_accuracy did not improve from 0.83837\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2432 - accuracy: 0.8915 - val_loss: 0.3083 - val_accuracy: 0.8230 - lr: 9.3104e-05\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2306 - accuracy: 0.8964\n",
      "Epoch 20: val_accuracy did not improve from 0.83837\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2306 - accuracy: 0.8964 - val_loss: 0.3513 - val_accuracy: 0.8262 - lr: 9.6376e-05\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2282 - accuracy: 0.8974\n",
      "Epoch 21: val_accuracy improved from 0.83837 to 0.85427, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.2282 - accuracy: 0.8974 - val_loss: 0.2855 - val_accuracy: 0.8543 - lr: 9.9763e-05\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9062\n",
      "Epoch 22: val_accuracy did not improve from 0.85427\n",
      "166/166 [==============================] - 30s 182ms/step - loss: 0.2089 - accuracy: 0.9062 - val_loss: 0.3325 - val_accuracy: 0.8246 - lr: 1.0327e-04\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2105 - accuracy: 0.9038\n",
      "Epoch 23: val_accuracy did not improve from 0.85427\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.2105 - accuracy: 0.9038 - val_loss: 0.3038 - val_accuracy: 0.8373 - lr: 1.0690e-04\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9077\n",
      "Epoch 24: val_accuracy did not improve from 0.85427\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.2031 - accuracy: 0.9077 - val_loss: 0.3016 - val_accuracy: 0.8283 - lr: 1.1065e-04\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1920 - accuracy: 0.9143\n",
      "Epoch 25: val_accuracy did not improve from 0.85427\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1920 - accuracy: 0.9143 - val_loss: 0.3050 - val_accuracy: 0.8246 - lr: 1.1454e-04\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9123\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 1.1856868513859809e-05.\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.85427\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1896 - accuracy: 0.9123 - val_loss: 0.3130 - val_accuracy: 0.8283 - lr: 1.1857e-05\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1826 - accuracy: 0.9182\n",
      "Epoch 27: val_accuracy improved from 0.85427 to 0.87228, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1826 - accuracy: 0.9182 - val_loss: 0.3035 - val_accuracy: 0.8723 - lr: 1.2274e-04\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1782 - accuracy: 0.9213\n",
      "Epoch 28: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1782 - accuracy: 0.9213 - val_loss: 0.2910 - val_accuracy: 0.8246 - lr: 1.2705e-04\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9221\n",
      "Epoch 29: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1760 - accuracy: 0.9221 - val_loss: 0.2940 - val_accuracy: 0.8347 - lr: 1.3151e-04\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9251\n",
      "Epoch 30: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1721 - accuracy: 0.9251 - val_loss: 0.2725 - val_accuracy: 0.8527 - lr: 1.3614e-04\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9202\n",
      "Epoch 31: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1718 - accuracy: 0.9202 - val_loss: 0.3130 - val_accuracy: 0.8029 - lr: 1.4092e-04\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9305\n",
      "Epoch 32: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1572 - accuracy: 0.9305 - val_loss: 0.3096 - val_accuracy: 0.8230 - lr: 1.4587e-04\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9359\n",
      "Epoch 33: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1517 - accuracy: 0.9359 - val_loss: 0.3927 - val_accuracy: 0.8272 - lr: 1.5100e-04\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1433 - accuracy: 0.9366\n",
      "Epoch 34: val_accuracy did not improve from 0.87228\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1433 - accuracy: 0.9366 - val_loss: 0.3057 - val_accuracy: 0.8267 - lr: 1.5630e-04\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9310\n",
      "Epoch 35: val_accuracy improved from 0.87228 to 0.87493, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1598 - accuracy: 0.9310 - val_loss: 0.2709 - val_accuracy: 0.8749 - lr: 1.6180e-04\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9377\n",
      "Epoch 36: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1451 - accuracy: 0.9377 - val_loss: 0.3077 - val_accuracy: 0.8590 - lr: 1.6748e-04\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9427\n",
      "Epoch 37: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1369 - accuracy: 0.9427 - val_loss: 0.3330 - val_accuracy: 0.8182 - lr: 1.7337e-04\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9302\n",
      "Epoch 38: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1572 - accuracy: 0.9302 - val_loss: 0.2878 - val_accuracy: 0.8431 - lr: 1.7946e-04\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9398\n",
      "Epoch 39: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1348 - accuracy: 0.9398 - val_loss: 0.2912 - val_accuracy: 0.8490 - lr: 1.8577e-04\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9349\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 1.922958908835426e-05.\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.1484 - accuracy: 0.9349 - val_loss: 0.3386 - val_accuracy: 0.8140 - lr: 1.9230e-05\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9462\n",
      "Epoch 41: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1248 - accuracy: 0.9462 - val_loss: 0.3038 - val_accuracy: 0.8484 - lr: 1.9905e-04\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1608 - accuracy: 0.9300\n",
      "Epoch 42: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1608 - accuracy: 0.9300 - val_loss: 0.2652 - val_accuracy: 0.8511 - lr: 2.0605e-04\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9456\n",
      "Epoch 43: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.1277 - accuracy: 0.9456 - val_loss: 0.3350 - val_accuracy: 0.8272 - lr: 2.1329e-04\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9527\n",
      "Epoch 44: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 30s 180ms/step - loss: 0.1113 - accuracy: 0.9527 - val_loss: 0.3620 - val_accuracy: 0.8262 - lr: 2.2079e-04\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9426\n",
      "Epoch 45: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1344 - accuracy: 0.9426 - val_loss: 0.3592 - val_accuracy: 0.8294 - lr: 2.2854e-04\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1205 - accuracy: 0.9461\n",
      "Epoch 46: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1205 - accuracy: 0.9461 - val_loss: 0.3236 - val_accuracy: 0.8368 - lr: 2.3658e-04\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9430\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 2.4488940834999087e-05.\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1350 - accuracy: 0.9430 - val_loss: 0.3920 - val_accuracy: 0.8262 - lr: 2.4489e-05\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9524\n",
      "Epoch 48: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1113 - accuracy: 0.9524 - val_loss: 0.3255 - val_accuracy: 0.8235 - lr: 2.5350e-04\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9570\n",
      "Epoch 49: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.1023 - accuracy: 0.9570 - val_loss: 0.3886 - val_accuracy: 0.8506 - lr: 2.6240e-04\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9565\n",
      "Epoch 50: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1066 - accuracy: 0.9565 - val_loss: 0.2980 - val_accuracy: 0.8728 - lr: 2.7163e-04\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9609\n",
      "Epoch 51: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0967 - accuracy: 0.9609 - val_loss: 0.3406 - val_accuracy: 0.8564 - lr: 2.8117e-04\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9513\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 2.910516050178558e-05.\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1127 - accuracy: 0.9513 - val_loss: 0.2951 - val_accuracy: 0.8718 - lr: 2.9105e-05\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9505\n",
      "Epoch 53: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1193 - accuracy: 0.9505 - val_loss: 0.4549 - val_accuracy: 0.8299 - lr: 3.0128e-04\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1362 - accuracy: 0.9471\n",
      "Epoch 54: val_accuracy did not improve from 0.87493\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1362 - accuracy: 0.9471 - val_loss: 0.3763 - val_accuracy: 0.8352 - lr: 3.1187e-04\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9665\n",
      "Epoch 55: val_accuracy improved from 0.87493 to 0.87864, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0822 - accuracy: 0.9665 - val_loss: 0.3084 - val_accuracy: 0.8786 - lr: 3.2283e-04\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0877 - accuracy: 0.9628\n",
      "Epoch 56: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0877 - accuracy: 0.9628 - val_loss: 0.3165 - val_accuracy: 0.8749 - lr: 3.3417e-04\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9603\n",
      "Epoch 57: ReduceLROnPlateau reducing learning rate to 3.459154977463186e-05.\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0969 - accuracy: 0.9603 - val_loss: 0.4857 - val_accuracy: 0.8246 - lr: 3.4592e-05\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9677\n",
      "Epoch 58: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0789 - accuracy: 0.9677 - val_loss: 0.2986 - val_accuracy: 0.8744 - lr: 3.5807e-04\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9611\n",
      "Epoch 59: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0976 - accuracy: 0.9611 - val_loss: 0.4832 - val_accuracy: 0.8283 - lr: 3.7066e-04\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9601\n",
      "Epoch 60: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0976 - accuracy: 0.9601 - val_loss: 0.4972 - val_accuracy: 0.8294 - lr: 3.8368e-04\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9569\n",
      "Epoch 61: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.1074 - accuracy: 0.9569 - val_loss: 0.4630 - val_accuracy: 0.8288 - lr: 3.9716e-04\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9610\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.111213202122599e-05.\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0935 - accuracy: 0.9610 - val_loss: 0.3270 - val_accuracy: 0.8739 - lr: 4.1112e-05\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9615\n",
      "Epoch 63: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0944 - accuracy: 0.9615 - val_loss: 0.4729 - val_accuracy: 0.8299 - lr: 4.2557e-04\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9725\n",
      "Epoch 64: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0710 - accuracy: 0.9725 - val_loss: 0.3011 - val_accuracy: 0.8686 - lr: 4.4052e-04\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0631 - accuracy: 0.9754\n",
      "Epoch 65: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0631 - accuracy: 0.9754 - val_loss: 0.4872 - val_accuracy: 0.8373 - lr: 4.5601e-04\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9670\n",
      "Epoch 66: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0859 - accuracy: 0.9670 - val_loss: 0.4130 - val_accuracy: 0.8458 - lr: 4.7203e-04\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9465\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 4.886186216026545e-05.\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.1291 - accuracy: 0.9465 - val_loss: 0.4500 - val_accuracy: 0.8362 - lr: 4.8862e-05\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9803\n",
      "Epoch 68: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0560 - accuracy: 0.9803 - val_loss: 0.3819 - val_accuracy: 0.8654 - lr: 5.0579e-04\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9715\n",
      "Epoch 69: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0730 - accuracy: 0.9715 - val_loss: 0.4006 - val_accuracy: 0.8262 - lr: 5.2356e-04\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9374\n",
      "Epoch 70: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1548 - accuracy: 0.9374 - val_loss: 0.5354 - val_accuracy: 0.7488 - lr: 5.4196e-04\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.8035\n",
      "Epoch 71: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.4046 - accuracy: 0.8035 - val_loss: 0.3697 - val_accuracy: 0.8055 - lr: 5.6101e-04\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9053\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 5.807243287563324e-05.\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.2102 - accuracy: 0.9053 - val_loss: 0.2897 - val_accuracy: 0.8574 - lr: 5.8072e-05\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1402 - accuracy: 0.9424\n",
      "Epoch 73: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1402 - accuracy: 0.9424 - val_loss: 0.4538 - val_accuracy: 0.8082 - lr: 6.0113e-04\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1255 - accuracy: 0.9485\n",
      "Epoch 74: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.1255 - accuracy: 0.9485 - val_loss: 0.6588 - val_accuracy: 0.7774 - lr: 6.2226e-04\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9580\n",
      "Epoch 75: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1057 - accuracy: 0.9580 - val_loss: 0.4212 - val_accuracy: 0.8203 - lr: 6.4412e-04\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9656\n",
      "Epoch 76: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0879 - accuracy: 0.9656 - val_loss: 0.4005 - val_accuracy: 0.8696 - lr: 6.6676e-04\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9665\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 6.901921587996185e-05.\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0813 - accuracy: 0.9665 - val_loss: 0.4147 - val_accuracy: 0.8442 - lr: 6.9019e-05\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9745\n",
      "Epoch 78: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0684 - accuracy: 0.9745 - val_loss: 0.4376 - val_accuracy: 0.8309 - lr: 7.1445e-04\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9656\n",
      "Epoch 79: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.0904 - accuracy: 0.9656 - val_loss: 0.3979 - val_accuracy: 0.8468 - lr: 7.3955e-04\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9763\n",
      "Epoch 80: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 30s 183ms/step - loss: 0.0626 - accuracy: 0.9763 - val_loss: 0.6770 - val_accuracy: 0.8161 - lr: 7.6554e-04\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9845\n",
      "Epoch 81: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0422 - accuracy: 0.9845 - val_loss: 0.5407 - val_accuracy: 0.8426 - lr: 7.9245e-04\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9800\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 8.202948956750334e-05.\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0537 - accuracy: 0.9800 - val_loss: 0.5131 - val_accuracy: 0.8468 - lr: 8.2029e-05\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9736\n",
      "Epoch 83: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0697 - accuracy: 0.9736 - val_loss: 0.4520 - val_accuracy: 0.8733 - lr: 8.4912e-04\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9804\n",
      "Epoch 84: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0549 - accuracy: 0.9804 - val_loss: 0.5500 - val_accuracy: 0.8362 - lr: 8.7896e-04\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9707\n",
      "Epoch 85: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0785 - accuracy: 0.9707 - val_loss: 0.4586 - val_accuracy: 0.8336 - lr: 9.0985e-04\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9768\n",
      "Epoch 86: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0628 - accuracy: 0.9768 - val_loss: 0.4477 - val_accuracy: 0.8559 - lr: 9.4182e-04\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9762\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 9.749223245307803e-05.\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0609 - accuracy: 0.9762 - val_loss: 0.4072 - val_accuracy: 0.8564 - lr: 9.7492e-05\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9818\n",
      "Epoch 88: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0483 - accuracy: 0.9818 - val_loss: 0.4588 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9827\n",
      "Epoch 89: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0461 - accuracy: 0.9827 - val_loss: 0.5223 - val_accuracy: 0.8315 - lr: 0.0010\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9791\n",
      "Epoch 90: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0593 - accuracy: 0.9791 - val_loss: 0.5121 - val_accuracy: 0.8400 - lr: 0.0011\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9754\n",
      "Epoch 91: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0640 - accuracy: 0.9754 - val_loss: 0.7971 - val_accuracy: 0.8135 - lr: 0.0011\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9725\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.0001158697297796607.\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.87864\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0665 - accuracy: 0.9725 - val_loss: 0.5303 - val_accuracy: 0.8511 - lr: 1.1587e-04\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9863\n",
      "Epoch 93: val_accuracy improved from 0.87864 to 0.88712, saving model to models/classification_wbc_binary_NEUTROPHIL_best.h5\n",
      "166/166 [==============================] - 31s 187ms/step - loss: 0.0382 - accuracy: 0.9863 - val_loss: 0.3750 - val_accuracy: 0.8871 - lr: 0.0012\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9826\n",
      "Epoch 94: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0467 - accuracy: 0.9826 - val_loss: 0.6554 - val_accuracy: 0.8272 - lr: 0.0012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9784\n",
      "Epoch 95: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 185ms/step - loss: 0.0546 - accuracy: 0.9784 - val_loss: 0.4712 - val_accuracy: 0.8659 - lr: 0.0013\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9751\n",
      "Epoch 96: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 30s 181ms/step - loss: 0.0633 - accuracy: 0.9751 - val_loss: 0.4138 - val_accuracy: 0.8108 - lr: 0.0013\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9815\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00013771143276244401.\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 184ms/step - loss: 0.0472 - accuracy: 0.9815 - val_loss: 0.4289 - val_accuracy: 0.8309 - lr: 1.3771e-04\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9708\n",
      "Epoch 98: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0747 - accuracy: 0.9708 - val_loss: 0.6067 - val_accuracy: 0.8410 - lr: 0.0014\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9681\n",
      "Epoch 99: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.0892 - accuracy: 0.9681 - val_loss: 0.6165 - val_accuracy: 0.8129 - lr: 0.0015\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - ETA: 0s - loss: 0.1862 - accuracy: 0.9139\n",
      "Epoch 100: val_accuracy did not improve from 0.88712\n",
      "166/166 [==============================] - 31s 186ms/step - loss: 0.1862 - accuracy: 0.9139 - val_loss: 0.5493 - val_accuracy: 0.7483 - lr: 0.0015\n"
     ]
    }
   ],
   "source": [
    "file_data = get_file_paths()\n",
    "\n",
    "def flow_df(generator,file_data,shuffle=True):\n",
    "    return generator.flow_from_dataframe(\n",
    "        file_data,\n",
    "        directory=None,\n",
    "        x_col='file_path',\n",
    "        y_col='classes',\n",
    "        target_size=(128,128),\n",
    "        classes=None,\n",
    "        # class_mode='binary',\n",
    "        batch_size=60,\n",
    "        shuffle=shuffle\n",
    "    );\n",
    "\n",
    "binary_models = []\n",
    "\n",
    "for c in CATEGORIES:\n",
    "    file_data['classes'] = file_data['category'].apply(lambda x: '0' if x==c else '1')\n",
    "    file_data_train = file_data[file_data['split']=='train'].reset_index(drop=True)\n",
    "    file_data_validation = file_data[file_data['split']=='validation'].reset_index(drop=True)\n",
    "    file_data_test = file_data[file_data['split']=='test'].reset_index(drop=True)\n",
    "\n",
    "    train_data = flow_df(training_datagen,file_data_train);\n",
    "    validation_data = flow_df(test_val_datagen,file_data_validation);\n",
    "    train_data_unshuffled = flow_df(training_datagen,file_data_train,shuffle=False);\n",
    "    validation_data_unshuffled = flow_df(test_val_datagen,file_data_validation,shuffle=False);\n",
    "    test_data_unshuffled = flow_df(test_val_datagen,file_data_test,shuffle=False);\n",
    "\n",
    "    new_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128,128,3)),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    binary_models.append(new_model)\n",
    "\n",
    "    model_path = 'models/classification_wbc_binary_'+c+'_best'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path+'.h5',\n",
    "        monitor = 'val_accuracy',\n",
    "        verbose = 1,\n",
    "        mode = 'max', \n",
    "        save_best_only = True\n",
    "    )\n",
    "\n",
    "    EPOCHS = 100\n",
    "    binary_models[-1].compile(loss = 'categorical_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(), # 'rmsprop',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "    model_history = binary_models[-1].fit(\n",
    "        train_data,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = validation_data,\n",
    "        verbose = 1,\n",
    "        callbacks = [reduce_lr,lr_scheduler,checkpoint] # observe for the future that 'reduce_lr' and 'lr_scheduler' cannot be mixed\n",
    "    )\n",
    "\n",
    "    with open(model_path+'_history.pickle','wb') as h:\n",
    "        pickle.dump(model_history.history,h,protocol=pickle.HIGHEST_PROTOCOL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "618438f6-932a-428e-99ad-17f8cf8e7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5a = tf.keras.models.load_model('models/classification_wbc_binary_EOSINOPHIL.h5')\n",
    "model_5b = tf.keras.models.load_model('models/classification_wbc_binary_LYMPHOCYTE.h5')\n",
    "model_5c = tf.keras.models.load_model('models/classification_wbc_binary_MONOCYTE.h5')\n",
    "model_5d = tf.keras.models.load_model('models/classification_wbc_binary_NEUTROPHIL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0a471d38-f9d5-4c56-bb19-4d8199291dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 61ms/step\n",
      "10/10 [==============================] - 1s 53ms/step\n",
      "10/10 [==============================] - 1s 47ms/step\n",
      "10/10 [==============================] - 1s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_5a = model_5a.predict(test_data_unshuffled)[:,0][np.newaxis]\n",
    "preds_5b = model_5b.predict(test_data_unshuffled)[:,0][np.newaxis]\n",
    "preds_5c = model_5c.predict(test_data_unshuffled)[:,0][np.newaxis]\n",
    "preds_5d = model_5d.predict(test_data_unshuffled)[:,0][np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "94cdff1b-52ee-4989-b30a-19fdbf808e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_5 = np.concatenate([preds_5a, preds_5b, preds_5c, preds_5d],axis=0).T\n",
    "preds_5 = [np.argmax(p) for p in preds_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "77ad296d-1a55-4803-bacc-2aaf81ce406a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_5 = np.mean([1 if preds_5[i]==test_data_unshuffled.classes[i] else 0 for i in range(len(preds_5))])\n",
    "accuracy_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "70bebf68-279f-4035-a3d7-5ff2da41c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1     2         3\n",
      "0  0.873333  0.006667  0.00  0.120000\n",
      "1  0.000000  1.000000  0.00  0.000000\n",
      "2  0.000000  0.000000  0.86  0.140000\n",
      "3  0.033333  0.000000  0.02  0.946667\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_data_unshuffled.classes, preds_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590dcdc-0b1c-4fb3-8572-a67d457c7361",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Outstanding - we've now achieved 92 percent!  The 'one vs rest' approach to this problem clearly bears fruit.\n",
    "<p style=\"font-weight: 500; color: #556;\">The next step is to train some additional models with a higher resolution, and tune other hyper parameters, to push out our benchmark even further. Lets try the same model architecture again, for our most challenging case, this tile using **binary_crossentropy**, and then also using a larger 240px resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "c97f3cc4-9c89-41e5-8a66-8372cf133c02",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 261s 2s/step - loss: 0.5709 - accuracy: 0.7487 - val_loss: 0.5650 - val_accuracy: 0.7493 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 270s 2s/step - loss: 0.5626 - accuracy: 0.7492 - val_loss: 0.5696 - val_accuracy: 0.7493 - lr: 5.4824e-05\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 253s 2s/step - loss: 0.5611 - accuracy: 0.7492 - val_loss: 0.5587 - val_accuracy: 0.7493 - lr: 6.0113e-05\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.5546 - accuracy: 0.7492 - val_loss: 0.5222 - val_accuracy: 0.7493 - lr: 6.5913e-05\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.5135 - accuracy: 0.7586 - val_loss: 0.4971 - val_accuracy: 0.8039 - lr: 7.2272e-05\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.4486 - accuracy: 0.8007 - val_loss: 0.3541 - val_accuracy: 0.8670 - lr: 7.9245e-05\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.3817 - accuracy: 0.8289 - val_loss: 0.3623 - val_accuracy: 0.8612 - lr: 8.6890e-05\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.3281 - accuracy: 0.8506 - val_loss: 0.2309 - val_accuracy: 0.8892 - lr: 9.5273e-05\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 245s 1s/step - loss: 0.2901 - accuracy: 0.8698 - val_loss: 0.1871 - val_accuracy: 0.9057 - lr: 1.0446e-04\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.2472 - accuracy: 0.8912 - val_loss: 0.1921 - val_accuracy: 0.9136 - lr: 1.1454e-04\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.2177 - accuracy: 0.9083 - val_loss: 0.2004 - val_accuracy: 0.8951 - lr: 1.2559e-04\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.1877 - accuracy: 0.9181 - val_loss: 0.2013 - val_accuracy: 0.9057 - lr: 1.3771e-04\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1652 - accuracy: 0.9304 - val_loss: 0.1864 - val_accuracy: 0.9163 - lr: 1.5100e-04\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.1585 - accuracy: 0.9362 - val_loss: 0.2134 - val_accuracy: 0.9147 - lr: 1.6557e-04\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 250s 2s/step - loss: 0.1458 - accuracy: 0.9369 - val_loss: 0.1897 - val_accuracy: 0.9184 - lr: 1.8154e-04\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 296s 2s/step - loss: 0.1369 - accuracy: 0.9438 - val_loss: 0.2109 - val_accuracy: 0.9194 - lr: 1.9905e-04\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1311 - accuracy: 0.9474 - val_loss: 0.2339 - val_accuracy: 0.8993 - lr: 2.1826e-04\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.1273 - accuracy: 0.9475 - val_loss: 0.2022 - val_accuracy: 0.9163 - lr: 2.3932e-04\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.1338 - accuracy: 0.9467 - val_loss: 0.2472 - val_accuracy: 0.9083 - lr: 2.6240e-04\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.1298 - accuracy: 0.9490 - val_loss: 0.2033 - val_accuracy: 0.9269 - lr: 2.8772e-04\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1157 - accuracy: 0.9511 - val_loss: 0.1929 - val_accuracy: 0.9232 - lr: 3.1548e-04\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.1306 - accuracy: 0.9470 - val_loss: 0.2046 - val_accuracy: 0.9247 - lr: 3.4592e-04\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.1055 - accuracy: 0.9600 - val_loss: 0.2554 - val_accuracy: 0.9004 - lr: 3.7929e-04\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.1008 - accuracy: 0.9603 - val_loss: 0.2739 - val_accuracy: 0.9136 - lr: 4.1588e-04\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.1086 - accuracy: 0.9549 - val_loss: 0.2040 - val_accuracy: 0.9131 - lr: 4.5601e-04\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1302 - accuracy: 0.9458 - val_loss: 0.2474 - val_accuracy: 0.9078 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.1075 - accuracy: 0.9570 - val_loss: 0.1842 - val_accuracy: 0.9184 - lr: 5.4824e-04\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1301 - accuracy: 0.9484 - val_loss: 0.1991 - val_accuracy: 0.9184 - lr: 6.0113e-04\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 249s 2s/step - loss: 0.0979 - accuracy: 0.9611 - val_loss: 0.2551 - val_accuracy: 0.9094 - lr: 6.5913e-04\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.0934 - accuracy: 0.9643 - val_loss: 0.1996 - val_accuracy: 0.9194 - lr: 7.2272e-04\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 256s 2s/step - loss: 0.1675 - accuracy: 0.9306 - val_loss: 0.1566 - val_accuracy: 0.9247 - lr: 7.9245e-04\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.1294 - accuracy: 0.9491 - val_loss: 0.1563 - val_accuracy: 0.9396 - lr: 8.6890e-04\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 250s 2s/step - loss: 0.1283 - accuracy: 0.9482 - val_loss: 0.1815 - val_accuracy: 0.9306 - lr: 9.5273e-04\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.0869 - accuracy: 0.9665 - val_loss: 0.3677 - val_accuracy: 0.9004 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.0939 - accuracy: 0.9637 - val_loss: 0.2513 - val_accuracy: 0.9232 - lr: 0.0011\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1400 - accuracy: 0.9444 - val_loss: 0.1921 - val_accuracy: 0.9290 - lr: 0.0013\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1103 - accuracy: 0.9589 - val_loss: 0.2082 - val_accuracy: 0.9226 - lr: 0.0014\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.0759 - accuracy: 0.9705 - val_loss: 0.2520 - val_accuracy: 0.9253 - lr: 0.0015\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 253s 2s/step - loss: 0.1419 - accuracy: 0.9455 - val_loss: 0.1767 - val_accuracy: 0.9290 - lr: 0.0017\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.0932 - accuracy: 0.9633 - val_loss: 0.2726 - val_accuracy: 0.9147 - lr: 0.0018\n",
      "Epoch 41/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.3908 - accuracy: 0.8415 - val_loss: 0.5683 - val_accuracy: 0.7493 - lr: 0.0020\n",
      "Epoch 42/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.5663 - accuracy: 0.7492 - val_loss: 0.5680 - val_accuracy: 0.7493 - lr: 0.0022\n",
      "Epoch 43/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.5668 - accuracy: 0.7492 - val_loss: 0.5720 - val_accuracy: 0.7493 - lr: 0.0024\n",
      "Epoch 44/50\n",
      "166/166 [==============================] - 246s 1s/step - loss: 0.5648 - accuracy: 0.7492 - val_loss: 0.5643 - val_accuracy: 0.7493 - lr: 0.0026\n",
      "Epoch 45/50\n",
      "166/166 [==============================] - 255s 2s/step - loss: 0.5671 - accuracy: 0.7492 - val_loss: 0.5710 - val_accuracy: 0.7493 - lr: 0.0029\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 253s 2s/step - loss: 0.5712 - accuracy: 0.7499 - val_loss: 0.5577 - val_accuracy: 0.7509 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 250s 2s/step - loss: 0.5426 - accuracy: 0.7506 - val_loss: 0.4956 - val_accuracy: 0.7509 - lr: 5.4824e-05\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.3890 - accuracy: 0.8264 - val_loss: 0.2711 - val_accuracy: 0.9020 - lr: 6.0113e-05\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.2581 - accuracy: 0.8992 - val_loss: 0.2794 - val_accuracy: 0.8797 - lr: 6.5913e-05\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.2019 - accuracy: 0.9241 - val_loss: 0.1260 - val_accuracy: 0.9470 - lr: 7.2272e-05\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.1730 - accuracy: 0.9303 - val_loss: 0.0950 - val_accuracy: 0.9735 - lr: 7.9245e-05\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.1410 - accuracy: 0.9423 - val_loss: 0.0876 - val_accuracy: 0.9931 - lr: 8.6890e-05\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.1245 - accuracy: 0.9514 - val_loss: 0.0665 - val_accuracy: 0.9931 - lr: 9.5273e-05\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 267s 2s/step - loss: 0.1141 - accuracy: 0.9536 - val_loss: 0.0490 - val_accuracy: 0.9995 - lr: 1.0446e-04\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 295s 2s/step - loss: 0.1042 - accuracy: 0.9603 - val_loss: 0.0577 - val_accuracy: 0.9910 - lr: 1.1454e-04\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.0725 - accuracy: 0.9705 - val_loss: 0.0540 - val_accuracy: 0.9947 - lr: 1.2559e-04\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.0740 - accuracy: 0.9721 - val_loss: 0.0439 - val_accuracy: 0.9963 - lr: 1.3771e-04\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 299s 2s/step - loss: 0.0931 - accuracy: 0.9634 - val_loss: 0.1011 - val_accuracy: 0.9618 - lr: 1.5100e-04\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.0918 - accuracy: 0.9635 - val_loss: 0.0529 - val_accuracy: 0.9878 - lr: 1.6557e-04\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.0576 - accuracy: 0.9765 - val_loss: 0.0436 - val_accuracy: 0.9883 - lr: 1.8154e-04\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.5686 - accuracy: 0.7499 - val_loss: 0.5555 - val_accuracy: 0.7509 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.5549 - accuracy: 0.7511 - val_loss: 0.5185 - val_accuracy: 0.7509 - lr: 5.4824e-05\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.4691 - accuracy: 0.7975 - val_loss: 0.3045 - val_accuracy: 0.9216 - lr: 6.0113e-05\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 246s 1s/step - loss: 0.2949 - accuracy: 0.8906 - val_loss: 0.2529 - val_accuracy: 0.9279 - lr: 6.5913e-05\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.2235 - accuracy: 0.9170 - val_loss: 0.2396 - val_accuracy: 0.8967 - lr: 7.2272e-05\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 294s 2s/step - loss: 0.1934 - accuracy: 0.9300 - val_loss: 0.3283 - val_accuracy: 0.9200 - lr: 7.9245e-05\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 293s 2s/step - loss: 0.1828 - accuracy: 0.9330 - val_loss: 0.2378 - val_accuracy: 0.8765 - lr: 8.6890e-05\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1627 - accuracy: 0.9392 - val_loss: 0.3092 - val_accuracy: 0.9232 - lr: 9.5273e-05\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1467 - accuracy: 0.9427 - val_loss: 0.2277 - val_accuracy: 0.9189 - lr: 1.0446e-04\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 299s 2s/step - loss: 0.1443 - accuracy: 0.9424 - val_loss: 0.3025 - val_accuracy: 0.9232 - lr: 1.1454e-04\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 291s 2s/step - loss: 0.1239 - accuracy: 0.9499 - val_loss: 0.1985 - val_accuracy: 0.9279 - lr: 1.2559e-04\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 290s 2s/step - loss: 0.1075 - accuracy: 0.9566 - val_loss: 0.2489 - val_accuracy: 0.9375 - lr: 1.3771e-04\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1106 - accuracy: 0.9561 - val_loss: 0.3679 - val_accuracy: 0.8914 - lr: 1.5100e-04\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1251 - accuracy: 0.9498 - val_loss: 0.2878 - val_accuracy: 0.9200 - lr: 1.6557e-04\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 291s 2s/step - loss: 0.1052 - accuracy: 0.9598 - val_loss: 0.2073 - val_accuracy: 0.9332 - lr: 1.8154e-04\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.0837 - accuracy: 0.9692 - val_loss: 0.4307 - val_accuracy: 0.9009 - lr: 1.9905e-04\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.0697 - accuracy: 0.9743 - val_loss: 0.3038 - val_accuracy: 0.9311 - lr: 2.1826e-04\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 291s 2s/step - loss: 0.0818 - accuracy: 0.9677 - val_loss: 0.3191 - val_accuracy: 0.9141 - lr: 2.3932e-04\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.0679 - accuracy: 0.9758 - val_loss: 0.3310 - val_accuracy: 0.9232 - lr: 2.6240e-04\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.0690 - accuracy: 0.9749 - val_loss: 0.3523 - val_accuracy: 0.9025 - lr: 2.8772e-04\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 292s 2s/step - loss: 0.1009 - accuracy: 0.9603 - val_loss: 0.1800 - val_accuracy: 0.9300 - lr: 3.1548e-04\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 287s 2s/step - loss: 0.1106 - accuracy: 0.9581 - val_loss: 0.2038 - val_accuracy: 0.9422 - lr: 3.4592e-04\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 246s 1s/step - loss: 0.0530 - accuracy: 0.9790 - val_loss: 0.3870 - val_accuracy: 0.9300 - lr: 3.7929e-04\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.0541 - accuracy: 0.9790 - val_loss: 0.3912 - val_accuracy: 0.9285 - lr: 4.1588e-04\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 246s 1s/step - loss: 0.0938 - accuracy: 0.9652 - val_loss: 0.2834 - val_accuracy: 0.9263 - lr: 4.5601e-04\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 261s 2s/step - loss: 0.1191 - accuracy: 0.9555 - val_loss: 0.4579 - val_accuracy: 0.9327 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.1070 - accuracy: 0.9569 - val_loss: 0.2489 - val_accuracy: 0.9258 - lr: 5.4824e-04\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.0658 - accuracy: 0.9748 - val_loss: 0.2872 - val_accuracy: 0.9359 - lr: 6.0113e-04\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 237s 1s/step - loss: 0.0364 - accuracy: 0.9879 - val_loss: 0.3360 - val_accuracy: 0.9412 - lr: 6.5913e-04\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 237s 1s/step - loss: 0.0788 - accuracy: 0.9697 - val_loss: 0.4209 - val_accuracy: 0.9316 - lr: 7.2272e-04\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 237s 1s/step - loss: 0.1245 - accuracy: 0.9527 - val_loss: 0.7044 - val_accuracy: 0.7509 - lr: 7.9245e-04\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 237s 1s/step - loss: 0.5678 - accuracy: 0.7511 - val_loss: 0.5736 - val_accuracy: 0.7509 - lr: 8.6890e-04\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 242s 1s/step - loss: 0.5633 - accuracy: 0.7511 - val_loss: 0.5665 - val_accuracy: 0.7509 - lr: 9.5273e-04\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 290s 2s/step - loss: 0.5568 - accuracy: 0.7511 - val_loss: 0.5200 - val_accuracy: 0.7509 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 289s 2s/step - loss: 0.4992 - accuracy: 0.7506 - val_loss: 0.4103 - val_accuracy: 0.7509 - lr: 0.0011\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 296s 2s/step - loss: 0.4022 - accuracy: 0.8032 - val_loss: 0.2469 - val_accuracy: 0.9269 - lr: 0.0013\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 291s 2s/step - loss: 0.3161 - accuracy: 0.8689 - val_loss: 0.2448 - val_accuracy: 0.9343 - lr: 0.0014\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 288s 2s/step - loss: 0.2620 - accuracy: 0.8923 - val_loss: 0.3519 - val_accuracy: 0.9115 - lr: 0.0015\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 289s 2s/step - loss: 0.2886 - accuracy: 0.8600 - val_loss: 0.2287 - val_accuracy: 0.9364 - lr: 0.0017\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 288s 2s/step - loss: 0.3954 - accuracy: 0.7989 - val_loss: 0.3809 - val_accuracy: 0.8516 - lr: 0.0018\n",
      "Epoch 41/50\n",
      "166/166 [==============================] - 288s 2s/step - loss: 0.2514 - accuracy: 0.8971 - val_loss: 0.2730 - val_accuracy: 0.9306 - lr: 0.0020\n",
      "Epoch 42/50\n",
      "166/166 [==============================] - 289s 2s/step - loss: 0.1679 - accuracy: 0.9311 - val_loss: 0.3424 - val_accuracy: 0.9327 - lr: 0.0022\n",
      "Epoch 43/50\n",
      "166/166 [==============================] - 288s 2s/step - loss: 0.5056 - accuracy: 0.8354 - val_loss: 0.5613 - val_accuracy: 0.7509 - lr: 0.0024\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 9957 validated image filenames belonging to 2 classes.\n",
      "Found 1887 validated image filenames belonging to 2 classes.\n",
      "Found 600 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "166/166 [==============================] - 258s 2s/step - loss: 0.5735 - accuracy: 0.7476 - val_loss: 0.5638 - val_accuracy: 0.7488 - lr: 5.0000e-05\n",
      "Epoch 2/50\n",
      "166/166 [==============================] - 252s 2s/step - loss: 0.5650 - accuracy: 0.7490 - val_loss: 0.5646 - val_accuracy: 0.7488 - lr: 5.4824e-05\n",
      "Epoch 3/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.5630 - accuracy: 0.7490 - val_loss: 0.5602 - val_accuracy: 0.7488 - lr: 6.0113e-05\n",
      "Epoch 4/50\n",
      "166/166 [==============================] - 248s 1s/step - loss: 0.5447 - accuracy: 0.7490 - val_loss: 0.5105 - val_accuracy: 0.7488 - lr: 6.5913e-05\n",
      "Epoch 5/50\n",
      "166/166 [==============================] - 253s 2s/step - loss: 0.5028 - accuracy: 0.7490 - val_loss: 0.4893 - val_accuracy: 0.7488 - lr: 7.2272e-05\n",
      "Epoch 6/50\n",
      "166/166 [==============================] - 257s 2s/step - loss: 0.4803 - accuracy: 0.7574 - val_loss: 0.4896 - val_accuracy: 0.7446 - lr: 7.9245e-05\n",
      "Epoch 7/50\n",
      "166/166 [==============================] - 250s 2s/step - loss: 0.4494 - accuracy: 0.7819 - val_loss: 0.5226 - val_accuracy: 0.7361 - lr: 8.6890e-05\n",
      "Epoch 8/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.4413 - accuracy: 0.7854 - val_loss: 0.4919 - val_accuracy: 0.7096 - lr: 9.5273e-05\n",
      "Epoch 9/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.4128 - accuracy: 0.8003 - val_loss: 0.4315 - val_accuracy: 0.8198 - lr: 1.0446e-04\n",
      "Epoch 10/50\n",
      "166/166 [==============================] - 251s 2s/step - loss: 0.3479 - accuracy: 0.8413 - val_loss: 0.3390 - val_accuracy: 0.8580 - lr: 1.1454e-04\n",
      "Epoch 11/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.3151 - accuracy: 0.8547 - val_loss: 0.3150 - val_accuracy: 0.8484 - lr: 1.2559e-04\n",
      "Epoch 12/50\n",
      "166/166 [==============================] - 266s 2s/step - loss: 0.2905 - accuracy: 0.8675 - val_loss: 0.2517 - val_accuracy: 0.8993 - lr: 1.3771e-04\n",
      "Epoch 13/50\n",
      "166/166 [==============================] - 247s 1s/step - loss: 0.2247 - accuracy: 0.9027 - val_loss: 0.2810 - val_accuracy: 0.8251 - lr: 1.5100e-04\n",
      "Epoch 14/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.2351 - accuracy: 0.8934 - val_loss: 0.2456 - val_accuracy: 0.8871 - lr: 1.6557e-04\n",
      "Epoch 15/50\n",
      "166/166 [==============================] - 249s 1s/step - loss: 0.2362 - accuracy: 0.8924 - val_loss: 0.2228 - val_accuracy: 0.8680 - lr: 1.8154e-04\n",
      "Epoch 16/50\n",
      "166/166 [==============================] - 249s 2s/step - loss: 0.2148 - accuracy: 0.9010 - val_loss: 0.2280 - val_accuracy: 0.8696 - lr: 1.9905e-04\n",
      "Epoch 17/50\n",
      "166/166 [==============================] - 256s 2s/step - loss: 0.2116 - accuracy: 0.9029 - val_loss: 0.2580 - val_accuracy: 0.8484 - lr: 2.1826e-04\n",
      "Epoch 18/50\n",
      "166/166 [==============================] - 275s 2s/step - loss: 0.1905 - accuracy: 0.9125 - val_loss: 0.2486 - val_accuracy: 0.8389 - lr: 2.3932e-04\n",
      "Epoch 19/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.2034 - accuracy: 0.9057 - val_loss: 0.3322 - val_accuracy: 0.8405 - lr: 2.6240e-04\n",
      "Epoch 20/50\n",
      "166/166 [==============================] - 283s 2s/step - loss: 0.1779 - accuracy: 0.9178 - val_loss: 0.2754 - val_accuracy: 0.8198 - lr: 2.8772e-04\n",
      "Epoch 21/50\n",
      "166/166 [==============================] - 272s 2s/step - loss: 0.2330 - accuracy: 0.8868 - val_loss: 0.3038 - val_accuracy: 0.8299 - lr: 3.1548e-04\n",
      "Epoch 22/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1883 - accuracy: 0.9139 - val_loss: 0.2709 - val_accuracy: 0.8368 - lr: 3.4592e-04\n",
      "Epoch 23/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1655 - accuracy: 0.9261 - val_loss: 0.2585 - val_accuracy: 0.8643 - lr: 3.7929e-04\n",
      "Epoch 24/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1677 - accuracy: 0.9262 - val_loss: 0.3408 - val_accuracy: 0.8474 - lr: 4.1588e-04\n",
      "Epoch 25/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1495 - accuracy: 0.9343 - val_loss: 0.3178 - val_accuracy: 0.8431 - lr: 4.5601e-04\n",
      "Epoch 26/50\n",
      "166/166 [==============================] - 273s 2s/step - loss: 0.1591 - accuracy: 0.9287 - val_loss: 0.2833 - val_accuracy: 0.8029 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "166/166 [==============================] - 273s 2s/step - loss: 0.1671 - accuracy: 0.9242 - val_loss: 0.2560 - val_accuracy: 0.8347 - lr: 5.4824e-04\n",
      "Epoch 28/50\n",
      "166/166 [==============================] - 276s 2s/step - loss: 0.1404 - accuracy: 0.9394 - val_loss: 0.3284 - val_accuracy: 0.8378 - lr: 6.0113e-04\n",
      "Epoch 29/50\n",
      "166/166 [==============================] - 272s 2s/step - loss: 0.3220 - accuracy: 0.8444 - val_loss: 0.3999 - val_accuracy: 0.7594 - lr: 6.5913e-04\n",
      "Epoch 30/50\n",
      "166/166 [==============================] - 275s 2s/step - loss: 0.3041 - accuracy: 0.8485 - val_loss: 0.2859 - val_accuracy: 0.8405 - lr: 7.2272e-04\n",
      "Epoch 31/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1742 - accuracy: 0.9244 - val_loss: 0.2641 - val_accuracy: 0.8617 - lr: 7.9245e-04\n",
      "Epoch 32/50\n",
      "166/166 [==============================] - 275s 2s/step - loss: 0.1534 - accuracy: 0.9334 - val_loss: 0.3939 - val_accuracy: 0.8479 - lr: 8.6890e-04\n",
      "Epoch 33/50\n",
      "166/166 [==============================] - 283s 2s/step - loss: 0.2688 - accuracy: 0.8800 - val_loss: 0.3086 - val_accuracy: 0.8161 - lr: 9.5273e-04\n",
      "Epoch 34/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1615 - accuracy: 0.9255 - val_loss: 0.3119 - val_accuracy: 0.8304 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "166/166 [==============================] - 275s 2s/step - loss: 0.1503 - accuracy: 0.9347 - val_loss: 0.3181 - val_accuracy: 0.8331 - lr: 0.0011\n",
      "Epoch 36/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.1949 - accuracy: 0.9125 - val_loss: 0.4428 - val_accuracy: 0.8378 - lr: 0.0013\n",
      "Epoch 37/50\n",
      "166/166 [==============================] - 274s 2s/step - loss: 0.2072 - accuracy: 0.9007 - val_loss: 0.3281 - val_accuracy: 0.8039 - lr: 0.0014\n",
      "Epoch 38/50\n",
      "166/166 [==============================] - 272s 2s/step - loss: 0.1889 - accuracy: 0.9151 - val_loss: 0.3871 - val_accuracy: 0.8325 - lr: 0.0015\n",
      "Epoch 39/50\n",
      "166/166 [==============================] - 273s 2s/step - loss: 0.1644 - accuracy: 0.9284 - val_loss: 0.3523 - val_accuracy: 0.8315 - lr: 0.0017\n",
      "Epoch 40/50\n",
      "166/166 [==============================] - 276s 2s/step - loss: 0.3645 - accuracy: 0.8397 - val_loss: 0.5679 - val_accuracy: 0.7488 - lr: 0.0018\n"
     ]
    }
   ],
   "source": [
    "file_data = get_file_paths()\n",
    "\n",
    "def flow_df(generator,file_data,shuffle=True):\n",
    "    return generator.flow_from_dataframe(\n",
    "        file_data,\n",
    "        directory=None,\n",
    "        x_col='file_path',\n",
    "        y_col='classes',\n",
    "        target_size=(240,240),\n",
    "        classes=None,\n",
    "        class_mode='binary',\n",
    "        batch_size=60,\n",
    "        shuffle=shuffle\n",
    "    );\n",
    "\n",
    "binary_models = []\n",
    "\n",
    "for c in CATEGORIES[:1]:\n",
    "    file_data['classes'] = file_data['category'].apply(lambda x: '0' if x==c else '1')\n",
    "    file_data_train = file_data[file_data['split']=='train'].reset_index(drop=True)\n",
    "    file_data_validation = file_data[file_data['split']=='validation'].reset_index(drop=True)\n",
    "    file_data_test = file_data[file_data['split']=='test'].reset_index(drop=True)\n",
    "\n",
    "    train_data = flow_df(training_datagen,file_data_train);\n",
    "    validation_data = flow_df(test_val_datagen,file_data_validation);\n",
    "    train_data_unshuffled = flow_df(training_datagen,file_data_train,shuffle=False);\n",
    "    validation_data_unshuffled = flow_df(test_val_datagen,file_data_validation,shuffle=False);\n",
    "    test_data_unshuffled = flow_df(test_val_datagen,file_data_test,shuffle=False);\n",
    "    break\n",
    "    \n",
    "    new_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu', input_shape=(240, 240, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    binary_models.append(new_model)\n",
    "\n",
    "    model_path = 'models/classification_wbc_binary2_'+c+'.h5'\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_path,\n",
    "        monitor = 'val_accuracy',\n",
    "        verbose = 0,\n",
    "        mode = 'max', \n",
    "        save_best_only = True\n",
    "    )\n",
    "\n",
    "    EPOCHS = 50\n",
    "    lr_scheduler = LearningRateScheduler(lambda epoch: 5e-5 * 10**(2*epoch/EPOCHS))\n",
    "    \n",
    "    binary_models[-1].compile(loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(), # 'rmsprop',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "    model_history = binary_models[-1].fit(\n",
    "        train_data,\n",
    "        epochs = EPOCHS,\n",
    "        validation_data = validation_data,\n",
    "        verbose = 1,\n",
    "        callbacks = [lr_scheduler,checkpoint]\n",
    "    )\n",
    "\n",
    "    with open(model_path+'_history.pickle','wb') as h:\n",
    "        pickle.dump(model_history.history,h,protocol=pickle.HIGHEST_PROTOCOL);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "0e486f5e-6ec7-4715-9aa9-908ed5b656fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6a = tf.keras.models.load_model('models/classification_wbc_binary2_EOSINOPHIL.h5')\n",
    "model_6b = tf.keras.models.load_model('models/classification_wbc_binary2_LYMPHOCYTE.h5')\n",
    "model_6c = tf.keras.models.load_model('models/classification_wbc_binary2_MONOCYTE.h5')\n",
    "model_6d = tf.keras.models.load_model('models/classification_wbc_binary2_NEUTROPHIL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "227fa7ca-f87a-425c-8969-052ee3f8c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 4s 486ms/step\n",
      "10/10 [==============================] - 4s 484ms/step\n",
      "10/10 [==============================] - 4s 482ms/step\n",
      "10/10 [==============================] - 4s 486ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_6a = model_6a.predict(test_data_unshuffled)\n",
    "preds_6b = model_6b.predict(test_data_unshuffled)\n",
    "preds_6c = model_6c.predict(test_data_unshuffled)\n",
    "preds_6d = model_6d.predict(test_data_unshuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "a6bc7a2f-4ab9-4a4e-8c50-5e07fa618eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_6 = np.concatenate([preds_6a, preds_6b, preds_6c, preds_6d],axis=1)\n",
    "preds_6 = [np.argmin(p) for p in preds_6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "22493e60-414b-405c-8cdc-ecdef9b6adee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8966666666666666"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_accuracy_binary = np.mean([1 if preds_6[i]==test_data_unshuffled_2.classes[i] else 0 for i in range(len(preds_6))])\n",
    "combined_accuracy_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "80932388-8902-41ab-a3ca-7b3efe531b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.880000  0.000000  0.000000  0.120000\n",
      "1  0.000000  1.000000  0.000000  0.000000\n",
      "2  0.000000  0.000000  0.813333  0.186667\n",
      "3  0.066667  0.026667  0.013333  0.893333\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_data_unshuffled_2.classes, preds_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ce4c8-9f4e-409f-83df-936bbf8243e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "####\n",
    "#### **TRANSFER LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55604ba-5b16-4180-9148-aca9326abe02",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">Can we get any incremental benefits from using existing models? Here we will explore using a pretrained model to see if any generic information can be learned from its convolutional layers and work to out benefit.\n",
    "<p style=\"font-weight: 500; color: #556;\">Here, we will begin to use the tf.Data.dataset format for our training, to aid speed and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "acfda975-117c-4d5a-84ab-ec4ba2fded3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "(IMG_HEIGHT,IMG_WIDTH) = (240,240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2b87ffa3-2b9f-470b-8af8-9ca4208c7e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9957 files belonging to 4 classes.\n",
      "Found 1887 files belonging to 4 classes.\n",
      "Found 600 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_7 = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH+'/train',\n",
    "    labels = 'inferred',\n",
    "    label_mode = 'int',\n",
    "    class_names = None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=84,\n",
    "    validation_split=None,\n",
    "    subset=None,\n",
    "    interpolation='bilinear',\n",
    "    follow_links=False,\n",
    "    crop_to_aspect_ratio=False\n",
    ")\n",
    "\n",
    "validation_data_7 = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH+'/validation',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=84,\n",
    ")\n",
    "\n",
    "test_data_unshuffled_7 = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH+'/test',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    shuffle=False, # easier if we shuffle only when we're ready to avoid gotchas\n",
    "    seed=84,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "70ad07f3-2bae-43ec-a122-f4c41e7c0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_7 = train_data_7.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE) #.batch(batch_size)\n",
    "validation_data_7 = validation_data_7.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_data_unshuffled_7 = test_data_unshuffled_7.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24feeb7f-8a6d-4787-bd2a-ac25f7316d2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 600; color: #556;\">FIRST BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "554bdad1-147b-459a-ae6f-7ebecaae290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(240, 240, 3),\n",
    "    include_top=False)\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs_new = tf.keras.Input(shape=(240, 240, 3))\n",
    "x = tf.keras.applications.xception.preprocess_input(inputs_new) # gives us values in the range [-1,1]\n",
    "x = base_model(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs_new = tf.keras.layers.Dense(4)(x) # ,activation='softmax'\n",
    "\n",
    "model_7 = tf.keras.Model(inputs_new, outputs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "a172d960-5c01-4ef1-ac0e-a0ef7cdf0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "853c9431-c170-4e3f-953f-4351d3f7ba6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "312/312 [==============================] - 210s 654ms/step - loss: 1.1592 - accuracy: 0.4948 - val_loss: 1.2055 - val_accuracy: 0.4732\n",
      "Epoch 2/10\n",
      "312/312 [==============================] - 219s 702ms/step - loss: 0.9641 - accuracy: 0.6182 - val_loss: 1.1728 - val_accuracy: 0.4812\n",
      "Epoch 3/10\n",
      "312/312 [==============================] - 226s 725ms/step - loss: 0.8805 - accuracy: 0.6599 - val_loss: 1.1493 - val_accuracy: 0.4971\n",
      "Epoch 4/10\n",
      "312/312 [==============================] - 210s 673ms/step - loss: 0.8220 - accuracy: 0.6911 - val_loss: 1.1809 - val_accuracy: 0.4870\n",
      "Epoch 5/10\n",
      "312/312 [==============================] - 211s 674ms/step - loss: 0.7802 - accuracy: 0.7037 - val_loss: 1.1689 - val_accuracy: 0.4918\n",
      "Epoch 6/10\n",
      "312/312 [==============================] - 210s 673ms/step - loss: 0.7476 - accuracy: 0.7203 - val_loss: 1.1714 - val_accuracy: 0.5003\n",
      "Epoch 7/10\n",
      "312/312 [==============================] - 211s 678ms/step - loss: 0.7166 - accuracy: 0.7335 - val_loss: 1.1513 - val_accuracy: 0.5045\n",
      "Epoch 8/10\n",
      "312/312 [==============================] - 212s 679ms/step - loss: 0.6909 - accuracy: 0.7463 - val_loss: 1.1510 - val_accuracy: 0.5140\n",
      "Epoch 9/10\n",
      "312/312 [==============================] - 211s 677ms/step - loss: 0.6725 - accuracy: 0.7540 - val_loss: 1.1422 - val_accuracy: 0.5156\n",
      "Epoch 10/10\n",
      "312/312 [==============================] - 212s 681ms/step - loss: 0.6523 - accuracy: 0.7616 - val_loss: 1.1657 - val_accuracy: 0.5125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4e4984bb0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7.fit(\n",
    "    train_data_7,\n",
    "    validation_data = validation_data_7,\n",
    "    batch_size = 32,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104d051-715b-4fda-877b-37f6392b1944",
   "metadata": {},
   "source": [
    "######\n",
    "<p style=\"font-weight: 500; color: #556;\">We can quickly see that the model, with its layers frozen, is prone to heavily overfit the training set, far more than we have observed previously.  This is hence not benefitial, and we may stand more to gain by unfreezing the model's layers and allowing all of the weights to get updated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d845f-dcad-4426-8b34-bb139bf416eb",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 600; color: #556;\">UNFREEZING AND FINE-TUNING</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "f39186c3-c71e-43a0-8609-b5949cf091fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "312/312 [==============================] - 625s 2s/step - loss: 0.2041 - accuracy: 0.9225 - val_loss: 1.4458 - val_accuracy: 0.7875 - lr: 5.0000e-05\n",
      "Epoch 2/10\n",
      "312/312 [==============================] - 652s 2s/step - loss: 0.0271 - accuracy: 0.9904 - val_loss: 1.5565 - val_accuracy: 0.7716 - lr: 6.2946e-05\n",
      "Epoch 3/10\n",
      "312/312 [==============================] - 620s 2s/step - loss: 0.0434 - accuracy: 0.9852 - val_loss: 1.4082 - val_accuracy: 0.8776 - lr: 7.9245e-05\n",
      "Epoch 4/10\n",
      "312/312 [==============================] - 631s 2s/step - loss: 3.7045e-04 - accuracy: 1.0000 - val_loss: 1.8114 - val_accuracy: 0.8638 - lr: 9.9763e-05\n",
      "Epoch 5/10\n",
      "312/312 [==============================] - 627s 2s/step - loss: 1.9093e-05 - accuracy: 1.0000 - val_loss: 1.8800 - val_accuracy: 0.8659 - lr: 1.2559e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "base_model.trainable = True # unfreezing *all* the layers unless there are any BatchNorms in there\n",
    "\n",
    "model_7.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-5),\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 2,\n",
    "    mode = 'min',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(\n",
    "    lambda epoch: 5e-5 * 10**(1*epoch/EPOCHS)\n",
    ")\n",
    "\n",
    "model_7.fit(\n",
    "    train_data_7,\n",
    "    validation_data = validation_data_7,\n",
    "    batch_size = 32,\n",
    "    epochs = EPOCHS,\n",
    "    callbacks = [early_stopping,lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6f413-d6e5-4413-950d-26e1e7c7fc6b",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">At this point, we have achieved 100% accuracy on the training data, so there is little point in continuing further.  Our highest accuracy has given us 87.76%, which is still below the level of our previous top performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b282062b-5edb-48ce-9cbd-32f42782cf25",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 600; color: #556;\">USING ONLY A PART OF THE BASE MODEL</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106a464-ba51-4d4e-8269-ce657dd6114a",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Here, we explors using just a part of the pretrained model, to see the effects of only using earlier layers before the Pooling Layers made the outputs small.  Intuitively, we my be able to gain by discarding very specific information in the later layers, and keeping more generic information\n",
    "<p style=\"font-weight: 500; color: #556;\">We'll introduce some new techniques here, including new ways to define the learning rate callbacks, as well as a method for augmentation that's compatible with the new dataset formats we're using for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27704928-b2b3-4250-8f58-aea343316dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_trajectory(epochs, start=1e-5, end=1e-3, mode='linear'):\n",
    "    # modes can be linear, exponential, plateau\n",
    "    if mode == 'linear':\n",
    "        return LearningRateScheduler(lambda epoch: start + ((end-start)*epoch/epochs))\n",
    "    elif (mode == 'exp') or (mode == 'exponential'):\n",
    "        return LearningRateScheduler(lambda epoch: start * 10**(np.log10(end/start)*(epoch)/(epochs-1)))\n",
    "    #elif mode == 'plateau':\n",
    "    #    return LearningRateScheduler(lambda epoch: start * np.log10(epoch/epochs)/np.log10(end/start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7b92f-3d21-4c74-aa98-36046c53be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('CPU'):\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(),  # 'horizontal'),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(-0.3, 0.3)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9317de-889b-480b-b258-28000b29bb27",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Below we quickly inspect the layers, and can see where we shift from an image height/width of 30 down to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "77fb53b0-57b1-4940-a627-9752fe0e7116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 (None, 30, 30, 256) add_73\n",
      "26 (None, 30, 30, 256) block4_sepconv1_act\n",
      "27 (None, 30, 30, 728) block4_sepconv1\n",
      "28 (None, 30, 30, 728) block4_sepconv1_bn\n",
      "29 (None, 30, 30, 728) block4_sepconv2_act\n",
      "30 (None, 30, 30, 728) block4_sepconv2\n",
      "31 (None, 30, 30, 728) block4_sepconv2_bn\n",
      "32 (None, 15, 15, 728) conv2d_100\n",
      "33 (None, 15, 15, 728) block4_pool\n",
      "34 (None, 15, 15, 728) batch_normalization_26\n"
     ]
    }
   ],
   "source": [
    "for l in range(25, 35):\n",
    "    print(l, base_model.layers[l].output.shape, base_model.layers[l].name) #.summary() #.layers[30] #.output.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149569cd-4712-4854-88f6-f03b1a5fcb4b",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">Let's try catching layer 30, right before the batch normalization layer.  We,ll use the funtional API to define a new model at the output point we desire, and then feed this model in as the base model for another new model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "08f28474-6536-4d9c-9162-b1de2f8e10e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(240, 240, 3),\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "# here we can make the model return whichever layer we want\n",
    "base_model_2 = tf.keras.Model(inputs=base_model.input, outputs=base_model.layers[30].output)\n",
    "base_model_2.trainable = True\n",
    "\n",
    "inputs_new = tf.keras.Input(shape=(240, 240, 3))\n",
    "x = data_augmentation(inputs_new)  # data augmentation for tf.Data.datasets\n",
    "x = tf.keras.applications.xception.preprocess_input(x)  # gives us values in the range [-1,1]\n",
    "x = base_model_2(x, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs_new = tf.keras.layers.Dense(4)(x)  # activation='softmax'\n",
    "\n",
    "model_8 = tf.keras.Model(inputs_new, outputs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fa519de2-4a7e-4f7f-ab80-d0e60fde8dcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "312/312 [==============================] - 281s 894ms/step - loss: 1.3993 - accuracy: 0.2592 - val_loss: 1.3814 - val_accuracy: 0.2591 - lr: 9.5455e-06\n",
      "Epoch 2/100\n",
      "312/312 [==============================] - 279s 894ms/step - loss: 1.3767 - accuracy: 0.2991 - val_loss: 1.3723 - val_accuracy: 0.3158 - lr: 1.0000e-05\n",
      "Epoch 3/100\n",
      "312/312 [==============================] - 276s 884ms/step - loss: 1.3662 - accuracy: 0.3270 - val_loss: 1.3634 - val_accuracy: 0.3127 - lr: 1.0476e-05\n",
      "Epoch 4/100\n",
      "312/312 [==============================] - 276s 885ms/step - loss: 1.3565 - accuracy: 0.3452 - val_loss: 1.3551 - val_accuracy: 0.3763 - lr: 1.0975e-05\n",
      "Epoch 5/100\n",
      "312/312 [==============================] - 288s 922ms/step - loss: 1.3467 - accuracy: 0.3563 - val_loss: 1.3430 - val_accuracy: 0.3784 - lr: 1.1498e-05\n",
      "Epoch 6/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 1.3346 - accuracy: 0.3709 - val_loss: 1.3340 - val_accuracy: 0.4388 - lr: 1.2045e-05\n",
      "Epoch 7/100\n",
      "312/312 [==============================] - 302s 966ms/step - loss: 1.3237 - accuracy: 0.3873 - val_loss: 1.3229 - val_accuracy: 0.4356 - lr: 1.2619e-05\n",
      "Epoch 8/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 1.3116 - accuracy: 0.4010 - val_loss: 1.3211 - val_accuracy: 0.3556 - lr: 1.3219e-05\n",
      "Epoch 9/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 1.2973 - accuracy: 0.4140 - val_loss: 1.2932 - val_accuracy: 0.4494 - lr: 1.3849e-05\n",
      "Epoch 10/100\n",
      "312/312 [==============================] - 302s 969ms/step - loss: 1.2773 - accuracy: 0.4353 - val_loss: 1.2790 - val_accuracy: 0.4033 - lr: 1.4508e-05\n",
      "Epoch 11/100\n",
      "312/312 [==============================] - 302s 966ms/step - loss: 1.2641 - accuracy: 0.4411 - val_loss: 1.2630 - val_accuracy: 0.4589 - lr: 1.5199e-05\n",
      "Epoch 12/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 1.2438 - accuracy: 0.4611 - val_loss: 1.2483 - val_accuracy: 0.4658 - lr: 1.5923e-05\n",
      "Epoch 13/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 1.2245 - accuracy: 0.4740 - val_loss: 1.2302 - val_accuracy: 0.4807 - lr: 1.6681e-05\n",
      "Epoch 14/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 1.2049 - accuracy: 0.4868 - val_loss: 1.2050 - val_accuracy: 0.4865 - lr: 1.7475e-05\n",
      "Epoch 15/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 1.1729 - accuracy: 0.5088 - val_loss: 1.1878 - val_accuracy: 0.4955 - lr: 1.8307e-05\n",
      "Epoch 16/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 1.1523 - accuracy: 0.5114 - val_loss: 1.1631 - val_accuracy: 0.5172 - lr: 1.9179e-05\n",
      "Epoch 17/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 1.1292 - accuracy: 0.5267 - val_loss: 1.1406 - val_accuracy: 0.5125 - lr: 2.0092e-05\n",
      "Epoch 18/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 1.0964 - accuracy: 0.5432 - val_loss: 1.1051 - val_accuracy: 0.5395 - lr: 2.1049e-05\n",
      "Epoch 19/100\n",
      "312/312 [==============================] - 302s 968ms/step - loss: 1.0600 - accuracy: 0.5608 - val_loss: 1.0666 - val_accuracy: 0.5782 - lr: 2.2051e-05\n",
      "Epoch 20/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 1.0292 - accuracy: 0.5748 - val_loss: 1.0648 - val_accuracy: 0.5257 - lr: 2.3101e-05\n",
      "Epoch 21/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.9920 - accuracy: 0.5975 - val_loss: 0.9987 - val_accuracy: 0.6041 - lr: 2.4201e-05\n",
      "Epoch 22/100\n",
      "312/312 [==============================] - 300s 962ms/step - loss: 0.9561 - accuracy: 0.6170 - val_loss: 0.9610 - val_accuracy: 0.6222 - lr: 2.5354e-05\n",
      "Epoch 23/100\n",
      "312/312 [==============================] - 301s 963ms/step - loss: 0.9206 - accuracy: 0.6377 - val_loss: 0.9156 - val_accuracy: 0.6534 - lr: 2.6561e-05\n",
      "Epoch 24/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.8761 - accuracy: 0.6541 - val_loss: 0.8830 - val_accuracy: 0.6529 - lr: 2.7826e-05\n",
      "Epoch 25/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.8234 - accuracy: 0.6804 - val_loss: 0.8225 - val_accuracy: 0.6534 - lr: 2.9151e-05\n",
      "Epoch 26/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.7877 - accuracy: 0.6935 - val_loss: 0.7982 - val_accuracy: 0.6757 - lr: 3.0539e-05\n",
      "Epoch 27/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.7226 - accuracy: 0.7279 - val_loss: 0.7263 - val_accuracy: 0.7202 - lr: 3.1993e-05\n",
      "Epoch 28/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.6796 - accuracy: 0.7410 - val_loss: 0.6928 - val_accuracy: 0.7366 - lr: 3.3516e-05\n",
      "Epoch 29/100\n",
      "312/312 [==============================] - 302s 968ms/step - loss: 0.6374 - accuracy: 0.7561 - val_loss: 0.6452 - val_accuracy: 0.7414 - lr: 3.5112e-05\n",
      "Epoch 30/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.5945 - accuracy: 0.7749 - val_loss: 0.6203 - val_accuracy: 0.7493 - lr: 3.6784e-05\n",
      "Epoch 31/100\n",
      "312/312 [==============================] - 302s 969ms/step - loss: 0.5633 - accuracy: 0.7815 - val_loss: 0.5890 - val_accuracy: 0.7700 - lr: 3.8535e-05\n",
      "Epoch 32/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 0.5246 - accuracy: 0.7902 - val_loss: 0.5864 - val_accuracy: 0.7525 - lr: 4.0370e-05\n",
      "Epoch 33/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 0.4955 - accuracy: 0.8053 - val_loss: 0.5292 - val_accuracy: 0.7615 - lr: 4.2292e-05\n",
      "Epoch 34/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 0.4752 - accuracy: 0.8071 - val_loss: 0.5294 - val_accuracy: 0.7928 - lr: 4.4306e-05\n",
      "Epoch 35/100\n",
      "312/312 [==============================] - 302s 968ms/step - loss: 0.4503 - accuracy: 0.8205 - val_loss: 0.4885 - val_accuracy: 0.7944 - lr: 4.6416e-05\n",
      "Epoch 36/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 0.4105 - accuracy: 0.8398 - val_loss: 0.5321 - val_accuracy: 0.7780 - lr: 4.8626e-05\n",
      "Epoch 37/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.3972 - accuracy: 0.8410 - val_loss: 0.4478 - val_accuracy: 0.8055 - lr: 5.0941e-05\n",
      "Epoch 38/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.3829 - accuracy: 0.8455 - val_loss: 0.4551 - val_accuracy: 0.8135 - lr: 5.3367e-05\n",
      "Epoch 39/100\n",
      "312/312 [==============================] - 300s 963ms/step - loss: 0.3605 - accuracy: 0.8576 - val_loss: 0.4489 - val_accuracy: 0.8166 - lr: 5.5908e-05\n",
      "Epoch 40/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 0.3396 - accuracy: 0.8674 - val_loss: 0.3661 - val_accuracy: 0.8315 - lr: 5.8570e-05\n",
      "Epoch 41/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.3268 - accuracy: 0.8708 - val_loss: 0.5058 - val_accuracy: 0.7753 - lr: 6.1359e-05\n",
      "Epoch 42/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.3104 - accuracy: 0.8788 - val_loss: 0.3609 - val_accuracy: 0.8368 - lr: 6.4281e-05\n",
      "Epoch 43/100\n",
      "312/312 [==============================] - 300s 963ms/step - loss: 0.2889 - accuracy: 0.8897 - val_loss: 0.3520 - val_accuracy: 0.8389 - lr: 6.7342e-05\n",
      "Epoch 44/100\n",
      "312/312 [==============================] - 302s 968ms/step - loss: 0.2751 - accuracy: 0.8966 - val_loss: 0.3618 - val_accuracy: 0.8256 - lr: 7.0548e-05\n",
      "Epoch 45/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.2630 - accuracy: 0.9001 - val_loss: 0.3492 - val_accuracy: 0.8421 - lr: 7.3907e-05\n",
      "Epoch 46/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.2425 - accuracy: 0.9055 - val_loss: 0.3336 - val_accuracy: 0.8521 - lr: 7.7426e-05\n",
      "Epoch 47/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.2391 - accuracy: 0.9073 - val_loss: 0.3220 - val_accuracy: 0.8643 - lr: 8.1113e-05\n",
      "Epoch 48/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.2180 - accuracy: 0.9193 - val_loss: 0.2839 - val_accuracy: 0.8670 - lr: 8.4975e-05\n",
      "Epoch 49/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.2021 - accuracy: 0.9248 - val_loss: 0.2912 - val_accuracy: 0.8654 - lr: 8.9022e-05\n",
      "Epoch 50/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.2010 - accuracy: 0.9213 - val_loss: 0.3779 - val_accuracy: 0.8203 - lr: 9.3260e-05\n",
      "Epoch 51/100\n",
      "312/312 [==============================] - 301s 963ms/step - loss: 0.1935 - accuracy: 0.9270 - val_loss: 0.2758 - val_accuracy: 0.8903 - lr: 9.7701e-05\n",
      "Epoch 52/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 0.1828 - accuracy: 0.9297 - val_loss: 0.2637 - val_accuracy: 0.8691 - lr: 1.0235e-04\n",
      "Epoch 53/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 0.1662 - accuracy: 0.9366 - val_loss: 0.4329 - val_accuracy: 0.8235 - lr: 1.0723e-04\n",
      "Epoch 54/100\n",
      "312/312 [==============================] - 301s 964ms/step - loss: 0.1570 - accuracy: 0.9413 - val_loss: 0.2841 - val_accuracy: 0.8686 - lr: 1.1233e-04\n",
      "Epoch 55/100\n",
      "312/312 [==============================] - 300s 963ms/step - loss: 0.1522 - accuracy: 0.9423 - val_loss: 0.3518 - val_accuracy: 0.8797 - lr: 1.1768e-04\n",
      "Epoch 56/100\n",
      "312/312 [==============================] - 301s 963ms/step - loss: 0.1589 - accuracy: 0.9395 - val_loss: 0.2316 - val_accuracy: 0.8855 - lr: 1.2328e-04\n",
      "Epoch 57/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.1441 - accuracy: 0.9445 - val_loss: 0.2440 - val_accuracy: 0.8802 - lr: 1.2915e-04\n",
      "Epoch 58/100\n",
      "312/312 [==============================] - 302s 967ms/step - loss: 0.1303 - accuracy: 0.9491 - val_loss: 0.3435 - val_accuracy: 0.8389 - lr: 1.3530e-04\n",
      "Epoch 59/100\n",
      "312/312 [==============================] - 301s 965ms/step - loss: 0.1203 - accuracy: 0.9541 - val_loss: 0.3165 - val_accuracy: 0.8490 - lr: 1.4175e-04\n",
      "Epoch 60/100\n",
      "312/312 [==============================] - 301s 966ms/step - loss: 0.1252 - accuracy: 0.9521 - val_loss: 0.3175 - val_accuracy: 0.8532 - lr: 1.4850e-04\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "model_8.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = learning_trajectory(EPOCHS, 1e-5, 1e-3, 'exp')\n",
    "\n",
    "history = model_8.fit(\n",
    "    train_data,\n",
    "    validation_data=validation_data,\n",
    "    batch_size=32,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "10b75859-e05b-4bf9-82d3-a549e37402df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model_8, 'models/classification_xception_multiclass.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83e59e77-6cf2-4fc8-ae3d-d85ddeeebe2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_8 = [np.argmax(p) for p in model_8.predict(test_data_unshuffled_7, verbose=0)]\n",
    "true_classes_8 = test_data_unshuffled_7.map(lambda x,y: y).unbatch().batch(600) \n",
    "true_classes_8 = iter(true_classes_8).next().numpy() # to prove they are indeed the same in the new dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e824e9ca-19a4-4296-8edf-6a17b5891b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_8 = np.mean([1 if preds_8[i]==true_classes_8[i] else 0 for i in range(len(preds_8))])\n",
    "accuracy_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b4550fb-e543-4a98-814b-1c3f96080478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1         2         3\n",
      "0  0.906667  0.0  0.000000  0.093333\n",
      "1  0.000000  1.0  0.000000  0.000000\n",
      "2  0.000000  0.0  0.886667  0.113333\n",
      "3  0.113333  0.0  0.000000  0.886667\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(true_classes_8, preds_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e2bef-2b96-4ebe-b454-2545863eb0ad",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">We've matched the best performance from our previous model - though surpassing it has provem hard.  Class 1 has reached a point of perfect prediction on our test set, though the other 3 retain some \"hard\" cases to resolve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03584e78-cf7c-43ba-abe8-598ff6c824a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "####\n",
    "#### **A SET OF BINARY MODELS USING TRANSFER LEARNING**\n",
    "<p style=\"font-weight: 500; color: #556;\">We can now combine several of our previous techniques, using transfer learning as well as leaning on a suite of binary models, to squeeze out just a little more performance if possible.\n",
    "<p style=\"font-weight: 500; color: #556;\">To resolve some compatibility issues (possibly confined to M1 Macbook GPUs), it is more performant to generate a pre-augmented dataset, rather than augment on load.  The code below tackles this issue:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3472c82-5cf5-4b8f-8856-b47306ba6de6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60a50d75-0fe2-4a72-a7de-3d5cc9b668b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def augmenting_model(input_shape = (240, 240)):\n",
    "    input_shape = input_shape+(3,)\n",
    "    with tf.device('CPU'):\n",
    "        data_augmentation = tf.keras.Sequential([\n",
    "            tf.keras.layers.RandomFlip(),\n",
    "            tf.keras.layers.RandomRotation(0.1),\n",
    "            tf.keras.layers.RandomZoom(-0.3,0.3)\n",
    "        ])\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    outputs = data_augmentation(inputs)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "def augment_process(data, path, labels, repeat=1): # we perform augmentation as a separate exercise, to bypass GPU/CPU issues\n",
    "    os.makedirs(path, exist_ok = True)\n",
    "    for c in labels: os.makedirs(os.path.join(path, c), exist_ok=True)\n",
    "\n",
    "    aug_model = augmenting_model()\n",
    "    for i, batch in enumerate(data.repeat(repeat)):\n",
    "        x, y = batch\n",
    "        x = aug_model(x.numpy())\n",
    "        for j in range(len(x)):\n",
    "            l = y[j].numpy()\n",
    "            if (CATEGORIES[l] in labels) or (l in labels):\n",
    "                im = Image.fromarray(x[j].numpy().astype(np.uint8))\n",
    "                im.save(os.path.join(path, CATEGORIES[l], ('0000'+str((i*len(x))+j))[-5:]+'.jpeg'))\n",
    "\n",
    "    data_aug = tf.keras.utils.image_dataset_from_directory( # read back in as augmented TF dataset\n",
    "        path,\n",
    "        labels='inferred',\n",
    "        label_mode='int',\n",
    "        class_names=None,\n",
    "        color_mode='rgb',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(240, 240),\n",
    "        shuffle=True,\n",
    "        seed=84,\n",
    "        validation_split=None,\n",
    "        subset=None,\n",
    "        interpolation='bilinear',\n",
    "        follow_links=False,\n",
    "        crop_to_aspect_ratio=False\n",
    "    )\n",
    "\n",
    "    return data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f2af3f3-9cd9-49fd-b8b4-729a52037242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 11:11:26.634655: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27283 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_aug = augment_process(\n",
    "    train_data_7,\n",
    "    os.path.join(DATA_PATH,'augmented_train'),\n",
    "    CATEGORIES,\n",
    "    repeat=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf52f8-61ca-46a4-b3ae-28f56e18d9f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">It is also possible to mass-agment into memory before proceeding with the model development and fitting, though this can cause other performance issues and is not the approach taken.  The code is nevertheless kept here for posterity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ee89527e-43a3-4ba0-b01a-a8183aec7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data, dataset_size=2500, batch_size=32, input_shape=(240, 240), write_path='augmented'):\n",
    "    model_aug = augmenting_model(input_shape = input_shape)\n",
    "    aug_data_x, aug_data_y = deque(), deque()\n",
    "    for i, batch in enumerate(data):\n",
    "        if (i > 0) and (i%50 == 0): print(i, 'augmented batches processed...')\n",
    "        xt, yt = batch\n",
    "        xt = np.round(model_aug(xt).numpy()*1.,0)\n",
    "        yt = yt.numpy()*1.\n",
    "        for j in range(len(xt)):\n",
    "            aug_data_x.append(xt[j])\n",
    "            aug_data_y.append(np.int32(yt[j]))\n",
    "    aug_data_x, aug_data_y = np.asarray(aug_data_x), np.asarray(aug_data_y)\n",
    "    train_data_augmented = []\n",
    "    for i in range((len(aug_data_x)//dataset_size)+1):\n",
    "        imin, imax = dataset_size*i, min(len(aug_data_x),dataset_size*(i+1))\n",
    "        tf_data_x = tf.data.Dataset.from_tensor_slices(aug_data_x[imin:imax])\n",
    "        tf_data_y = tf.data.Dataset.from_tensor_slices(aug_data_y[imin:imax])\n",
    "        tf_data = tf.data.Dataset.zip((tf_data_x, tf_data_y)).cache().batch(batch_size).shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "        train_data_aug.append(tf_data)\n",
    "    return train_data_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b176f30c-011e-4a32-a6e4-8ab0c1746924",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 augmented batches processed...\n",
      "100 augmented batches processed...\n",
      "150 augmented batches processed...\n",
      "200 augmented batches processed...\n",
      "250 augmented batches processed...\n",
      "300 augmented batches processed...\n"
     ]
    }
   ],
   "source": [
    "train_data_aug = augment_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fc7a3-dd22-44a1-ab3c-5e6cee97c064",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 600; color: #556;\">MAP MULTICLASS TO BINARY DATASETS</p>\n",
    "<p style=\"font-weight: 500; color: #556;\">We'll need to map the tensorflow dataset objects from multiclass to binary using a transform on the labels, as follows:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "ad8dba91-ef6a-44e2-aeab-f0d1b2dd9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_datasets(data, pos_label):\n",
    "    return data.map(lambda x,y: (x,tf.map_fn(lambda z: 1 if z==pos_label else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "4a759f7b-4c57-409f-80ee-82cc2ff5d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_binary, train_data_binary_aug, val_data_binary, test_data_binary = [],[],[],[]\n",
    "for j in range(len(CATEGORIES)):\n",
    "    train_data_binary.append(make_binary_datasets(train_data_7, j))\n",
    "    train_data_binary_aug.append(make_binary_datasets(train_data_aug, j))\n",
    "    val_data_binary.append(make_binary_datasets(validation_data_7, j))\n",
    "    test_data_binary.append(make_binary_datasets(test_data_unshuffled_7, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828cebc-69e4-46b2-b439-530f520b96ec",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">Now we'll define the new models, using the same set of trainable layrers from our previous attempts, but with a binary loss function.  We'll also experiment with a more aggressive learning rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5de56254-7a9b-475e-9af4-8589220e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "lrs = (1e-4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "efd568a0-dc55-4407-ac5e-81e9302a2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_model(input_shape=(240,240), base_model_trainable=True, base_model_last_layer=None):\n",
    "    input_shape = input_shape+(3,)\n",
    "\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    # here we can make the model return whichever layer we want\n",
    "    if base_model_last_layer is not None:\n",
    "        base_model_test = tf.keras.Model(inputs = base_model.input, outputs = base_model.get_layer(base_model_last_layer).output)\n",
    "    base_model_test.trainable = base_model_trainable\n",
    "\n",
    "    inputs_new = tf.keras.Input(shape = input_shape)\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs_new)  # gives us values in the range [-1,1]\n",
    "    x = base_model_test(x, training = False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(16, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs_new = tf.keras.layers.Dense(1)(x)\n",
    "    model_new = tf.keras.Model(inputs_new, outputs_new)\n",
    "\n",
    "    model_new.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "32189a49-193d-4524-b6a0-484b45d8f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True)\n",
    "lr_scheduler = learning_trajectory(EPOCHS, lrs, 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f19d5591-fcf2-49b2-b733-2703667c39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_9, history_9 = [],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b8f47-40a9-4585-a20e-2734e90d9c68",
   "metadata": {},
   "source": [
    "<p style=\"font-weight: 500; color: #556;\">Having seen the limited benefits of frozen layers in our case (we may indeed be benefiting more from the model architecture than from the pretrained weights), we can just make our chosen layers trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "2248c9af-fc2b-47ec-b877-42ee2d2243c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "312/312 [==============================] - 591s 2s/step - loss: 0.3562 - accuracy: 0.8308 - val_loss: 0.2995 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 587s 2s/step - loss: 0.0837 - accuracy: 0.9681 - val_loss: 0.9900 - val_accuracy: 0.9189 - lr: 1.2743e-04\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 587s 2s/step - loss: 0.0819 - accuracy: 0.9722 - val_loss: 0.1650 - val_accuracy: 0.9465 - lr: 1.6238e-04\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 585s 2s/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.5373 - val_accuracy: 0.7493 - lr: 2.0691e-04\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 585s 2s/step - loss: 0.1196 - accuracy: 0.9497 - val_loss: 0.1999 - val_accuracy: 0.9438 - lr: 2.6367e-04\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 585s 2s/step - loss: 0.0412 - accuracy: 0.9876 - val_loss: 0.2427 - val_accuracy: 0.9555 - lr: 3.3598e-04\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 586s 2s/step - loss: 0.0659 - accuracy: 0.9757 - val_loss: 0.2465 - val_accuracy: 0.9433 - lr: 4.2813e-04\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 635s 2s/step - loss: 0.0481 - accuracy: 0.9840 - val_loss: 0.4713 - val_accuracy: 0.9364 - lr: 5.4556e-04\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 635s 2s/step - loss: 0.1369 - accuracy: 0.9525 - val_loss: 0.2211 - val_accuracy: 0.9020 - lr: 6.9519e-04\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_0/assets\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_0/assets\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 640s 2s/step - loss: 0.1161 - accuracy: 0.9392 - val_loss: 1.9258 - val_accuracy: 0.7557 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 633s 2s/step - loss: 0.0194 - accuracy: 0.9935 - val_loss: 8.2151e-05 - val_accuracy: 1.0000 - lr: 1.2743e-04\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 5.0225e-04 - accuracy: 0.9996 - val_loss: 4.3064e-04 - val_accuracy: 0.9995 - lr: 1.6238e-04\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 3.6202e-04 - accuracy: 0.9995 - val_loss: 1.7756e-04 - val_accuracy: 1.0000 - lr: 2.0691e-04\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.1219 - accuracy: 0.9447 - val_loss: 0.1547 - val_accuracy: 0.9592 - lr: 2.6367e-04\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_1/assets\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_1/assets\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 638s 2s/step - loss: 0.1231 - accuracy: 0.9441 - val_loss: 1.4319 - val_accuracy: 0.9311 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0281 - accuracy: 0.9941 - val_loss: 0.3472 - val_accuracy: 0.9348 - lr: 1.2743e-04\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 635s 2s/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 1.4957 - val_accuracy: 0.9385 - lr: 1.6238e-04\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.1299 - accuracy: 0.9521 - val_loss: 0.5790 - val_accuracy: 0.9380 - lr: 2.0691e-04\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0108 - accuracy: 0.9960 - val_loss: 1.3901 - val_accuracy: 0.9380 - lr: 2.6367e-04\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 1.5355 - val_accuracy: 0.9380 - lr: 3.3598e-04\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_2/assets\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_2/assets\n",
      "Epoch 1/20\n",
      "312/312 [==============================] - 640s 2s/step - loss: 0.2334 - accuracy: 0.8830 - val_loss: 0.5903 - val_accuracy: 0.8728 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0620 - accuracy: 0.9759 - val_loss: 0.7004 - val_accuracy: 0.8877 - lr: 1.2743e-04\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 635s 2s/step - loss: 0.0381 - accuracy: 0.9852 - val_loss: 2.3201 - val_accuracy: 0.7520 - lr: 1.6238e-04\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0432 - accuracy: 0.9839 - val_loss: 0.5097 - val_accuracy: 0.8839 - lr: 2.0691e-04\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.0566 - accuracy: 0.9774 - val_loss: 0.9732 - val_accuracy: 0.8728 - lr: 2.6367e-04\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 36). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_3/assets\n",
      "INFO:tensorflow:Assets written to: models/xception_binary_3/assets\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    models_9.append(make_binary_model(base_model_last_layer=base_model.layers[30])\n",
    "    history_9.append(models_binary[i].fit(\n",
    "        train_data_binary_aug[i],  # augmented training data\n",
    "        validation_data=val_data_binary[i],  # non-augmented validation data :)\n",
    "        batch_size=32,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[lr_scheduler, early_stopping]\n",
    "    ))\n",
    "    tf.keras.models.save_model(models_9[i], 'models/classification_xception_binary_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6e58d1-5af8-4b51-a937-51bd7863ae41",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">We can also further fine-tune the models using a low learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "id": "43bd25ae-c41a-46eb-961e-08ff8de486fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_EPOCHS = 20\n",
    "lrs = (1e-6, 1e-6)\n",
    "lr_scheduler = learning_trajectory(FINE_EPOCHS, lrs[0], lrs[1], 'exp')\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=4, mode='max', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "326a3f72-a359-44da-b2a2-2782c6761310",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_binary_ft = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0500561d-8d9d-4763-91ad-9736731ede08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    history_binary_ft.append(models_9[i].fit(\n",
    "        train_data_binary_aug[i], # augmented training data\n",
    "        validation_data=val_data_binary[i], # non-augmented validation data :)\n",
    "        batch_size=32,\n",
    "        epochs=FINE_EPOCHS,\n",
    "        initial_epoch=history_binary[i].epoch[-1]+1,\n",
    "        callbacks=[lr_scheduler, early_stopping]\n",
    "    ))\n",
    "    tf.keras.models.save_model(models_9[i], 'models/classification_xception_binary_fine_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e939a93-bb95-4337-9f0b-c8af53f99d4e",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">All that's left is to calculate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "0be862bf-7efa-4775-9c06-eeade2aee377",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_9a = models_9[0].predict(test_data_unshuffled_7, verbose=0)[:,0][np.newaxis]\n",
    "preds_9b = models_9[1].predict(test_data_unshuffled_7, verbose=0)[:,0][np.newaxis]\n",
    "preds_9c = models_9[2].predict(test_data_unshuffled_7, verbose=0)[:,0][np.newaxis]\n",
    "preds_9d = models_9[3].predict(test_data_unshuffled_7, verbose=0)[:,0][np.newaxis]\n",
    "\n",
    "preds_9 = np.concatenate([preds_9a, preds_9b, preds_9c, preds_9d], axis=0).T\n",
    "preds_9 = [np.argmax(p) for p in preds_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "3dfde4de-7c69-45be-be84-c19ba9eeb6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_9 = np.mean([1 if preds_9[i]==true_classes_8[i] else 0 for i in range(len(preds_9))])\n",
    "accuracy_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "7101bfad-496b-4564-aead-dccddbbbc5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.933333  0.026667  0.000000  0.040000\n",
      "1  0.000000  1.000000  0.000000  0.000000\n",
      "2  0.000000  0.000000  0.746667  0.253333\n",
      "3  0.053333  0.000000  0.000000  0.946667\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(true_classes_8, preds_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbb2a2-256c-4b45-9442-09b5d2abc6aa",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">We have a diminished performance on our precision for class 2 (false-positives sneaking in, though it does have perfect recall).\n",
    "<p style=\"font-weight: 500; color: #556;\">This is a good opportunity to use our previous (non-binary) model which happened to give us a stronger precision for the class, taking its predictions and using our new model only for the other three classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "68bd3536-eb16-43a6-81e3-ab559d9ecc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416666666666667"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_9_8 = [2 if preds_8[i]==2 else preds_9[i] for i in range(len(preds_9))]\n",
    "accuracy_9_8 = np.mean([1 if preds_9_8[i]==true_classes_8[i] else 0 for i in range(len(preds_9))])\n",
    "accuracy_9_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "adee6511-6673-4c53-be9a-89bc98736a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3\n",
      "0  0.933333  0.026667  0.000000  0.040000\n",
      "1  0.000000  1.000000  0.000000  0.000000\n",
      "2  0.000000  0.000000  0.886667  0.113333\n",
      "3  0.053333  0.000000  0.000000  0.946667\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(true_classes_8, preds_9_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b8a2f52d-b9d9-48b0-b29a-1a1cfe9eb071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[140,   4,   0,   6],\n",
       "       [  0, 150,   0,   0],\n",
       "       [  0,   0, 133,  17],\n",
       "       [  8,   0,   0, 142]], dtype=int32)>"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(\n",
    "    true_classes_8, preds_9_8,\n",
    "    num_classes=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe83398-15fd-424d-a5b4-719740783f32",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">With over 94% accuracy, the hybrid case gives us an outstandung result here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa224033-9a40-453f-ac1f-8e23f5172c77",
   "metadata": {
    "tags": []
   },
   "source": [
    "####\n",
    "#### **USING TRIPLET LOSS TO HELP RESOLVE THE MOST DIFFICULT CASES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55045682-8d9d-4573-bf2a-92f1876f4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "147c76a2-1a53-4394-a101-dddd7488e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplet_loss_model(input_shape=(240,240), base_model_trainable=True, base_model_last_layer=None):\n",
    "    input_shape = input_shape+(3,)\n",
    "\n",
    "    base_model = tf.keras.applications.Xception(\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    # here we can make the model return whichever layer we want\n",
    "    if base_model_last_layer is not None:\n",
    "        base_model_test = tf.keras.Model(inputs=base_model.input, outputs=base_model.get_layer(base_model_last_layer).output)\n",
    "    base_model_test.trainable = base_model_trainable\n",
    "\n",
    "    inputs_new = tf.keras.Input(shape=input_shape)\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs_new) # gives us values in the range [-1,1]\n",
    "    x = base_model_test(x, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) # lets experiment with some stronger regularization here \n",
    "    outputs_new = tf.keras.layers.Dense(728)(x)\n",
    "    model_new = tf.keras.Model(inputs_new, outputs_new)\n",
    "\n",
    "    model_new.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(),\n",
    "        loss = tfa.losses.TripletSemiHardLoss()\n",
    "    )\n",
    "\n",
    "    return model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "2caedd63-9277-44ea-81b7-b970bd177d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "lrs = (1e-4, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "d9a79b4b-30e9-4734-8880-33b05670a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', restore_best_weights=True)\n",
    "lr_scheduler = learning_trajectory(EPOCHS, lrs, 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "0eebc87a-6a78-4921-b535-b0181353a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "312/312 [==============================] - 609s 2s/step - loss: 0.4934 - val_loss: 0.5546 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "312/312 [==============================] - 601s 2s/step - loss: 0.1206 - val_loss: 0.5000 - lr: 1.2743e-04\n",
      "Epoch 3/20\n",
      "312/312 [==============================] - 604s 2s/step - loss: 0.0764 - val_loss: 0.4281 - lr: 1.6238e-04\n",
      "Epoch 4/20\n",
      "312/312 [==============================] - 605s 2s/step - loss: 0.0668 - val_loss: 0.4199 - lr: 2.0691e-04\n",
      "Epoch 5/20\n",
      "312/312 [==============================] - 592s 2s/step - loss: 0.1700 - val_loss: 0.5763 - lr: 2.6367e-04\n",
      "Epoch 6/20\n",
      "312/312 [==============================] - 590s 2s/step - loss: 0.0379 - val_loss: 0.7026 - lr: 3.3598e-04\n",
      "Epoch 7/20\n",
      "312/312 [==============================] - 634s 2s/step - loss: 0.2078 - val_loss: 0.4127 - lr: 4.2813e-04\n",
      "Epoch 8/20\n",
      "312/312 [==============================] - 601s 2s/step - loss: 0.2289 - val_loss: 0.4036 - lr: 5.4556e-04\n",
      "Epoch 9/20\n",
      "312/312 [==============================] - 596s 2s/step - loss: 0.6520 - val_loss: 0.9998 - lr: 6.9519e-04\n",
      "Epoch 10/20\n",
      "312/312 [==============================] - 602s 2s/step - loss: 0.9967 - val_loss: 0.9913 - lr: 8.8587e-04\n",
      "Epoch 11/20\n",
      "312/312 [==============================] - 601s 2s/step - loss: 0.9838 - val_loss: 0.9997 - lr: 0.0011\n",
      "Epoch 12/20\n",
      "312/312 [==============================] - 596s 2s/step - loss: 0.9960 - val_loss: 0.9998 - lr: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa3d7b3940>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tl = make_triplet_loss_model(base_model_last_layer='block13_sepconv2_act')\n",
    "model_tl.fit(\n",
    "    train_data_aug, # augmented training data\n",
    "    validation_data=validation_data, # non-augmented validation data :)\n",
    "    batch_size=32,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "37618155-b2b7-4304-b1ab-eb7177892e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model_tl, 'models/xception_tl') # TO DO - register the triplet loss function as a custom model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "96e43c69-0df5-4386-9f15-49c43b6a030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lrs = (1e-5, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "40c04859-65b1-4328-a683-61138b0c2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=25, mode='max', restore_best_weights=True)\n",
    "lr_scheduler = learning_trajectory(EPOCHS, lrs[0], lrs[1], 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458b4ac-0d37-4e92-8d39-3c860f00448d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, train_batch in enumerate(train_data_aug):\n",
    "    train_x, train_y = train_batch\n",
    "    pred_tl = model_tl.predict(train_x, verbose=0)[0]\n",
    "    train_y_real = train_y.numpy() if i == 0 else np.concatenate([train_y_real, train_y.numpy()], axis=0)\n",
    "    pred_tl_train = pred_tl if i == 0 else np.concatenate([pred_tl_train, pred_tl], axis=0)\n",
    "\n",
    "for i, val_batch in enumerate(validation_data_7):\n",
    "    val_x, val_y = val_batch\n",
    "    pred_tl = model_tl.predict(val_x, verbose = 0)[0]\n",
    "    val_y_real = val_y.numpy() if i == 0 else np.concatenate([val_y_real, val_y.numpy()], axis=0)\n",
    "    pred_tl_val = pred_tl if i == 0 else np.concatenate([pred_tl_val, pred_tl], axis=0)\n",
    "\n",
    "for i, test_batch in enumerate(test_data_unshuffled_7):\n",
    "    test_x, test_y = test_batch\n",
    "    pred_tl = model_tl.predict(test_x, verbose = 0)[0]\n",
    "    test_y_real = test_y.numpy() if i == 0 else np.concatenate([test_y_real, test_y.numpy()], axis=0)\n",
    "    pred_tl_test = pred_tl if i == 0 else np.concatenate([pred_tl_test, pred_tl], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820c411b-55c5-44c6-8fdb-c1a6ad786c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tl_train = tf.data.Dataset.from_tensor_slices((pred_tl_train, train_y_real)).batch(32)\n",
    "embedding_tl_val = tf.data.Dataset.from_tensor_slices((pred_tl_val, val_y_real)).batch(32)\n",
    "embedding_tl_test = tf.data.Dataset.from_tensor_slices((pred_tl_test, test_y_real)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c72d0592-4758-497c-a5b7-96f9afd1d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_classify = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model_tl_classify.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b0d47d1-6e08-4cdd-a3b4-adc58952159b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "853/853 [==============================] - 10s 6ms/step - loss: 1.1764 - accuracy: 0.8792 - val_loss: 1.0528 - val_accuracy: 0.8765 - lr: 1.0000e-05\n",
      "Epoch 2/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.7908 - accuracy: 0.9842 - val_loss: 0.7712 - val_accuracy: 0.8765 - lr: 1.1514e-05\n",
      "Epoch 3/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.4626 - accuracy: 0.9850 - val_loss: 0.5800 - val_accuracy: 0.8755 - lr: 1.3257e-05\n",
      "Epoch 4/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.2416 - accuracy: 0.9861 - val_loss: 0.5054 - val_accuracy: 0.8771 - lr: 1.5264e-05\n",
      "Epoch 5/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.1283 - accuracy: 0.9874 - val_loss: 0.5007 - val_accuracy: 0.8771 - lr: 1.7575e-05\n",
      "Epoch 6/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0749 - accuracy: 0.9892 - val_loss: 0.5203 - val_accuracy: 0.8786 - lr: 2.0236e-05\n",
      "Epoch 7/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0486 - accuracy: 0.9904 - val_loss: 0.5470 - val_accuracy: 0.8792 - lr: 2.3300e-05\n",
      "Epoch 8/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0341 - accuracy: 0.9916 - val_loss: 0.5769 - val_accuracy: 0.8781 - lr: 2.6827e-05\n",
      "Epoch 9/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0256 - accuracy: 0.9926 - val_loss: 0.6110 - val_accuracy: 0.8781 - lr: 3.0888e-05\n",
      "Epoch 10/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.6488 - val_accuracy: 0.8760 - lr: 3.5565e-05\n",
      "Epoch 11/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0180 - accuracy: 0.9942 - val_loss: 0.6873 - val_accuracy: 0.8771 - lr: 4.0949e-05\n",
      "Epoch 12/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.7239 - val_accuracy: 0.8776 - lr: 4.7149e-05\n",
      "Epoch 13/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.7577 - val_accuracy: 0.8771 - lr: 5.4287e-05\n",
      "Epoch 14/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.7887 - val_accuracy: 0.8771 - lr: 6.2506e-05\n",
      "Epoch 15/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0140 - accuracy: 0.9953 - val_loss: 0.8164 - val_accuracy: 0.8765 - lr: 7.1969e-05\n",
      "Epoch 16/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.8421 - val_accuracy: 0.8765 - lr: 8.2864e-05\n",
      "Epoch 17/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.8663 - val_accuracy: 0.8760 - lr: 9.5410e-05\n",
      "Epoch 18/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0125 - accuracy: 0.9956 - val_loss: 0.8903 - val_accuracy: 0.8749 - lr: 1.0985e-04\n",
      "Epoch 19/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.9162 - val_accuracy: 0.8744 - lr: 1.2649e-04\n",
      "Epoch 20/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0116 - accuracy: 0.9956 - val_loss: 0.9437 - val_accuracy: 0.8739 - lr: 1.4563e-04\n",
      "Epoch 21/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0111 - accuracy: 0.9958 - val_loss: 0.9743 - val_accuracy: 0.8733 - lr: 1.6768e-04\n",
      "Epoch 22/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0108 - accuracy: 0.9960 - val_loss: 1.0062 - val_accuracy: 0.8739 - lr: 1.9307e-04\n",
      "Epoch 23/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0103 - accuracy: 0.9961 - val_loss: 1.0414 - val_accuracy: 0.8733 - lr: 2.2230e-04\n",
      "Epoch 24/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0100 - accuracy: 0.9963 - val_loss: 1.0774 - val_accuracy: 0.8728 - lr: 2.5595e-04\n",
      "Epoch 25/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0097 - accuracy: 0.9964 - val_loss: 1.1113 - val_accuracy: 0.8733 - lr: 2.9471e-04\n",
      "Epoch 26/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0093 - accuracy: 0.9966 - val_loss: 1.1457 - val_accuracy: 0.8718 - lr: 3.3932e-04\n",
      "Epoch 27/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0090 - accuracy: 0.9966 - val_loss: 1.1804 - val_accuracy: 0.8728 - lr: 3.9069e-04\n",
      "Epoch 28/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 1.2337 - val_accuracy: 0.8733 - lr: 4.4984e-04\n",
      "Epoch 29/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 1.2777 - val_accuracy: 0.8728 - lr: 5.1795e-04\n",
      "Epoch 30/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0080 - accuracy: 0.9968 - val_loss: 1.3284 - val_accuracy: 0.8728 - lr: 5.9636e-04\n",
      "Epoch 31/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0076 - accuracy: 0.9968 - val_loss: 1.3827 - val_accuracy: 0.8723 - lr: 6.8665e-04\n",
      "Epoch 32/100\n",
      "853/853 [==============================] - 4s 5ms/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 1.4548 - val_accuracy: 0.8718 - lr: 7.9060e-04\n"
     ]
    }
   ],
   "source": [
    "history_tl = model_tl_classify.fit(\n",
    "    embedding_tl_train,\n",
    "    epochs=100,\n",
    "    validation_data=embedding_tl_val,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8f9ab0f8-11c1-445f-938d-25f29a453044",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_batch in enumerate(embedding_tl_test):\n",
    "    test_x, test_y = test_batch\n",
    "    pred_tl = model_tl_classify.predict(test_x, verbose=0)\n",
    "    test_y_real = test_y.numpy() if i == 0 else np.concatenate([test_y_real, test_y.numpy()], axis=0)\n",
    "    pred_tl_all = pred_tl if i == 0 else np.concatenate([pred_tl_all, pred_tl], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0b3d0e79-63e7-44c1-bfea-f48ab4a77a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tl_all = [np.argmax(p) for p in pred_tl_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "59fef8d2-a4e0-462f-89d9-78f234ae0032",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 2, 3, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(pred_tl_all)  # delete this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "54f35cac-bbeb-46b9-8863-0da8250d9be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1         2         3\n",
      "0  0.886667  0.0  0.000000  0.113333\n",
      "1  0.000000  1.0  0.000000  0.000000\n",
      "2  0.000000  0.0  0.746667  0.253333\n",
      "3  0.020000  0.0  0.006667  0.973333\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_y_real, pred_tl_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd261ba5-4d99-4211-803a-59c418a03a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[133,   0,   0,  17],\n",
       "       [  0, 150,   0,   0],\n",
       "       [  0,   0, 112,  38],\n",
       "       [  3,   0,   1, 146]], dtype=int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(\n",
    "    test_y_real,\n",
    "    pred_tl_all,\n",
    "    num_classes=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b812fa17-7132-442a-b0f8-a970f3156070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.167%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', '{:.3%}'.format(np.mean([1 if diff == 0 else 0 for diff in (pred_tl_all - test_y_real)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f5505-f8e2-484a-a4ca-51be07d3d7a5",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">The results of this method fall short, for now, of our best.  What happens if we supplement our weakest class (class 2) with a model which performed far better (model 8)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ef218f5-62db-4950-855e-89c2ec8b78ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.667%\n"
     ]
    }
   ],
   "source": [
    "zz = [2 if preds_8[i]==2 else pred_tl_all[i] for i in range(len(preds_8))]\n",
    "print('Accuracy:', '{:.3%}'.format(np.mean([1 if diff == 0 else 0 for diff in (zz - test_y_real)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7d712-1b63-4a20-916b-0a6a4079e631",
   "metadata": {
    "tags": []
   },
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">We're now much closer to our previous peak performance but can we do ever better?\n",
    "<p style=\"font-weight: 500; color: #556;\">Let's try the triplet loss for just the two \"hardest\" classes as a binary problem, and increase the number of instances per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "79a45018-9950-4835-8ed4-3d9bd69303c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_aug_v2 = augment_process(\n",
    "    train_data_7,\n",
    "    os.path.join(DATA_PATH, 'augmented_train_v2'),\n",
    "    [CATEGORIES[j] for j in [2,3]],\n",
    "    repeat=4\n",
    ").cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39fe53f3-3b9a-4d5a-aeb5-801f8ebf1208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1887 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH+'/validation',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=None,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=84,\n",
    ")\n",
    "\n",
    "# subset validation data to keep only classes 2 and 3\n",
    "x_tensors,y_tensors = [],[]\n",
    "labels = [2,3]\n",
    "for i,t in enumerate(validation_data):\n",
    "    if t[1].numpy() in labels:\n",
    "        x_tensors.append(t[0])\n",
    "        y_tensors.append(t[1])\n",
    "\n",
    "tf_labels = tf.constant(labels) \n",
    "\n",
    "validation_data_v2 = tf.data.Dataset.from_tensor_slices((x_tensors, y_tensors)).batch(32)\n",
    "validation_data_v2 = validation_data_v2.map(lambda x, y: (x, tf.map_fn(lambda z: tf.cast(tf.where(tf.equal(z,tf_labels))[0][0], dtype='int32'), y)))\n",
    "validation_data_v2 = validation_data_v2.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecc3ffc4-781b-401b-84ca-94b395007723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_PATH+'/test',\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    batch_size=None,\n",
    "    image_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=84,\n",
    ")\n",
    "\n",
    "# subset validation data to keep only classes 2 and 3\n",
    "x_tensors,y_tensors = [],[]\n",
    "labels = [2,3]\n",
    "for i,t in enumerate(test_data):\n",
    "    if t[1].numpy() in labels:\n",
    "        x_tensors.append(t[0])\n",
    "        y_tensors.append(t[1])\n",
    "    \n",
    "@tf.function\n",
    "def map_labels(labels,z):\n",
    "    tf_labels = tf.constant(labels)\n",
    "    return tf.cast(tf.where(tf.equal(z,tf_labels))[0][0],dtype='int32')\n",
    "\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_tensors, y_tensors)).batch(32)\n",
    "test_data = test_data.map(lambda x, y: (x, tf.map_fn(lambda z: map_labels(labels,z), y)))\n",
    "test_data = test_data.cache().prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "55b145fc-d465-46ab-bfbb-1641938aaa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "lrs = (1e-7, 1e-5)  # using a VERY low learning rate here to address observed sensitivity of the loss function\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "lr_scheduler = learning_trajectory(EPOCHS, lrs[0], lrs[1], 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ea942ac4-1641-459e-a5b8-d6c453e01764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "622/622 [==============================] - 1190s 2s/step - loss: 0.9266 - val_loss: 0.9631 - lr: 1.0000e-07\n",
      "Epoch 2/20\n",
      "622/622 [==============================] - 1173s 2s/step - loss: 0.9211 - val_loss: 0.9631 - lr: 1.2743e-07\n",
      "Epoch 3/20\n",
      "622/622 [==============================] - 1133s 2s/step - loss: 0.9118 - val_loss: 0.9634 - lr: 1.6238e-07\n",
      "Epoch 4/20\n",
      "622/622 [==============================] - 1140s 2s/step - loss: 0.8955 - val_loss: 0.9630 - lr: 2.0691e-07\n",
      "Epoch 5/20\n",
      "622/622 [==============================] - 1103s 2s/step - loss: 0.8790 - val_loss: 0.9602 - lr: 2.6367e-07\n",
      "Epoch 6/20\n",
      "622/622 [==============================] - 1102s 2s/step - loss: 0.8565 - val_loss: 0.9509 - lr: 3.3598e-07\n",
      "Epoch 7/20\n",
      "622/622 [==============================] - 1102s 2s/step - loss: 0.8102 - val_loss: 0.9214 - lr: 4.2813e-07\n",
      "Epoch 8/20\n",
      "622/622 [==============================] - 1100s 2s/step - loss: 0.7213 - val_loss: 0.6854 - lr: 5.4556e-07\n",
      "Epoch 9/20\n",
      "622/622 [==============================] - 1100s 2s/step - loss: 0.5426 - val_loss: 0.4019 - lr: 6.9519e-07\n",
      "Epoch 10/20\n",
      "622/622 [==============================] - 1101s 2s/step - loss: 0.3065 - val_loss: 0.3315 - lr: 8.8587e-07\n",
      "Epoch 11/20\n",
      "622/622 [==============================] - 1100s 2s/step - loss: 0.1523 - val_loss: 0.3789 - lr: 1.1288e-06\n",
      "Epoch 12/20\n",
      "622/622 [==============================] - 1100s 2s/step - loss: 0.0738 - val_loss: 0.6385 - lr: 1.4384e-06\n",
      "Epoch 13/20\n",
      "622/622 [==============================] - 1100s 2s/step - loss: 0.0324 - val_loss: 0.8864 - lr: 1.8330e-06\n",
      "Epoch 14/20\n",
      "622/622 [==============================] - 1118s 2s/step - loss: 0.0155 - val_loss: 0.5667 - lr: 2.3357e-06\n",
      "Epoch 15/20\n",
      "622/622 [==============================] - 1127s 2s/step - loss: 0.0072 - val_loss: 1.2476 - lr: 2.9764e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4b7bab0d0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tl_binary = make_triplet_loss_model(base_model_last_layer='block13_sepconv2_act')\n",
    "model_tl_binary.fit(\n",
    "    train_data_aug_v2, # augmented training data\n",
    "    validation_data=validation_data_v2, # non-augmented validation data :)\n",
    "    batch_size=32,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d49c621b-e00b-4bc8-8718-c16688119b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model_tl_binary, 'models/xception_tl_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8085ed48-211c-4553-8cec-018153afdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_binary = tf.keras.models.load_model('models/xception_tl_binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78f6795a-2b7c-4d99-8b54-c578aa02db52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, train_batch in enumerate(train_data_aug_v2):\n",
    "    train_x, train_y = train_batch\n",
    "    pred_tl = model_tl_binary.predict(train_x, verbose = 0)\n",
    "    train_y_real_binary = train_y.numpy() if i == 0 else np.concatenate([train_y_real_binary, train_y.numpy()], axis = 0)\n",
    "    pred_tl_train_binary = pred_tl if i == 0 else np.concatenate([pred_tl_train_binary, pred_tl], axis = 0)\n",
    "\n",
    "for i, val_batch in enumerate(validation_data_v2):\n",
    "    val_x, val_y = val_batch\n",
    "    pred_tl = model_tl_binary.predict(val_x, verbose = 0)\n",
    "    val_y_real_binary = val_y.numpy() if i == 0 else np.concatenate([val_y_real_binary, val_y.numpy()], axis = 0)\n",
    "    pred_tl_val_binary = pred_tl if i == 0 else np.concatenate([pred_tl_val_binary, pred_tl], axis = 0)\n",
    "\n",
    "for i, test_batch in enumerate(test_data):  # we actually test on all 4 classes again\n",
    "    test_x, test_y = test_batch\n",
    "    pred_tl = model_tl_binary.predict(test_x, verbose = 0)\n",
    "    test_y_real_binary = test_y.numpy() if i == 0 else np.concatenate([test_y_real_binary, test_y.numpy()], axis = 0)\n",
    "    pred_tl_test_binary = pred_tl if i == 0 else np.concatenate([pred_tl_test_binary, pred_tl], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "577680aa-b609-436a-8df8-0f4b615baba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_tl_binary_train = tf.data.Dataset.from_tensor_slices((pred_tl_train_binary, train_y_real_binary)).batch(32)\n",
    "embedding_tl_binary_val = tf.data.Dataset.from_tensor_slices((pred_tl_val_binary, val_y_real_binary)).batch(32)\n",
    "embedding_tl_binary_test = tf.data.Dataset.from_tensor_slices((pred_tl_test_binary, test_y_real_binary)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3dba9a38-c062-467d-b31c-da06be27f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tl_binary_classify = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model_tl_binary_classify.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af4d1b19-d0bf-4064-9574-ae8553e71769",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "lrs = (1e-6, 1e-4)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_accuracy', patience = 10, mode = 'max', restore_best_weights = True)\n",
    "lr_scheduler = learning_trajectory(EPOCHS, lrs[0], lrs[1], 'exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "633ac830-6d5b-4094-a5d0-d9c7b3c4bc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1091/1091 [==============================] - 7s 6ms/step - loss: 0.5167 - accuracy: 0.8146 - val_loss: 0.4818 - val_accuracy: 0.9195 - lr: 1.0000e-06\n",
      "Epoch 2/50\n",
      "1091/1091 [==============================] - 6s 5ms/step - loss: 0.3807 - accuracy: 0.9550 - val_loss: 0.3803 - val_accuracy: 0.9470 - lr: 1.0985e-06\n",
      "Epoch 3/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.2818 - accuracy: 0.9773 - val_loss: 0.3062 - val_accuracy: 0.9534 - lr: 1.2068e-06\n",
      "Epoch 4/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.2107 - accuracy: 0.9838 - val_loss: 0.2566 - val_accuracy: 0.9492 - lr: 1.3257e-06\n",
      "Epoch 5/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.1602 - accuracy: 0.9854 - val_loss: 0.2244 - val_accuracy: 0.9386 - lr: 1.4563e-06\n",
      "Epoch 6/50\n",
      "1091/1091 [==============================] - 6s 5ms/step - loss: 0.1240 - accuracy: 0.9864 - val_loss: 0.2044 - val_accuracy: 0.9301 - lr: 1.5999e-06\n",
      "Epoch 7/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.0970 - accuracy: 0.9875 - val_loss: 0.1922 - val_accuracy: 0.9258 - lr: 1.7575e-06\n",
      "Epoch 8/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.0764 - accuracy: 0.9881 - val_loss: 0.1897 - val_accuracy: 0.9216 - lr: 1.9307e-06\n",
      "Epoch 9/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.0627 - accuracy: 0.9884 - val_loss: 0.1926 - val_accuracy: 0.9195 - lr: 2.1210e-06\n",
      "Epoch 10/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.0535 - accuracy: 0.9885 - val_loss: 0.1983 - val_accuracy: 0.9184 - lr: 2.3300e-06\n",
      "Epoch 11/50\n",
      "1091/1091 [==============================] - 5s 5ms/step - loss: 0.0474 - accuracy: 0.9884 - val_loss: 0.2054 - val_accuracy: 0.9174 - lr: 2.5595e-06\n",
      "Epoch 12/50\n",
      "1091/1091 [==============================] - 6s 5ms/step - loss: 0.0434 - accuracy: 0.9883 - val_loss: 0.2127 - val_accuracy: 0.9153 - lr: 2.8118e-06\n",
      "Epoch 13/50\n",
      "1091/1091 [==============================] - 6s 5ms/step - loss: 0.0409 - accuracy: 0.9883 - val_loss: 0.2195 - val_accuracy: 0.9153 - lr: 3.0888e-06\n"
     ]
    }
   ],
   "source": [
    "history_tl = model_tl_binary_classify.fit(\n",
    "    embedding_tl_binary_train,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = embedding_tl_binary_val,\n",
    "    callbacks = [early_stopping, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e60f8455-711e-49bc-82a9-8468a3b20acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/xception_tl_binary_classify/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/xception_tl_binary_classify/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(model_tl_binary_classify, 'models/xception_tl_binary_classify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0ada49c7-831a-4a22-9a53-e27ce7a22668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, test_batch in enumerate(embedding_tl_binary_test):\n",
    "    test_x, test_y = test_batch\n",
    "    pred_tl = model_tl_binary_classify.predict(test_x, verbose = 0)\n",
    "    test_y_real_binary = test_y.numpy() if i == 0 else np.concatenate([test_y_real_binary, test_y.numpy()], axis = 0)\n",
    "    pred_tl_all_binary = pred_tl if i == 0 else np.concatenate([pred_tl_all_binary, pred_tl], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "70651c2b-8896-4139-986d-40c2018d85f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.500%\n"
     ]
    }
   ],
   "source": [
    "pred_tl_all_binary = 2+np.asarray([int(p) for p in np.round(pred_tl_all_binary.flatten(),0)])\n",
    "pred_tl_all_binary_final = np.concatenate([pred_tl_all[:300],pred_tl_all_binary],axis=0)\n",
    "print('Accuracy:','{:.3%}'.format(np.mean([1 if diff == 0 else 0 for diff in (pred_tl_all_binary_final - test_y_real)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a1bb896d-556e-4e6c-b4fa-7a874d9e2620",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 0 3 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
      " 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0\n",
      " 3 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 3\n",
      " 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 3 3 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n",
      " 2 2 2 2 2 2 2 2 3 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2\n",
      " 2 3 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2\n",
      " 3 2 2 2 2 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(pred_tl_all_binary_final)  # can remove this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4d5cd515-dbbe-406e-9c24-dc6774212bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0    1         2         3\n",
      "0  0.886667  0.0  0.000000  0.113333\n",
      "1  0.000000  1.0  0.000000  0.000000\n",
      "2  0.000000  0.0  0.913333  0.086667\n",
      "3  0.000000  0.0  0.020000  0.980000\n"
     ]
    }
   ],
   "source": [
    "print(make_confusion_matrix(test_y_real, pred_tl_all_binary_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d6c2c5f6-f298-45a4-b3c4-0a27741faaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[  0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0],\n",
       "       [  0,   0, 137,  13],\n",
       "       [  0,   0,   3, 147]], dtype=int32)>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(\n",
    "    test_y_real_binary+2,\n",
    "    pred_tl_all_binary,\n",
    "    num_classes=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815debf-f072-4b69-afba-c87e8ab5bd29",
   "metadata": {},
   "source": [
    "#####\n",
    "<p style=\"font-weight: 500; color: #556;\">We've achieved our best score using this technique, 94.5%.  There are potentially more gains to be had in making binary cases for all classes - this, however, I will leave for later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fcb2b-5fa9-44e8-a5e4-305b7991f736",
   "metadata": {},
   "source": [
    "####\n",
    "#### **CONCLUSIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e63c06-2a41-45e0-9a01-a17d6abfd46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 tf1",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
